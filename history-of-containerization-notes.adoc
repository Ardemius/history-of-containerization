= L'√©volution de la conteneurisation, de 1979 √† aujourd'hui
Thomas SCHWENDER <icon:github[] https://github.com/Ardemius/[GitHub] / icon:twitter[role="aqua"] https://twitter.com/thomasschwender[@thomasschwender]>
// Handling GitHub admonition blocks icons
ifndef::env-github[:icons: font]
ifdef::env-github[]
:status:
:outfilesuffix: .adoc
:caution-caption: :fire:
:important-caption: :exclamation:
:note-caption: :paperclip:
:tip-caption: :bulb:
:warning-caption: :warning:
endif::[]
:imagesdir: ./images
:resourcesdir: ./resources
:source-highlighter: highlightjs
:highlightjs-languages: asciidoc
// We must enable experimental attribute to display Keyboard, button, and menu macros
:experimental:
// Next 2 ones are to handle line breaks in some particular elements (list, footnotes, etc.)
:lb: pass:[<br> +]
:sb: pass:[<br>]
// check https://github.com/Ardemius/personal-wiki/wiki/AsciiDoctor-tips for tips on table of content in GitHub
:toc: macro
:toclevels: 4
// To number the sections of the table of contents
//:sectnums:
// Add an anchor with hyperlink before the section title
:sectanchors:
// To turn off figure caption labels and numbers
:figure-caption!:
// Same for examples
//:example-caption!:
// To turn off ALL captions
// :caption:

toc::[]

== Overview

Proposition de sujet (conference) pour Devoxx France 2022 et 2023

Talk ayant pour but de retracer l'histoire de la conteneurisation, de donner ses grandes dates depuis ses d√©buts jusqu'√† nos jours, et d'expliquer les raisons derri√®re chaque grande √©volution.

L'autre id√©e derri√®re ce talk : "mais POURQUOI on a fait tout √ßa ?"

Ce talk ne va pas rentrer dans le d√©tail de toutes les √©tapes

*Th√®me principal* : Cloud, Containers et Infrastructure, DevOps

*Niveau de la pr√©sentation* : Ouvert √† tout niveau d'audience, d√©butant comme expert

*Tags* : 

    * Containers
    * history
    * container characteristics

== Recherche du nom de la conf√©rence

=== Devoxx France 2022 CFP

* A Brief History of Containers
* A brief history of containerization, from yesterday to today

* Petite histoire de la conteneurisation, d'hier √† nos jours
* *L'√©volution de la conteneurisation, de 1979 √† aujourd'hui* : on va garder ce titre

=== Devoxx France 2023 CFP

* De chroot √† Docker, Podman, et maintenant les modules Wasm, histoire acc√©l√©r√©e de plus de 40 ans d'√©volution de la conteneurisation.
* chroot, Docker, Podman et maintenant modules Wasm, 40 ans d'√©volution de la conteneurisation
* *De chroot √† Docker, Podman, et maintenant les modules Wasm, 40 ans d'√©volution de la conteneurisation*

== Abstract

=== Devoxx France 2022 CFP

* Expliquer le "pourquoi ces changements, ces √©volutions ?" afin de comprendre comment nous en sommes arriv√©s √† la situation actuelle

.id√©es de formulation
----
La plupart d'entre nous ont d√©couvert le concept de conteneur en 2013 avec l'arriv√©e de Docker.
2013 avait marqu√© l'arriv√©e de Docker, et fait conna√Ætre le concept de conteneur au plus grand nombre.

Aujourd'hui, avec l'av√®nement du Cloud et de Kubernetes, les conteneurs sont partout autour de nous.


Mais en fait, l'histoire de ces derniers / des conteneurs est bien plus √©tendue que cela, et il faut remonter √† la fin des ann√©es 70s pour en trouver le v√©ritable d√©but / commencement avec l'apparition de chroot.
Mais l'histoire de ces derniers / des conteneurs est en fait bien plus √©tendue que cela, et il faut remonter √† la fin des ann√©es 70s pour en trouver le v√©ritable d√©but / commencement avec l'apparition de chroot.
Mais l'histoire de la conteneurisation a en fait d√©but√© il y a bien longtemps, d√®s la fin des ann√©es 70s avec l'apparition de chroot, et depuis

Toutefois, l'histoire de la conteneurisation reste globalement m√©connue.
Qui pour se souvenir que tout a finalement commenc√© en 1979 avec l'apparition de chroot, qui marqua le d√©but de l'isolation des process ?
Qu'il a fallu attendre d'avoir les cgroups (2008), puis plus tard les user namespaces (2013) pour qu'enfin Docker puisse voir le jour ?
Que la g√©n√©ralisation des containers, cons√©quence de l'adoption de Kubernetes par tous les Cloud providers (2018), entra√Æna l'apparition des sandbox runtimes et des daemonless runtimes ?

Au cours de ce talk, nous allons d√©tailler les grandes √©tapes qui ont marqu√© l'histoire de la conteneurisation, et expliquer ce qui les a d√©clench√©es / POURQUOI elles ont eu lieu.
L'objectif est qu'en fin de s√©ance, vous ayez compris quelle orientation a suivi la conteneurisation depuis toutes ces ann√©es, et ce vers quoi nous allons.

Le but de ce talk est de vous expliquer POURQUOI les choses ont chang√©s, afin de vous permettre de comprendre l'orientation qu'a suivi la conteneurisaiton depuis toutes ses ann√©es.
Pourquoi a-t-il 


Apr√®s ce sont suivis les Linux namespaces, les cgroups, Docker, l'arriv√©e des low-level et high-level runtimes avec runc et containerd, jusqu'aux derniers sandbox runtimes et daemonless runtimes.

Au cours de ce talk, nous allons d√©tailler les √©tapes majeures de la conteneurisation, et vous permettre de comprendre POURQUOI <nous> sommes pass√©s de l'une √† l'autre, et quelles orientations ont suivies / suivent les containers <aujourd'hui>

et il faut remonter √† la fin des ann√©es 70 pour voir appara√Ætre chroot, le point de d√©part / l'anc√™tre de la containerisation

et a √©t√© marqu√©e par de
et de nombreux √©v√®nements et shift technologiques ont √©t√© v√©cus pour arriver aux conteneurs que nous conaissons aujourd'hui

Au cours de ce talk, nous allons d√©tailler

cgroups et namespaces, arriv√©e de docker, split et arriv√©e des low-level et high-level runtimes avec runc et containerd, etc.
Nous allons d√©tailler dans ce talk les grandes √©tapes de la conteneurisation, et vous permettre de comprendre POURQUOI nous 

Avec l'av√®nement du Cloud et de Kubernetes, les containers sont aujourd'hui partout autour de nous.
2013 avait 

Chaque 
Plus Docker, il y a maintenant runc et containerd
----

.version finale
----
La plupart d'entre nous ont d√©couvert le concept de conteneur en 2013 avec l'arriv√©e de Docker.
Aujourd'hui, avec l'av√®nement du Cloud et de Kubernetes, les conteneurs sont partout autour de nous.

Toutefois, l'histoire de la conteneurisation reste globalement m√©connue.
Qui pour se souvenir que tout a finalement commenc√© en 1979 avec l'apparition de chroot, qui marqua le d√©but de l'isolation des process ?
Qu'il a fallu attendre d'avoir les cgroups (2008), puis plus tard les user namespaces (2013) pour qu'enfin Docker puisse voir le jour ?
Que la g√©n√©ralisation des containers, cons√©quence de l'adoption de Kubernetes par tous les Cloud providers (2018), entra√Æna l'apparition des sandbox runtimes et des daemonless runtimes ?

Au cours de ce talk, nous allons d√©tailler les grandes √©tapes qui ont marqu√© l'histoire de la conteneurisation, et expliquer POURQUOI elles ont eu lieu.
L'objectif est qu'en sortant, vous ayez compris quelles orientations a suivi la conteneurisation depuis toutes ces ann√©es, et ce vers quoi nous allons.
----

.version finale au format markdown (pour le CFP de Devoxx France)
----
La plupart d'entre nous ont d√©couvert le concept de conteneur en **2013** avec l'arriv√©e de **Docker**.  
Aujourd'hui, avec l'av√®nement du Cloud et de Kubernetes, les conteneurs sont partout autour de nous.

Toutefois, **l'histoire de la conteneurisation** reste globalement m√©connue.  
Qui pour se souvenir que tout a finalement commenc√© en **1979** avec l'apparition de **chroot**, qui marqua le d√©but de l'isolation des process ?  
Qu'il a fallu attendre d'avoir les **cgroups** (2008), puis plus tard les **user namespaces** (2013) pour qu'enfin Docker puisse voir le jour ?  
Que la g√©n√©ralisation des containers, cons√©quence de **l'adoption de Kubernetes** par tous les Cloud providers (2018), entra√Æna l'apparition des **sandbox runtimes** et des **daemonless runtimes** ?

Au cours de ce talk, nous allons d√©tailler les **grandes √©tapes** qui ont marqu√© l'histoire de la conteneurisation, et expliquer **POURQUOI** elles ont eu lieu.  
L'objectif est qu'en sortant, vous ayez compris **quelles orientations a suivi la conteneurisation** depuis toutes ces ann√©es, et ce vers quoi nous allons.
----

=== Devoxx France 2023 CFP

.version finale
----
La plupart d'entre nous ont d√©couvert le concept de conteneur en 2013 avec l'arriv√©e de Docker.
Aujourd'hui, avec l'av√®nement du Cloud et de Kubernetes, les conteneurs sont partout autour de nous.

Toutefois, l'histoire de la conteneurisation reste globalement m√©connue.
Qui pour se souvenir que tout a commenc√© en 1979 avec l'apparition de chroot, qui marqua le d√©but de l'isolation des process ?
Qu'il a fallu attendre d'avoir les cgroups (2008), puis les user namespaces (2013) pour qu'enfin Docker puisse voir le jour ?
Que la g√©n√©ralisation des containers, cons√©quence de l'adoption de Kubernetes par tous les Cloud providers (2018), entra√Æna l'apparition des sandbox runtimes et des daemonless runtimes ?
Sans oublier la r√©cente pouss√©e de WebAssembly et de ses modules (2022), nos nouveaux "containers Javascript".

Au cours de ce talk, nous allons d√©tailler les grandes √©tapes qui ont marqu√© l'histoire de la conteneurisation, et expliquer POURQUOI elles ont eu lieu.
L'objectif est qu'en sortant, vous ayez compris quelles orientations a suivi la conteneurisation depuis toutes ces ann√©es, et ce vers quoi nous allons.
----

.version finale au format markdown (pour le CFP de Devoxx France)
----
La plupart d'entre nous ont d√©couvert le concept de conteneur en **2013** avec l'arriv√©e de **Docker**.  
Aujourd'hui, avec l'av√®nement du Cloud et de Kubernetes, les conteneurs sont partout autour de nous.

Toutefois, **l'histoire de la conteneurisation** reste globalement m√©connue.  
Qui pour se souvenir que tout a commenc√© en **1979** avec l'apparition de **chroot**, qui marqua le d√©but de l'isolation des process ?  
Qu'il a fallu attendre d'avoir les **cgroups** (2008), puis les **user namespaces** (2013) pour que Docker puisse voir le jour ?  
Que la g√©n√©ralisation des containers, cons√©quence de **l'adoption de Kubernetes** par tous les Cloud providers (2018), entra√Æna l'apparition des **sandbox runtimes** et des **daemonless runtimes** ?  
Sans oublier la r√©cente pouss√©e de **WebAssembly** et de ses **modules** (2022), nos nouveaux "containers Javascript".

Au cours de ce talk, nous allons d√©tailler les **grandes √©tapes** qui ont marqu√© l'histoire de la conteneurisation, et expliquer **POURQUOI** elles ont eu lieu.  
L'objectif est qu'en sortant, vous ayez compris **quelles orientations a suivi la conteneurisation** depuis toutes ces ann√©es, et ce vers quoi nous allons.
----

== Message pour le comit√©

* Donner le lien vers le repo GitHub, pour montrer la recherche documentaire
* Expliquer que l'id√©e du talk est venue de la prez √† la StarTECH sur les low-level et high-level containers : quelques personnes connaissaient ces concepts, mais personne ne savaient d'o√π ils venaient.

----
Bonjour,

Mi-2021 j'ai fait une pr√©sentation √† la communaut√© technique de ma soci√©t√© sur les low-level et les high-level container runtimes (ex : runc et containerd). Il est apparu que, si certains en avaient d√©j√† entendu parler, **aucun ne savait r√©ellement comment ni pourquoi les conteneurs en √©taient arriv√©s l√†**.

C'est de l√† d'o√π m'est venue l'id√©e de ce talk : **d√©tailler la chronologie des grandes √©tapes de la conteneurisation**, de ses d√©buts √† nos jours, et surtout **expliquer POURQUOI ces √©tapes ont eu lieu**.  
J'ai demand√© aux membres de la communaut√© s'ils trouvaient le sujet int√©ressant, et les retours ont √©t√© tr√®s enthousiastes.

J'ai d√©j√† termin√© ma recherche documentaire, ainsi que l'√©tude associ√©e en tr√®s grande partie.  
Ceux qui le souhaitent peuvent d√©j√† jeter un oeil √† la chronologie que je compte pr√©senter dans mes notes sur GitHub :  
[https://github.com/Ardemius/history-of-containerization/blob/main/history-of-containerization-notes.adoc#8-frise-temporelle-r%C3%A9duite](https://github.com/Ardemius/history-of-containerization/blob/main/history-of-containerization-notes.adoc#8-frise-temporelle-r%C3%A9duite)  
J'ai dans l'id√©e de la passer dans un outil comme [https://www.timetoast.com/](https://www.timetoast.com/) pour la rendre plus dynamique.

**Je suis preneur de toutes vos questions et retours** pour approfondir le sujet.  
Il est d√©j√† pr√©vu avec ma communaut√© technique que je fasse plusieurs pr√©sentations blanches d√©but d'ann√©e pour aider ma pr√©paration.
----

=== Devoxx France 2022 CFP

.version markdown pour le CFP de Devoxx France 2022
----
Bonjour,

Mi-2021 j'ai fait une pr√©sentation √† la communaut√© technique de ma soci√©t√© sur les low-level et les high-level container runtimes (ex : runc et containerd). Il est apparu que, si certains en avaient d√©j√† entendu parler, aucun ne savait r√©ellement comment ni pourquoi les conteneurs en √©taient arriv√©s l√†.

C'est de l√† d'o√π m'est venue l'id√©e de ce talk : d√©tailler la chronologie des grandes √©tapes de la conteneurisation, de ses d√©buts √† nos jours, et surtout expliquer POURQUOI ces √©tapes ont eu lieu.
J'ai demand√© aux membres de la communaut√© s'ils trouvaient le sujet int√©ressant, et les retours ont √©t√© tr√®s enthousiastes.

J'ai d√©j√† termin√© ma recherche documentaire, ainsi que l'√©tude associ√©e en tr√®s grande partie. Ceux qui le souhaitent peuvent d√©j√† jeter un oeil √† la chronologie que je compte pr√©senter dans mes notes sur GitHub :  
https://github.com/Ardemius/history-of-containerization/blob/main/history-of-containerization-notes.adoc#8-frise-temporelle-r%C3%A9duite  
J'ai dans l'id√©e de la passer dans un outil comme https://www.timetoast.com/ pour la rendre plus dynamique.

Je suis preneur de toutes vos questions et retours pour approfondir le sujet.  
Il est d√©j√† pr√©vu avec ma communaut√© technique que je fasse plusieurs pr√©sentations blanches d√©but d'ann√©e pour aider ma pr√©paration.
----

=== DevFest Nantes 2022 CFP

.version markdown pour le CFP de DevFest Nantes 2022 (section "r√©f√©rences du talk")
----
Bonjour,

Mi-2021 j'ai fait une pr√©sentation √† la communaut√© technique de ma soci√©t√© sur les low-level et les high-level container runtimes (ex : runc et containerd). Il est apparu que, si certains en avaient d√©j√† entendu parler, aucun ne savait r√©ellement comment ni pourquoi les conteneurs en √©taient arriv√©s l√†.

C'est de l√† d'o√π m'est venue l'id√©e de ce talk : d√©tailler la chronologie des grandes √©tapes de la conteneurisation, de ses d√©buts √† nos jours, et surtout expliquer **POURQUOI** ces √©tapes ont eu lieu.
J'ai demand√© aux membres de la communaut√© s'ils trouvaient le sujet int√©ressant, et les retours ont √©t√© tr√®s enthousiastes.

J'ai termin√© ma recherche documentaire, ainsi que l'√©tude associ√©e en tr√®s grande partie. Ceux qui le souhaitent peuvent d√©j√† jeter un oeil √† la **chronologie** que je compte pr√©senter dans mes notes sur GitHub :  
https://github.com/Ardemius/history-of-containerization/blob/main/history-of-containerization-notes.adoc#8-frise-temporelle-r%C3%A9duite  
J'ai dans l'id√©e de la passer dans un outil comme https://www.timetoast.com/ pour la rendre plus dynamique.

J'ai d√©j√† fait de 1eres pr√©sentations de ce talk √† mon JUG (https://github.com/startechsofteam) et dans notre √©quivalent des BBL, et ai eu de tr√®s bon retours (m√™me si le talk n'√©tait pas encore totalement termin√©).  
Je compte faire une (ou plusieurs) pr√©sentations blanches compl√®tes dans les quelques mois √† venir.

Je suis **preneur de toutes vos questions et retours** pour approfondir le sujet üôÇ
----

.version markdown pour le CFP de DevFest Nantes 2022 (section "message pour le comit√©")
----
Bonjour,

Comme expliqu√© dans la partie "r√©f√©rences" du talk, l'id√©e de ce dernier m'est venu lors d'une pr√©sentation plus sp√©cifique des container runtimes.  
Durant celle-ci, j'ai constat√© que l'histoire de la conteneurisation √©tait peu connue, d'o√π l'id√©e d'en pr√©senter une chronologie.

Un r√©sum√© de celle-ci est disponible sur le GitHub du talk (https://github.com/Ardemius/history-of-containerization/blob/main/history-of-containerization-notes.adoc#8-frise-temporelle-r%C3%A9duite).  
**Je suis preneur de tous retours et questions dessus**, tout particuli√®rement **si vous souhaitez que je mette davantage l'accent sur certaines parties**.  
Le sujet est vaste, et j'ai effectu√© beaucoup de recherches si chaque partie, dont je propose une synth√®se tenant en 45 min.  
Dans ce temps, je peux insister davantage sur certains points, n'h√©sitez pas √† me faire part de vos pr√©f√©rences üôÇ
----

=== Devoxx France 2023 CFP

.version markdown pour le CFP de Devoxx France 2023
----
Bonjour,

Mi-2021 j'ai fait une pr√©sentation √† la communaut√© technique de ma soci√©t√© sur les low-level et les high-level container runtimes (ex : runc et containerd). Il est apparu que, si certains en avaient d√©j√† entendu parler, aucun ne savait r√©ellement comment ni pourquoi les conteneurs en √©taient arriv√©s l√†.

C'est de l√† d'o√π m'est venue l'id√©e de ce talk : d√©tailler la chronologie des grandes √©tapes de la conteneurisation, de ses d√©buts √† nos jours, et surtout expliquer POURQUOI ces √©tapes ont eu lieu.
J'ai demand√© aux membres de la communaut√© s'ils trouvaient le sujet int√©ressant, et les retours ont √©t√© tr√®s enthousiastes.

J'ai d√©j√† en grande partie termin√© ma recherche documentaire, ainsi que l'√©tude associ√©e.  
J'ai √©galement d√©j√† donn√© un talk √† ma soci√©t√© sur une premi√®re partie de la chronologie, et ai obtenu de bons retours (le sujet a r√©ellement int√©ress√© les personnes pr√©sentes).

Celles et ceux qui le souhaitent peuvent d√©j√† jeter un oeil √† la chronologie que je compte pr√©senter dans mes notes sur GitHub :  
https://github.com/Ardemius/history-of-containerization/blob/main/history-of-containerization-notes.adoc#8-frise-temporelle-r%C3%A9duite  

J'ai derni√®rement compl√©t√© la chronologie avec les modules WebAssembly, et ai des pr√©sentations blanches de pr√©vues d√©but 2023 pour peaufiner le tout.

Ma pr√©paration du talk est accessible sur GitHub : https://github.com/Ardemius/history-of-containerization  
Je suis preneur de toutes vos questions et retours pour approfondir le sujet üôÇ 
----


















== RESSOURCES

* A : https://www.tutorialworks.com/difference-docker-containerd-runc-crio-oci/ : excellente ressource
    ** Jeter un oeil au site parent, qui est vraiment tr√®s bien : https://www.tutorialworks.com/
    ** Attention, les sch√©mas ne font pas suffisamment appara√Ætre le *Docker Daemon* (*dockerd*) selon moi

* B : https://blog.engineering.publicissapient.fr/2019/12/23/docker-est-mort-vive-docker/
    ** Dans les 1eres minutes, le Docker Daemon est mieux pr√©sent√©
        *** reprendre le sch√©ma en 2:01, il est complet AVEC le docker daemon
            **** pour un sch√©ma complet, voir √©galement : http://sysblog.informatique.univ-paris-diderot.fr/wp-content/uploads/2020/03/Docker-2.3.png
        *** on pourrait √©galement faire appara√Ætre le *docker registry* sur le sch√©ma

    ** Tr√®s bonne pr√©sentation des diff√©rents √©l√©ments de "Docker", qui est un fork de *Moby*
        *** https://mobyproject.org/ : Moby is an open framework created by Docker to assemble specialized container systems without reinventing the wheel.
            **** Moby permet de pratiquer avec la plomberie de Docker "Docker internals", il n'est pas conseill√© si l'on souhaite simplement un moyen simple et rapide de lancer des containers

* Sch√©ma de Docker en 2019 (r√©cent) : https://www.codetd.com/en/article/6502770
    ** montre les 3 parties de *Docker engine*, √† savoir : Docker Daemon (dockerd), ContainerD, RunC
        *** NON ! Pr√©f√©rer l'explication fournie plus bas : +
        Docker Engine = Docker Server (impl√©ment√© √† l'aide de dockerd, qui lui m√™me utilise containerd, qui lui m√™me utilise runc) + API + CLI
        *** Docker Engine est qualifi√© de *container runtime* par Docker m√™me (https://www.docker.com/products/container-runtime) +
        Je donne cette pr√©cision car en parlant de container runtimes, on parle plut√¥t de containerd et runc
    ** pour des d√©finitions de *ContainerD* et *RunC*, voir https://jfrog.com/knowledge-base/the-basics-7-alternatives-to-docker-all-in-one-solutions-and-standalone-container-tools/
        *** voir √©galement https://docs.docker.com/engine/api/, o√π il est √©crit : +
            "Docker provides an API for interacting with the Docker daemon (called the Docker Engine API), as well as SDKs for Go and Python"
        *** NON ! Plus clair, site m√™me de Docker : https://docs.docker.com/engine/ : 
+
----
Docker Engine acts as a client-server application with:

- A server with a long-running daemon process dockerd.
- APIs which specify interfaces that programs can use to talk to and instruct the Docker daemon.
- A command line interface (CLI) client docker.
----
            **** A l'aide de cette derni√®re explication, on se rend compte que *Docker Engine* regroupe en fait la *CLI*, la Docker Engine *API* et le *Docker daemon*. +
            Ce dernier est peut-√™tre consid√©r√© ici comme *"englobant" containerd et runc*, √©tant donn√© que le *sch√©ma d'architecture* https://docs.docker.com/get-started/overview/#docker-architecture montre le docker daemon en lien avec la gestion des images, elle-m√™me li√©e aux containers
            **** concernant la *Docker Engine API* permettant l'interaction avec le Docker Daemon, voir https://docs.docker.com/engine/api/

    ** autre bon sch√©ma : https://www.aquasec.com/cloud-native-academy/docker-container/docker-architecture/ +
    Ce dernier indique √©galement que le Docker Engine englobe la CLI, l'API de comm avec le docker daemon, et le docker daemon lui-m√™me +
    PAR CONTRE, est-ce toujours totalement d'actualit√© ? Aucune mention √† runc et containerd, ce qui me pose un petit probl√®me...
        *** OUI, c'est bien toujours d'actualit√©. Vu plus bas, le docker server (impl√©ment√© √† l'aide de docker daemon) contient bien / utilise bien containerd et runc.
    ** r√©ponse finale ici : https://www.studytrails.com/2018/12/04/docker-architecture-engine-containerd-runc/ +
    *Docker Engine* est bien compos√© de : 
        *** *Docker Server*, qui est impl√©ment√© √† l'aide de *docker daemon (dockerd)*, et qui est responsable de la cr√©ation des images, containers, networks et volumes
            **** Et on consid√®re que le *Docker Server contient containerd et runc*
        *** a *RESTFul API* to talk to the docker server -> donc une API pour parler √† dockerd, c'est √† dire *Docker Engine API*
        *** une *CLI* (the docker command)
    
    ** *dockerd* is the thing that helps you *work with volumes*, *networking* or even *orchestration*. +
    And of course it *can launch containers* or *manage images* as well, *but containerd is listening on linux socket* and this is *just translated to calls to its GRPC API*. +
    see https://alenkacz.medium.com/whats-the-difference-between-runc-containerd-docker-3fc8f79d4d6e

    ** Une bonne comparaison, rapide et efficace entre Docker et Kubernetes : (https://www.threatstack.com/blog/diving-deeper-into-runtimes-kubernetes-cri-and-shims) +
    "*Docker* is a technology for automating the process of deploying containers. *Kubernetes* is orchestration software that gives us an API to manage how the containers will run." +
    "In a broad sense, Docker runs on nodes, and Kubernetes runs clusters of nodes. To run containers in pods, Kubernetes uses runtimes. Considering what we know about runtimes and how they are defined, Docker can be considered a runtime for Kubernetes, and is a high-level runtime as defined in our last post."

    ** On pourrait √©galement d√©finir Docker tr√®s simplement ainsi : *Docker allows to run containerized apps*
        *** Au final, les composants de Docker ont pour but de : *build des images*, et *run des containers*
    ** Une autre tr√®s bonne comparaison entre Kubernetes et Docker, Docker Composer et Docker Swarm : https://dzone.com/articles/kubernetes-vs-docker-differences-explained
        *** "Docker, which is the container engine solution, its container orchestration solution Docker Compose, and Docker Swarm, which is a cluster-container orchestration solution."
        *** Kubernetes, the alternative cluster-container solution
        *** *Docker Compose* : Managing multi-containerized applications on the same host is a complicated and time-consuming task. Docker Compose, the orchestration tool for *a single host*, manages multi-containerized applications defined on one host using the Compose file format. 
        *** *Docker Swarm* : Developers can design an application to run on *multiple containers on different hosts*, which creates the need for an orchestration solution for a cluster of containers across different hosts. For this reason, Docker Inc. introduced Docker Swarm.
        *** Kubernetes is more widely used than Swarm in large environments because it provides high availability, load balancing, scheduling, and monitoring to provide an always-on, reliable, and robust solution.
        *** Une TRES BONNE DEFINITION de ce que sont Docker, Docker Composer et Docker Swarm, √† quoi ils servent :
        {lb}
        "Docker is an open-source platform to package and *run applications in standard containers* that can run across different platforms in the same behavior. With Docker, *containerized applications are isolated from the host*, which offers the flexibility of delivering applications to any platform running any OS. Furthermore, the Docker engine manages containers and allows them to run simultaneously on the same host.""
        {lb}
        Due to the client-server architecture, Docker consists of client- and server-side components (*Docker client* and *Docker daemon*). The client and the daemon (*Dockerd*) can run on the same system, or you can connect the client to a remote daemon. *The daemon processes the API requests sent by the client* in addition to managing the other Docker objects (containers, networks, volumes, images, etc.).
        {lb}
        *Docker Desktop is the installer of Docker client and daemon* and includes other components like Docker Compose, Docker CLI (Command Line Interface), and more. It can be installed on different platforms: Windows, Linux, and macOS.
        {lb}
        Developers can design an application to run on multiple containers on the same host, which creates *the need to manage multiple containers at the same time*. For this reason, Docker Inc. introduced *Docker Compose*. Docker vs Docker Compose can be summarized as follows: Docker can manage a container, while Compose can manage multiple containers *on one host*.
        {lb}
        *Docker Swarm* or Docker in Swarm mode is *a cluster of Docker engines* that can be enabled after installing Docker. Swarm allows *managing multiple containers on different hosts*, unlike Compose, which allows managing multiple containers on the same host only.

* dockerd vs containerd vs runc : https://stackoverflow.com/questions/46649592/dockerd-vs-docker-containerd-vs-docker-runc-vs-docker-containerd-ctr-vs-docker-c
    ** on y trouve aussi une bonne explication sur *shim* : +
    "(docker-)containerd-shim - After runC actually runs the container, it exits (allowing us to not have any long-running processes responsible for our containers). The shim is the component which sits between containerd and runc to facilitate this."

    ** toujours concernant shim (*docker-containerd-shim*), voir pour une bonne explication : https://www.threatstack.com/blog/diving-deeper-into-runtimes-kubernetes-cri-and-shims +
    Le point essentiel de shim est de permettre "It allows for *daemon-less containers*." +
    "It basically sits as the parent of the container‚Äôs processes to facilitate communications, and eliminates the long running runtime processes for containers." +
    "The processes of the *shim and the container* are bound tightly; however, they are *totally separated from the process of the container manager*" +
    "Shim allows a runtime (runC) to exit after the container is started. Without this we would still be subject to long runtime processes."
        *** cet article d√©crit √©galement tr√®s bien Kubernetes et Docker, et les liens entre Kubelet, impl√©mentation de CRI (CRI-O) et un low-level container runtime (tr√®s souvent runc)
    ** autre bon article sur le sujet : https://alenkacz.medium.com/whats-the-difference-between-runc-containerd-docker-3fc8f79d4d6e
        *** *containerd-shim* is the *parent process of every container started* and it *also allows daemon-less containers* (meaning you can upgrade docker daemon without restarting all your containers, which was a big pain)
    ** voir √©galement https://oziie.medium.com/something-missed-history-of-container-technology-e978f202464a :
        *** It provides container operation by using runC. It also provides a ‚Äú*Daemonless container*‚Äù environment. This means that there is no need for a long-running runtime process for containers. There are 2 benefits of running a Daemonless container :
            **** *runC* stops after container starts and it doesn‚Äôt have to work during the working container process.
            **** *containerd-shim* :  It keeps file information such as stdin (standard input), stdout (standard output), stderr (standard error), even if Docker or containerd becomes inoperable for any reason.

    ** *dockershim* est √©galement tr√®s bien expliqu√© dans https://www.tutorialworks.com/difference-docker-containerd-runc-crio-oci/ : +
    "In tech terms, a shim is a component in a software system, which acts as a *bridge between different APIs*, or as a compatibility layer. A shim is sometimes added when you want to use a third-party component, but you need a little bit of glue code to make it work."

* autre *FANTASTIQUE ressource*, la s√©rie d'articles de *Ian Lewis* (2017/12) : https://www.ianlewis.org/en/container-runtimes-part-1-introduction-container-r
    ** en fait, toutes les diff√©rentes facettes de l'√©cosyst√®me des containers y sont pr√©sent√©es (docker, dockerd, containerd, runc)
    ** et une fois lu, voir √©galement https://alenkacz.medium.com/whats-the-difference-between-runc-containerd-docker-3fc8f79d4d6e, qui cite la s√©rie d'articles de Ian Lewis

* pour une explication de ce qui a amen√© aux containers, avec les *namespaces*, les *cgroups* (control groups), l'isolation des appels (*seccomp-bpf*), et finalement les "containers Docker", voir l'excellent article https://jvns.ca/blog/2016/10/10/what-even-is-a-container/
    ** Docker a fourni un wrapping simple et facile d'utilisation de ces fonctionnalit√©s du kernel Linux (et en a √©galement apport√© d'autres √©galement)
    ** Regarder absolument le super Zine "How Containers work" de *Julia Evans* (2020) : https://wizardzines.gumroad.com/l/containers-zine/buyonegiveone / https://jvns.ca/blog/2020/04/27/new-zine-how-containers-work/
        *** Ce Zine contient une description sympa des *container Kernel features* : 
            **** *pivot_root* : set a process's root directory to a directory with the contents of the container image
                ***** difference between pivot_root and *chroot* : chroot is easy to escape from if you're root and pivot root isn't +
                -> so containers use pivot_root instead of chroot
            **** *cgroups* : limit memory / CPU usage for a group of processes
            **** *namespaces* : allow processes to have their own network / PIDs / users / hostname / mounts / and more !
            **** *seccomp-bpf* : security: prevent dangerous system calls
                ***** seccomp means "secure computing"
                ***** bpf, pour Berkeley Packet Filter, est une extension de seccomp
            **** *capabilities* : security: avoid giving root access +
            Capabilities allow to reduce the privileges of an active process
            **** *overlay filesystems* : optimization to reduce disk space used by containers which are using the same image
            **** quand on utilise *toutes les fonctionnalit√©s pr√©c√©dentes*, on a un *container*
            **** Et un GROS reminder : *A container is a group of processes*

    ** LCC (Les Cast Codeurs) 270 : interview de *Nicolas De Loof* sur Docker et Docker Compose 
        *** D√©finition de Docker : "Docker est un moyen de lancer des applications, des process, mais on va prendre le process Linux, celui que tu veux faire tourner sur ta machine de PROD, et on va te donner un moyen simple de le faire tourner chez toi tout pareil"
            **** L'id√©e c'est vraiment, cf Nicolas, "moyen de lancer des applications"

    ** Cf wikipedia (https://en.wikipedia.org/wiki/Cgroups), *cgroups* : +
    "cgroups (abbreviated from control groups) is a Linux kernel feature that limits, accounts for, and isolates the resource usage (CPU, memory, disk I/O, network, etc.) of a collection of processes."
        *** la vid√©o https://www.youtube.com/watch?v=sK5i-N34im8[cgroups, namespaces, and beyond: what are containers made from?] de J√©r√¥me PETAZZONI (Docker) explique en d√©tails les diff√©rentes fonctionnalit√©s des *cgroups*, *diff√©rents types de namespaces*. +
        ATTENTION ! Elle date de 2015 !
            **** Il est √©galement question des *container runtimes* qui sont bas√©s sur les cgroups et les namespaces. +
            Exemples de container runtimes bas√©s sur des namespaces et des cgroups : 
                ***** *LXC* (Linux Containers) : easy for sysadmins / OPS, hard for devs (requires significant elbow grease)
                ***** *systemd-nspawn*
                ***** *Docker*
                ***** *rkt*
                ***** *runC*
                ***** All those container runtimes use the same kernel features (at that time, 2015 ?)
            **** et maintenant des container runtimes qui ne sont PAS bas√©s sur les namespaces et les cgroups : 
                **** *OpenVZ* : by example Travis CI gives you root in OpenVZ
                **** *Jails* / *Zones*
            **** la vid√©o de J√©r√¥me se termine par un live demo d'une cr√©ation de container *√† la main* (un d√©but de container)
            **** autre tr√®s bonne vid√©o de container compl√®tement cr√©√© √† la main en Go, https://www.youtube.com/watch?v=Utf-A4rODH8, de *Liz RICE* (2016/10)
                **** Voir √©galement le Gist en GO de *Julien Friedman* dont Liz s'est inspir√©e : https://gist.github.com/julz/c0017fa7a40de0543001 (au final on build un container en ~55 lignes de Go)

        *** le travail sur les *cgroups* a commenc√© en 2006 chez Google sous le nom "process containers", avant d'√™tre renomm√© en "control groups" pour √©viter toute confusion avec le terme "container" dans un contexte Linux Kernel.
            **** cf Wikipedia (https://en.wikipedia.org/wiki/Cgroups) : +
            "A control group (abbreviated as cgroup) is a *collection of processes that are bound by the same criteria* and associated with a set of parameters or limits. These groups can be *hierarchical*, meaning that *each group inherits limits from its parent group*. The kernel provides access to multiple controllers (also called subsystems) through the cgroup interface;[2] for example, the "memory" controller limits memory use, "cpuacct" accounts CPU usage, etc."

        *** Development and maintenance of cgroups was then taken over by Tejun Heo. Tejun Heo redesigned and rewrote cgroups. This rewrite is now called version 2, the documentation of *cgroups v2* first appeared in Linux kernel 4.5 released on 14 March 2016. +
        Unlike v1, cgroups v2 has only a *single process hierarchy* and discriminates between processes, not threads.

    ** *namespaces* are a Linux feature allowing your processes to be separated from the other processes on the computer. +
    You can have PID namespace, networking namespace, mount namespace. +
    Namespaces can be creates using the `unshare` program.

    ** Pour les *dates* de cr√©ation des *cgroups* et *namespaces*, voir cet article : https://www.silicon.co.uk/software/open-source/linux-kernel-cgroups-namespaces-containers-186240

        *** *cgroups* were originally developed by Paul Menage and Rohit Seth of Google, and their first features were merged into *Linux 2.6.24* (*2008/01*) +
        Cf Wikipedia (https://en.wikipedia.org/wiki/Cgroups) : 
        "Engineers at Google (primarily *Paul Menage* and *Rohit Seth*) *started the work on this feature in 2006* under the name "*process containers*".[1] In late 2007, the nomenclature changed to "control groups" to avoid confusion caused by multiple meanings of the term "container" in the Linux kernel context, and the control groups functionality was merged into the Linux kernel mainline in *kernel version 2.6.24*, which was *released in January 2008*."

        *** *user namespaces* were originally developed by *Eric Biederman*, and the final major namespace was merged into *Linux 3.8*. +
        Cf Wikipedia (https://en.wikipedia.org/wiki/Linux_namespaces) : 
        "The Linux Namespaces originated in *2002 in the 2.4.19 kernel* (2002/08/03) with work on the *mount namespace* kind. Additional namespaces were added beginning in 2006[2] and continuing into the future. +
        Adequate containers support functionality was finished in kernel *version 3.8* with the *introduction of User namespaces*."
            **** Et l'info tr√®s int√©ressante est ici : ce sont les user namespaces, introduit avec le kernel 3.8 de Linux qui ont chang√© la donne, et dont Solomon Hykes dit en 2013 (voir la conf ci-dessous, √† 16:19) que, √ßa y est, "les namespaces marchent maintenant".
            **** https://kernelnewbies.org/Linux_3.8 : "*Linux 3.8* was released on Mon, *18 Feb 2013*."

Une bonne d√©finition d'un *container runtime* : +
.https://www.quora.com/What-is-container-runtime-in-Kubernetes/answer/John-Sundarraj
----
A container runtime is a library or software which has the ability to create, deploy and manage containers on its own. Basically, container runtimes are responsible for container lifecycle. It provides simple API layer to create, deploy and manage containers.
----

* *D√©finition d'un runtime "classique" :* 

    ** https://fr.wikipedia.org/wiki/Environnement_d%27ex%C3%A9cution +
    Un *environnement d'ex√©cution* ou *runtime* est un *logiciel responsable de l'ex√©cution des programmes informatiques* √©crits dans un langage de programmation donn√©. Un runtime offre des services d'ex√©cution de programmes tels que les entr√©es-sorties, l'arr√™t des processus, l'utilisation des services du syst√®me d'exploitation, le traitement des erreurs de calcul, la g√©n√©ration d'√©v√©nements, l'utilisation de services offerts dans un autre langage de programmation, le d√©bogage, le profilage et le ramasse-miette. +
    Contrairement √† un logiciel de d√©veloppement permettant de programmer et d√©velopper son application, *un runtime ne permet QUE l'ex√©cution d'un programme*. Un runtime peut √™tre vu comme une machine virtuelle : de la m√™me mani√®re qu'un code natif est ex√©cut√© par le processeur, un code objet est ex√©cut√© par le runtime. Le runtime sert alors √† ex√©cuter du code objet en mettant le code natif ad hoc √† disposition du processeur pour ex√©cution

    ** On peut reparler ici des diff√©rences entre JRE et JDK (et JVM) : https://www.digitalocean.com/community/tutorials/difference-jdk-vs-jre-vs-jvm
        *** *JRE* is the implementation of JVM. It provides *a platform to execute java programs*. JRE consists of JVM, Java binaries, and other classes to execute any program successfully.

    ** ChatGPT : 
        *** In software development, a runtime (also called runtime environment or runtime system) is a software layer that provides a platform or framework for running and executing code. It is responsible for managing the execution of code, including loading, interpreting, and executing program instructions, as well as providing the necessary support for accessing system resources and external libraries. +
        A runtime is typically associated with a specific programming language or technology, and provides the necessary environment for executing code written in that language or technology. For example, a Java runtime environment (JRE) provides the platform for running Java applications, while a Node.js runtime provides the environment for running JavaScript code on a server.


* *Docker was released for the 1st time the 2013/03/20*

* *Why we built Docker ?* by Solomon Hykes (foundateur de dotCloud √† l'√©poque, puis Docker) : https://www.youtube.com/watch?v=3N3n9FzebAA (2013/06/07, EXCELLENTE conf, toujours d'actualit√©).
Le talk a √©t√© donn√© √† la conf√©rence dotScale 2013, juste apr√®s la 1ere publication de Docker.
* Pour d'autres explications par Solomon sur la cr√©ation de Docker et ses d√©buts, voir : https://www.youtube.com/watch?v=KF9Awj74dMw

La grande raison de l'√©poque : *shipping software from A to B, reliably and automatically*
    ** It has to behave the same way on both machine, and this with technological stack behind applications being more and more complex
    ** and your shipping place can be different depending on developer environment, servers, etc etc. (a lot of possible combinations that result finally in different environments)
    ** 08:39 (https://youtu.be/3N3n9FzebAA?t=519), to avoid all those shipping problems in the (shipping) industry, one day in the 1950s, people agreed on using a standard box, with standard dimensions, weight, way to open the doors, etc etc. AND it resulted with the creation on the container we know today. +
    This "ugly box" allows *separation of concerns* : je cr√©e un outil / soft, je veux le shipper, je le mets dans le container, et ma responsabilit√© pour le shipping s'arr√™te l√†. Je ne m'int√©resse QU'A mon produit, et PAS au container. +
    De la m√™me fa√ßon, pour les personnes en charge du shipping, elles n'ont pas besoin de s'int√©resser √† ce qu'il y a dans le container : elles savent que le container a une taille, un poids, des dimensions donn√©es, et que TOUS ces containers peuvent √™tre utilis√©s via les m√™mes moyens standards.
        *** ces "bo√Ætes" ont r√©ellement chang√© le monde √† cette √©poque : AVANT, c'√©tait une gal√®re de livrer du fait de toutes les combinaisons possibles de packaging des produits √† livrer.
            **** pour info, article sur l'histoire des shipping containers : https://mccontainers.com/blog/the-history-of-containers/ +
            "A couple of ISO standards were set to determine terminology, dimensions, classifications, identifiers and so on. Thanks to these standards we nowadays have the 20‚Äô and 40‚Äô containers, the 20‚Äô container (Twenty-foot Equivalent Unit, or TEU) being the standard volume."
            **** la standardisation des containers dans il est fait mention ci-dessus arriva en 1967 (https://fr.wikipedia.org/wiki/Conteneur)
        *** We finally wanted to do the same in our IT world for our own shipping needs.
        
    ** Avant, on avait bien d√©j√† des archives comme des jars, rvms, etc. MAIS ce *sandboxing n'√©tait pas complet*

    ** Il y avait bien *les VMs* : cette fois-ci, on a l'appli et on livre finalement toute la machine avec. On est maintenant s√ªr qu'on a bien le m√™me "contexte" √† chaque livraison.
        *** C'est la seule fa√ßon de s'assurer de share software in a truly reliable and repeatable way : to *ship the WHOLE system with the application* (because, truly, the system is PART OF the application)
        *** *le souci* avec les VMs est que l'*on ship trop de choses* : hard drives, network interfaces, le total de RAM, le type de processeur, etc. 
            **** Et il ne faut pas que ce soit le d√©veloppeur qui d√©cide comment l'on va faire fonctionner son application sur toutes les infrastructures possibles, ce n'est pas son r√¥le (on brise la "separation of concerns" pr√©c√©dente)
                ***** Pour reprendre l'analogie avec les "vrais" containers, cela reviendrait √† imposer le mod√®le de grue avec lequel les d√©charger, et le mod√®le de bateau avec lequel les transporter.
                ***** In our IT world, the infrastructure provider is NOT free to make those choices just because you give them to him with your application.
        *** autre souci, *les VMs sont volumineuses* : est-ce facile d'en faire tourner 10 en parall√®le ? Non.
            **** En fait, les VMs ont certains des "d√©fauts" des machines classiques : elles mettent du temps √† booter, consomment beaucoup de RAM, etc etc. Pas le plus pratique pour un dev dans son travail quotidien.
        
    ** Pour avoir le *meilleur des 2 mondes*, archives et VMs, il faudrait : 
        *** Sandbox the entire system
        *** without machine details
        *** and without the performance hit
        *** Et tout ceci est rendu *possible gr√¢ce aux fonctionnalit√©s du kernel Linux*, tout particuli√®rement le *namespacing* qui a √©t√© rendu "r√©ellement" fonctionnel derni√®rement
            **** avec ce nouveau namespacing (2013), on peut maintenant isoler n'importe quel process des autres, et faire "croire" √† ce process qu'il a sa propre VM (alors qu'il ne l'a pas)
                ***** mais utiliser ces fonctionnalit√©s d'isolation du kernel Linux n'est pas √©vident, ce qu'il manque est une fa√ßon standard de les utiliser (un container standard pour cela) : c'est ce qu'est Docker +
                Docker est avant tout : 
                ***** un standard container format
                ***** simple tools that enable people running the infrastructure to take that container (without knowing what is inside), and then run it

    ** Donc, pour r√©sumer, on a fait Docker dans le but de *shipper*. +
    Il fallait donc que Docker ne soit pas "trop inf√¢me" √† utiliser.
        *** on avait d√©j√† les Linux Containers (LXC) avant, mais ce type de Operating System (OS) Containers n'est pas des plus simples √† utiliser. Ces derniers sont plut√¥t √† destination des sysadmin, pas des √©quipes qui "ship"


* https://www.ianlewis.org/en/container-runtimes-part-1-introduction-container-r

    ** developers who want to run apps in containers will need more than just the features that low-level runtimes provide, they need APIs and features around image formats, image management, and sharing images, which are provided by high-level runtimes.
    ** Developers who implement low-level runtimes will say that higher level runtimes like *containerd* and *cri-o* are not actually container runtimes, as from their perspective they outsource the implementation of running a container to *runc*.

* https://www.ianlewis.org/en/container-runtimes-part-2-anatomy-low-level-contai : *LOW LEVEL CONTAINER RUNTIME*

    ** le concept de *low-level container runtime* est mis en avant
    ** Low-level runtimes have a limited feature set and typically perform the low-level tasks for *running a container* (ex : runC)
        ** low-level runtimes are responsible for the mechanics of actually running a container
        ** raison pour laquelle de nombreux low-level container runtime s'appellent "run<quelque chose>"
    ** *Namespaces* let you virtualize system resources, like the file system or networking for each container.
        *** Namespaces are "what you can see"
    ** *cgroups* provide a way to limit the amount of resources, such as CPU and memory, that each container can use.
        *** control groups are "what you can use"
    ** At their core, low-level container runtimes are responsible for setting up these namespaces and cgroups for containers, and then running commands inside those namespaces and cgroups.

    ** Examples of low-level container runtimes : 

        *** *lmctfy* (Let Me Contain That For You) : projet by Google, based on the internal container runtime that *Borg* uses. +
        It supports container hierarchies that use cgroups hierarchies via the container names (a root container called "busybox" could create sub-containers under the name "busybox/sub1" or "busybox/sub2") +
        While lmctfy provides some interesting features and ideas, other runtimes were more usable so Google decided it would be better for the community to focus worked on Docker's "libcontainer" instead of lmctfy.

            *** *libcontainer* : voir http://igm.univ-mlv.fr/~dr/XPOSE2014/Docker/fonctionnement.html +
            "Libcontainer est une biblioth√®que √©crite en Go pour la cr√©ation de conteneurs avec des espaces de noms, les groupes de contr√¥le, les capacit√©s et les contr√¥les d'acc√®s du syst√®me de fichiers. Cette librairie a √©t√© d√©velopp√©e pour faire le travail de lxc tout en simplifiant l'installation de docker. Elle vous permet de g√©rer le cycle de vie du conteneur, effectuer des op√©rations suppl√©mentaires apr√®s que le container soit cr√©√©."
            *** *Borg* is Google's cluster manager that runs hundreds of thousands of jobs, from many thousands of different applications, across a number of clusters each with up to tens of thousands of machines. +
            See https://research.google/pubs/pub43438/ for more details
            *** https://faun.pub/the-missing-introduction-to-containerization-de1fbb73efc5 : The libcontainer repository has been archived now. +
            Voir le repo https://github.com/docker-archive/libcontainer, et l'article de blog http://blog.docker.com/2015/06/open-container-project-foundation/. +
            Ce dernier, datant du 2015/06/15 annonce la cr√©ation de l'Open Container Projet (OCP, plus tard rebaptis√© OCI) et la donation de *runc* par Docker √† ce projet. +
            Il y est expliqu√© que *libcontainer* a √©t√© la base de *runc* : +
            "Docker has taken the entire contents of the libcontainer project, including [nsinit], and all modifications needed to make it run independently of Docker,  and donated it to this effort. This codebase, called runC, can be found at github/opencontainers/runc. libcontainer will cease to operate as a separate project."

        *** *runC* : most widely used container runtime
            **** originally developed as part of Docker, then extracted as a separate tool and library.
                ***** So runC is the low-level runtime that was broken off from Docker.
            **** runC implements the *OCI runtime spec* (Open Container Initiative)
                ***** Pour plus d√©tails, lire l'OCI runtime spec : https://github.com/opencontainers/runtime-spec
            **** https://www.tutorialworks.com/difference-docker-containerd-runc-crio-oci/ : runc is responsible for creating and running the container process.
            **** pour une tr√®s bonne ressource sur runc, voir https://www.agaetis.fr/blogpost/les-runtimes-oci
                ***** il est question de *runc* et de *crun* comme des "native runtimes", auxquels on va comparer les "*sandbox runtimes*" que *gVisor*, *Nabla containers* et *Kata containers* +
                Ces derniers sont pr√©sent√©s comme "limitant les interactions entre le conteneur et le kernel pour r√©duire au maximum la surface d‚Äôattaque, permettant ainsi une plus grande isolation. Dans cette cat√©gorie nous allons voir gVisor,  Nabla containers et Kata containers." Donc un accent mis sur la *s√©curit√©*.
                ***** concernant plus pr√©cis√©ment runc et crun, il est expliqu√© que : +
                "Ensuite viens crun, un runtime en C d√©velopp√© par Red Hat. Il est suppos√© plus performant que runc et est le runtime par d√©faut de Podman. M√™me si crun a support√© *cgroups v2* avant runc, ce dernier a rattrap√© son retard depuis."

        *** *rkt* (CoreOS *Rocket*):
            **** developed by CoreOS, which was later acquired by Red Hat
            **** provides all features provided by low-level container runtimes, PLUS some high-level ones
            **** As said by Docker : "rkt is CoreOS‚Äôs pod-native container engine"
            **** *projet ended / discontinued on 2020/02* and is not maintained anymore.
                ***** for more details on the reasons, see https://github.com/rkt/rkt/issues/4024 +
                The main ones seem to be : 
                ***** the previous development team at CoreOS got dismantled, and post Red Hat acquisition there are no plan to push the development forward
                ***** no more have development plans for rkt (from the new development team)
                ***** a declining engagement from the community

* https://www.ianlewis.org/en/container-runtimes-part-3-high-level-runtimes : *HIGH LEVEL CONTAINER RUNTIMES*

    ** *high-level runtimes* are responsible for *transport and management of container images*, unpacking the image, and *passing off to the low-level runtime* to *run the container*.
    ** Typically, high-level runtimes provide a *daemon* application and an *API* that remote applications can use to logically run containers and monitor them but they sit on top of and *delegate to low-level runtimes* or other high-level runtimes for the actual work. +
    High-level runtimes can also provide *features* that sound low-level, but are *used across individual containers on a machine*. For example, one feature might be the management of network namespaces, and allowing containers to join another container's network namespace.
    ** Exemples of high-level container runtime : 

        *** *Docker*
            **** Originally built as a monolithic daemon, *dockerd*, and the *docker client (Docker CLI)* application. +
            The daemon provided most of the logic of building containers, managing the images, and running containers, along with an API. +
            The command line client could be run to send commands and to get information from the daemon.
            **** It really was *the first* popular runtime to incorporate all of the features needed during the lifecycle of building and running containers, hence its success.
            **** A la base Docker faisait tout, les low et les high level features, mais cela a depuis (v1.11) √©t√© scind√© en diff√©rentes briques, dont containerd et runC. +
            Docker se compose donc maintenant (2021) de docker CLI, dockerd, docker-containerd et docker-runc (les 2 derniers √©tant simplement des versions packag√©es de containerd et runc) ainsi que la Docker Engine API
                ***** *dockerd* provides features such as *building images*, and dockerd uses docker-containerd to provide features such as image management and running containers. For instance, Docker's build step is actually just some logic that interprets a Dockerfile, runs the necessary commands in a container using containerd, and *saves the resulting container file system as an image*.

        *** *ContainerD* 
            **** final "d" for daemon, containerd is a daemon
            **** is the high-level runtime that was split off from Docker.
            **** implements downloading images, managing them, and running containers from images. +
            When it needs to *run a container* it unpacks the image into an OCI runtime bundle and *shells out to runc* to run it.
            **** Containerd also provides an API and client application that can be used to interact with it. The *containerd command line client* is *ctr*.
            ****  In contrast with Docker, containerd is *focused solely on running containers*, so it *does NOT provide a mechanism for building containers*.
                ***** Docker was focused on end-user and developer use cases, whereas containerd is focused on operational use cases, such as running containers on servers. Tasks such as building container images are left to other tools.
                ***** traduction simple : containerd can't build images (c'est le travail du daemon dockerd par exemple)
            **** containerd is made *compliant with CRI* through its *CRI plugin* "cri-containerd" (as coming from Docker, it is NOT natively compliant with CRI which comes from Kubernetes)
                ***** see https://github.com/containerd/cri for more details

        *** *rkt*
            **** CAREFUL ! See above, *projet ended in 2020/02* !
            **** rkt is a runtime that has both low-level and high-level features
            **** rkt allows you to *build container images*, *fetch* and *manage container images* in a local repository, and *run them* all from a single command

* https://www.ianlewis.org/en/container-runtimes-part-4-kubernetes-container-run : *KUBERNETES CONTAINER RUNTIMES & CRI*

    ** *Kubernetes* runtimes are *high-level container runtimes* that support the *Container Runtime Interface* (*CRI*) (mandatory to integrate with Kubernetes)

        *** CRI was introduced in Kubernetes 1.5 and acts as a *bridge* between the *kubelet* and the *container runtime*.
            **** *kubelet* : https://kubernetes.io/docs/concepts/overview/components/#kubelet (or https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/) +
            "An *agent* that runs on each node in the cluster. It *makes sure that containers are running in a Pod*. +
            The kubelet takes a set of PodSpecs that are provided through various mechanisms and ensures that the containers described in those PodSpecs are running and healthy. The *kubelet doesn't manage containers which were not created by Kubernetes*"
            **** The kubelet is responsible for managing the container workloads for its node. +
            When it comes to actually run the workload, the kubelet uses CRI to communicate with the container runtime running on that same node. +
            In this way *CRI is simply an abstraction layer* or API that allows you to switch out container runtime implementations instead of having them built into the kubelet.
                ***** *CRI √©vite donc de coupler kubelet avec le container runtime* (logique, c'est une interface)

    ** The runtime is expected to handle the *management of images* and to *support Kubernetes pods*, as well as *manage the individual containers*. As a consequence, a Kubernetes runtime must be a high-level runtime per our definition in part 3.

    ** *containerd*
        *** implements CRI as a plugin, which is enabled by default
        *** it *supports multiple low-level runtimes* via something called a "runtime handler" starting in version 1.2. The runtime handler is passed via a field in CRI and based on that runtime handler containerd runs an application called a *shim* to start the container. This can be used to run containers using low-level runtimes other than runc, like *gVisor*, *Kata Containers*, or *Nabla Containers*.
            **** *gVisor*, *Kata Containers* et *Nabla Containers* sont souvent compar√©s car mettant tous en avant une *isolation tr√®s forte vis √† vis de l'host*
            **** https://alenkacz.medium.com/whats-the-difference-between-runc-containerd-docker-3fc8f79d4d6e : +
            kata containers "is claiming to be all the isolation you love from VMs but that can be easily plugged into all the tooling we have around containers. This means you can spin up these VMs (or kata containers if you wish) through docker or Kubernetes."

    ** *Docker*
        *** Nowadays, Docker itself isn't necessary to support CRI, which is done through the use of containerd

    ** *cri-o*
        *** cri-o is a lightweight *CRI runtime* made as a *Kubernetes specific high-level runtime*.
        *** It supports the management of OCI compatible images and pulls from any OCI compatible image registry.
        *** It *supports runc* and *Clear Containers* as low-level runtimes. +
        It supports other OCI compatible low-level runtimes in theory, but relies on compatibility with the runc OCI command line interface, so in practice it isn't as flexible as containerd's shim API.
        *** *CRI-O* was created to provide a lightweight runtime for Kubernetes which adds an *abstraction layer between the cluster and the runtime that allows for various OCI runtime technologies* (https://developers.redhat.com/blog/2018/11/20/buildah-podman-containers-without-daemons#)

    ** the *CRI Specification*
        *** CRI is a *protocol buffers* and *gRPC* API.
        *** CRI *defines several remote procedure calls* (RPCs) and *message types*. The RPCs are for operations like "pull image" (ImageService.PullImage), "create pod" (RuntimeService.RunPodSandbox), "create container" (RuntimeService.CreateContainer), "start container" (RuntimeService.StartContainer), "stop container" (RuntimeService.StopContainer), etc.
        *** We can interact with a CRI runtime directly using the crictl tool. crictl lets us send gRPC messages to a CRI runtime directly from the command line.

*OCI* : *Image spec* ET *Runtime spec*

    * https://fr.wikipedia.org/wiki/Open_Container_Initiative : L'*Open Container Initiative* (OCI) est un projet de la Fondation Linux visant √† *concevoir des normes ouvertes* pour la virtualisation au niveau du syst√®me d'exploitation, surtout les *conteneurs Linux*. Il existe actuellement deux sp√©cifications en cours de d√©veloppement et en cours d'utilisation: la sp√©cification d'ex√©cution (runtime-spec) et la sp√©cification d'image (image-spec).

    * https://www.docker.com/blog/oci-release-of-v1-0-runtime-and-image-format-specifications/ (TRES BONNE RESSOURCE) : +
    "the *Open Container Project* (OCP) was formed to create a set of container standards and was launched under the auspices of the Linux Foundation in *June 2015 at DockerCon*. It became the Open Container Initiative (*OCI*) as the project evolved that Summer."
        ** cet article du blog de Docker, √©crit par Patrick CHANEZON le 19/07/2017, contient √©galement le *d√©tail de toutes les contributions de Docker √† l'OCI* jusqu'√† cette date.
        ** Voici √©galement l'article du blog de Docker annon√ßant la cr√©ation de l'OCP (plus tard renomm√© OCI) : https://www.docker.com/blog/open-container-project-foundation/
            *** Docker will be donating both our base container format and runtime, runC, to this project, to help form the cornerstone for the new technology.  And, in a particularly exciting recent development, the talented people behind *appc* are now joining us as *co-founders*.
                **** Behing appc (App containers) is the people of rkt, and so CoreOS

    * https://faun.pub/docker-containerd-standalone-runtimes-heres-what-you-should-know-b834ef155426 : +
    "Formed in June 2015, the Open Container Initiative (OCI) aims to establish common standards for software containers in order to avoid a potential fragmentation and divisions inside the container ecosystem."

    * https://opencontainers.org/ : +
    "The Open Container Initiative is an open governance structure for the express purpose of *creating open industry standards around container formats and runtimes*." +
    "Established in *June 2015* by Docker and other leaders in the container industry, the OCI currently contains two specifications: the Runtime Specification (*runtime-spec*) and the Image Specification (*image-spec*). The Runtime Specification outlines how to run a ‚Äúfilesystem bundle‚Äù that is unpacked on disk. At a high-level an OCI implementation would download an OCI Image then unpack that image into an OCI Runtime filesystem bundle. At this point the OCI Runtime Bundle would be run by an OCI Runtime."

    * cf "https://www.tutorialworks.com/difference-docker-containerd-runc-crio-oci/" : the Open Container Initiative (OCI) which publishes specifications for images and containers.
        *** cf https://faun.pub/docker-containerd-standalone-runtimes-heres-what-you-should-know-b834ef155426, il est bien question de specifications pour des image-spec et runtime-spec
            **** Dans le sch√©ma de https://www.tutorialworks.com/difference-docker-containerd-runc-crio-oci/, il est expliqu√© que : +
            "OCI provides specifications for container images and running containers."

    * "https://blog.engineering.publicissapient.fr/2019/12/23/docker-est-mort-vive-docker/" voir en 2:06
    * *runc* est une impl√©mentation de la runtime-spec de l'OCI 
        ** runC a √©t√© publi√© pour la premi√®re fois en 2015/07 (https://fr.wikipedia.org/wiki/Open_Container_Initiative)
    * image-spec (OCI image spec) : https://github.com/opencontainers/image-spec
    * runtime-spec (OCI runtime spec) : https://github.com/opencontainers/runtime-spec

    * NEWS : 2023 ! Now the *OCI now contains 3 specifications* : runtime-spec, image-spec AND NOW *distribution-spec*
        ** https://opencontainers.org/
        ** https://opencontainers.org/about/overview/ : pour plusieurs d√©finitions r√©centes, concises et claires, pour les 3 sp√©cifications.
            *** *Runtime Specification* : The Runtime Specification outlines how to run a ‚Äúfilesystem bundle‚Äù that is unpacked on disk. At a high-level an OCI implementation would download an OCI Image then unpack that image into an OCI Runtime filesystem bundle. At this point the OCI Runtime Bundle would be run by an OCI Runtime.
            *** *image-spec* : The OCI Image Format contains sufficient information to launch the application on the target platform (e.g. command, arguments, environment variables, etc). This specification defines how to create an OCI Image, which will generally be done by a build system, and output an image manifest, a filesystem (layer) serialization, and an image configuration. +
            At a high level the image manifest contains metadata about the contents and dependencies of the image including the content-addressable identity of one or more filesystem serialization archives that will be unpacked to make up the final runnable filesystem. The image configuration includes information such as application arguments, environments, etc. The combination of the image manifest, image configuration, and one or more filesystem serializations is called the OCI Image.
            *** *distribution specification* : The distribution specification reached v1.0 in May 2020 (2020/05) and was introduced to OCI as an effort to standardize the API to distribute container images. However, the specification is designed generically enough to be leveraged as a distribution mechanism for any type of content.
                **** ERREUR DE DATE DANS LA DOC OFFICIELLE !!!! +
                La v1.0.0 de la 3e spec n'a √©t√© rajout√©e en 2020/05 mais en 2021/05 ! +
                Cf l'announcement de l'OCI : https://opencontainers.org/posts/announcements/2021-05-04-oci-dist-spec-v1/ +
                L'annoucement tout comme le commit date du *2021/05/05*.
                "*Reaching v1.0 means the OCI Distribution Spec is stable* and ready to serve as the baseline for the distribution of container images across platforms"
                    ***** https://github.com/opencontainers/distribution-spec/releases
                **** ChatGPT : This specification defines how container images are transferred and stored. It specifies the format of the image manifest, the metadata about the image, and the protocol for distributing and fetching images from a registry. It also defines the API for interacting with container registries. +
                The Distribution Specification of the OCI provides a common format for container image metadata and a standard protocol for interacting with container registries. This makes it easier for developers to create and share container images that can be run on any OCI-compliant runtime, while also improving the security and reliability of container image distribution.

        ** *2021/05* : The distribution specification reached v1.0 in May 2021 and was introduced to OCI as an effort to standardize the API to distribute container images. However, the specification is designed generically enough to be leveraged as a *distribution mechanism* for any type of content.
        ** https://github.com/opencontainers/distribution-spec : +
        The OCI Distribution Spec project defines an API protocol to facilitate and standardize the distribution of content.

* Attention ! Fin 2020 (d√©cembre) *deprecation de docker/docker-shim* (dockershim)
    ** oui, c'est bien confirm√© : "the Kubernetes community announced it is deprecating Docker as a container runtime after v1.20". +
    Donc, il s'agit bien de la deprecation de *docker-shim*, ET *NON* de containerd-shim, qui n'a rien √† voir sinon le "shim" dans le nom. +
    "Docker-shim was a temporary solution proposed by the Kubernetes community to add support for Docker so that it could serve as its container runtime." +
    Pour plus de d√©tails, voir : 
        *** https://kubesphere.io/blogs/dockershim-out-of-kubernetes/
        *** https://linoxide.com/docker-alternative-container-tools/
        *** https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/ (2020/12/02) : l'annonce officielle sur le blog de Kubernetes
        *** voir √©galement ce site de 2018, https://kubernetes.io/blog/2018/05/24/kubernetes-containerd-integration-goes-ga/, qui a de bons *sch√©mas faisant appara√Ætre dockershim*, ainsi que le CRI-plugin de containerd (le tout en lien avec kubelet)
            **** dockershim is "Docker's CRI implementation"
        *** et pour un sch√©ma montrant bien l'avant et l'apr√®s dockershim, voir https://medium.com/nttlabs/docker-20-10-59cc4bd59d37 (2020/12/09)

A VOIR / FACULTATIF : 

* Attention ! 2021/09, changement de licence Docker Desktop, on ne peut plus l'utiliser sur Windows en entreprise.
* Parler de Docker Desktop qui conseille maintenant de passer, avec WSL 2, aux Linux Containers ?

=== Les daemonless et rootless containers et podman

* La 1ere release sur le repo https://github.com/containers/podman/releases date du 2018/04/05

* Pour information, pourquoi podman a pour logo un groupe de phoques ("seal" en anglais) ? Parce que, justement, un groupe de phoques est appel√© "a seal POD" en anglais... ;)

* Une pr√©sentation de *Podman*, √† Devoxx France 2021 (2021/10), par Benjamin Vouillaume : https://www.youtube.com/watch?v=pUFIG2AMDhg
    ** Podman est √©crit en Go et support√© massivement par RedHat
    ** Podman utilise *crun*, runtime concurrent de *runc* (√©galement OCI), d√©velopp√© pour Podman
        *** crun semble (beaucoup) plus performant que runc
        *** et la raison d'√™tre, le pourquoi avoir eu besoin de cr√©√© *crun* sont les *cgroups v2*
        *** que permettent les cgroups v2 ? 
            **** Faire marcher les containers en *rootless*, c'est √† dire *sans que nous soyons root* pour d√©marrer nos conteneurs +
            C'est un peu la *raison d'√™tre de Podman* : fournir une interface semblable √† Docker, tout en √©tant plus s√©cure avec le rootless (*on ne d√©marre pas les containers en root*)
    ** Podman est *daemonless*, contrairement √† Docker, qui, √† partir de la 1.11, fait :
        *** systemd -> 
        *** commande Docker run qui va d√©marrer le container -> 
        *** le Docker engine qui tourne pour interpr√©ter cette commande -> 
        *** containerd qui tourne pour interpr√©ter les informations que l'Engine va lui envoyer ->
        *** qui lui-m√™me va appeler runc ->
        *** qui lui m√™me va faire tourner votre application
    ** ALORS que Podman va directement appeler crun, et il n'y a pas de daemon. +
        Donc *pas* de processus qui tourne en arri√®re plan pour g√©rer nos containers.
        *** L'int√©r√™t du daemonless est la s√©curit√©. +
        Via de l'Audit Log sur Docker, on se rend compte que tout est en root, tout passe par le daemon (dockerd), donc on ne sait pas qui a fait quoi avec le container
    ** *application container* vs *system container*
        *** *application container* : ceux qu'on utilise le plus fr√©quemment, on met 1 process dans 1 container (ce que recommande Docker)
        *** *system container* : on va d√©marrer plein de process dans un container, ce dernier √©tant au final davantage une "micro-VM" mais containeris√©e. +
        On peut faire des system container avec Docker, mais il n'a pas r√©ellement √©t√© fait pour, alors que c'est support√© par Podman. +
        Dans Podman, il est possible de d√©marrer directement systemd, le process parent d'une arborescence d'un OS, dans un container.
    ** Podman est tr√®s adapt√© √† Kubernetes. +
    Podman sait g√©rer les pods kubernetes, ce que ne sait pas faire Docker
        *** pods : plusieurs containers isol√©s mais avec des √©l√©ments communs (souvent la partie network)
        *** On va pouvoir jouer un fichier Kubernetes existant directement sur podman pour d√©marrer vos pods

* https://podman.io/ : What is Podman? Podman is a *daemonless* container engine for developing, managing, and running OCI Containers on your Linux System. Containers can either be run as root or in *rootless* mode.

=== Les tests containers

* Regarder ce que les containers peuvent faire pour les tests d'int√©gration (*Testcontaineurs*)

=== Histoire des containers

Alors, ce n'est pas une chronologie √† proprement parler, mais cet article de Baeldung d√©crit tr√®s bien les d√©buts de la containerization, avec les namespaces et les cgroups, jusqu'√† Docker : +
https://www.baeldung.com/linux/docker-containers-evolution

En fait, on trouve plus d'infos que je ne le pensais via les recherches Google "evolution of containers" et "history of containers", surtout en passant par la recherche images de Google

    ** https://www.redhat.com/en/blog/history-containers (2015/08) TRES BIEN

        *** *2000* : "jails", an early implementation of container technology, was added to FreeBSD
        *** *2001* : container technology made it to the Linux side of the house +
        "Jacques G√©linas created the VServer project, which according to the 0.0 version‚Äôs change log allowed ‚Äúrunning several general purpose Linux server on a single box with a high degree of Independence and security.‚Äù" +
        The Linux-VServer solution was the first effort on Linux to ‚Äúseparate the user-space environment into distinct units (Virtual Private Servers) in such a way that each VPS looks and feels like a real server to the processes contained within.‚Äù
        *** *2006* : Paul Menage (Google) travaille sur les "process containers", plus tard renomm√© en cgroups (control groups) +
        "Cgroups allow processes to be grouped together, and ensure that each group gets a share of memory, CPU and disk I/O; preventing any one container from monopolizing any of these resources"
        *** *fin 2007* : ajout des 1eres briques de l'impl√©mentation des user namespaces dans le kernel Linux 2.6.23 par Eric Biederman (Red Hat) +
        "Red Hatter Eric W. Biederman‚Äôs 2008 user namespaces patches being arguably the most complex and one of the most important namespaces in the context of containers. The implementation of user namespaces allows a process to have it‚Äôs own set of users and in particular to *allows a process root privileges inside a container, but not outside*."
        *** *2008* : cr√©ation du projet Linux Containers (LXC) par des ing√©nieurs d'IBM. +
        "It layered some userspace tooling on top of cgroups and namespaces"
            **** https://fr.wikipedia.org/wiki/LXC : initial release 2008/08/06
        *** *2014/02/20* : release de la 1ere version 1.0 de LXC
        *** *2014/06/07* : toute premi√®re release de *Kubernetes* par Google (1er commit GitHub), qui le pr√©sente comme une version open source de Borg (Google‚Äôs *internal* container cluster-management system)
            **** Kubernetes en peu de mots : un gestionnaire de cluster de conteneurs open source
            **** pour cette date du 06/06, voir https://techcrunch.com/2018/06/06/four-years-after-release-of-kubernetes-1-0-it-has-come-long-way/
            **** Pour plus de d√©tails sur l'histoire de Kubernetes, voir https://blog.risingstack.com/the-history-of-kubernetes/
        *** *2015* : Docker Inc donne la codebase du projet Docker √† l'OCI. +
        "In June 2015, Docker the company, the largest contributor to Docker the project (Red Hat is the second), donated the project‚Äôs existing codebase to the Open Container Initiative, a lightweight governance structure under the auspices of the Linux Foundation created to *prevent fragmentation* and promote open standards by ‚Äúcloud giants‚Äù including Red Hat."
            **** ce "prevent fragmentation" est tr√®s probablement la principal raison du "split" de Docker op√©r√© par Docker Inc
        *** *2015/07/21* : release de la 1ere version de Kubernetes par Google, et cr√©ation de la CNCF comme umbrella projet de la Linux Foundation. +
        Google versera / contribuera cette v1.0 de Kubernetes √† la CNCF en tant que tout 1er projet et √©l√©ment fondateur. +
        Pour rappel, la CNCF se d√©finit comme "a Linux Foundation project that was founded in 2015 to help advance container technology and align the tech industry around its evolution" (voir https://en.wikipedia.org/wiki/Cloud_Native_Computing_Foundation et https://fr.wikipedia.org/wiki/Cloud_Native_Computing_Foundation)

    ** https://d2iq.com/blog/brief-history-containers (2018/07)

        *** *1970s* : +
        "The *original idea* of a container has been around since the 1970s, when the concept was first employed on *Unix systems* to *better isolate application code*. While useful in certain application development and deployment scenarios, the *biggest drawback* to containers in those early days was the simple fact that they were *anything but portable*." +
        "Back in the 1970s, *early containers created an isolated environment where services and applications could run without interfering with other processes* ‚Äì producing something akin to a sandbox to test applications, services, and other processes. The original idea was to isolate the container's workload from production systems in way that *enabled developers to test their applications and processes on production hardware without risking disruption to other services*."

    ** https://blog.aquasec.com/a-brief-history-of-containers-from-1970s-chroot-to-docker-2016 (2020/01) (TRES BIEN)

        *** *1979* : "During the development of Unix version 7 in 1979, the *chroot* system call was introduced, changing the root directory of a process and its children to a new location in the filesystem." +
        "This advance was *the beginning process isolation*: segregating file access for each process. Chroot was added to BSD in 1982."
        *** *2000* : FreeBSD Jails +
        At that time, "a small shared-environment hosting provider came up with FreeBSD jails to achieve *clear-cut separation between its services and those of its customers* for *security* and *ease of administration*. FreeBSD Jails allows administrators to partition a FreeBSD computer system into several independent, smaller systems ‚Äì called ‚Äújails‚Äù ‚Äì with the ability to assign an IP address for each system and configuration."
            **** https://en.wikipedia.org/wiki/FreeBSD_jail : "Jails were first introduced in FreeBSD version 4.0, that was released on *March 14, 2000*"
        *** *2001* : Linux VServer +
        "Like FreeBSD Jails, Linux VServer is a jail mechanism that can partition resources (file systems, network addresses, memory) on a computer system. Introduced in 2001, this operating system virtualization that is implemented by patching the Linux kernel. Experimental patches are still available, but the last stable patch was released in 2006."
        *** *2004* : Solaris Containers +
        "In 2004, the first public beta of Solaris Containers was released that combines system resource controls and boundary separation provided by zones, which were able to leverage features like snapshots and cloning from ZFS."
            **** Cf Wikipedia, les principales caract√©ristiques du syst√®me de fichier ZFS pour Solaris sont, entre autres, sa tr√®s haute capacit√© de stockage, et la gestion de volume.
        *** *2005* : Open VZ (Open Virtuzzo) +
        "This is an operating system-level virtualization technology for Linux which uses a patched Linux kernel for virtualization, isolation, resource management and checkpointing. The code was not released as part of the official Linux kernel."
        *** *2006* : Process Containers (later renamed cgroups / Control Groups) +
        "Process Containers (launched by Google in 2006) was designed for limiting, accounting and isolating resource usage (CPU, memory, disk I/O, network) of a collection of processes. It was renamed ‚ÄúControl Groups (cgroups)‚Äù a year later and eventually merged to Linux kernel 2.6.24."
        *** *2008* : LXC +
        "LXC (LinuX Containers) was the first, most complete implementation of Linux container manager. It was implemented in 2008 using cgroups and Linux namespaces, and it works on a single Linux kernel *without requiring any patches*."
        *** *2011* : Warden +
        "CloudFoundry started Warden in 2011, using LXC in the early stages and later replacing it with its own implementation. Warden can isolate environments on any operating system, running as a daemon and providing an API for container management. It developed a client-server model to manage a collection of containers across multiple hosts, and Warden includes a service to manage cgroups, namespaces and the process life cycle."
        *** *2013* : LMCTFY +
        "Let Me Contain That For You (LMCTFY) kicked off in 2013 as an open-source version of Google's container stack (based on Borg internals), providing Linux application containers. Applications can be made ‚Äúcontainer aware,‚Äù creating and managing their own subcontainers. Active deployment in LMCTFY stopped in 2015 after Google started contributing core LMCTFY concepts to libcontainer, which is now part of the Open Container Foundation."
            **** initial release 2013/10/13, et final release (0.4.5) 2014/03/28
        *** *2013* : Docker +
        "When Docker emerged in 2013, containers exploded in popularity. It‚Äôs no coincidence the growth of Docker and container use goes hand-in-hand." +
        "Just as Warden did, Docker also used LXC in its initial stages and later replaced that container manager with its own library, libcontainer. But there‚Äôs no doubt that Docker separated itself from the pack by offering an entire ecosystem for container management."
        *** *2014/11* : 1ere release de rkt (https://blog.wescale.fr/2017/01/23/introduction-a-rkt/)
        *** *2017* : *Docker's donation of containerd project to the CNCF*
            **** Cette donation a eu le *2017/03/15*, voir l'annonce de Solomon Hykes https://www.docker.com/blog/docker-donates-containerd-to-cncf/ +
            Cet article explique √©galement que containerd a √©t√© cr√©√© en 2016/12 : +
            "Back in December 2016, Docker spun out its core container runtime functionality into a standalone component, incorporating it into a separate project called containerd, [...]"
        *** 2017/03 : versement / contribution de rkt √† la CNCF
        *** 2017/10 : DockerCon 2017, Docker announced they will support the Kubernetes container orchestrator, and Azure and AWS fell in line, with AKS (Azure Kubernetes Service) and Amazon EKS (Amazon Elastic Kubernetes Service)
        *** *2018* : *L'av√®nement de Kubernetes*, o√π tous les Cloud providers commencent √† proposer leur offre de Kubernetes manag√© +
        "The massive adoption of Kubernetes pushed cloud vendors such as AWS, Google with GKE (Google Kubernetes Engine), Azure, and Oracle with Container Engine for Kubernetes, to offer managed Kubernetes services. Furthermore, leading software vendors such as VMWare, RedHat, and Rancher started offering Kubernetes-based management platforms."
        
            **** √©mergences des "*sandbox runtimes*" : *Kata containers*, *gVisor*, *Nabla* : +
            "We also witnessed emerging hybrid technologies that combine *VM-like isolation with container speed*. Open source projects such as Kata containers, gVisor, and Nabla attempt to provide *secured container runtimes* with lightweight virtual machines that perform the same way container do, but provide *stronger workload isolation*." +
            Voir cet article https://www.agaetis.fr/blogpost/les-runtimes-oci qui expliquent bien ce que sont les "*sandbox runtimes*" comme gVisor, Nabla containers et Kata containers : +
            "Les sandbox runtimes, des runtimes qui *isolent un peu plus les conteneurs de la machine h√¥te* en limitant les interactions entre le kernel et les conteneurs." +
            L'accent est donc mis sur la *SECURITE* : il faut combler les failles de s√©curit√© des containers popularis√©s par Docker, c'est la raison d'√™tre des sandbox runtimes. +
            "Les sandbox runtimes *limitent les interactions entre le conteneur et le kernel* pour *r√©duire au maximum la surface d‚Äôattaque*, permettant ainsi une plus grande isolation. Dans cette cat√©gorie nous allons voir gVisor,  Nabla containers et Kata containers. Chacun utilisent une m√©thode diff√©rente pour y arriver". +
            Rappelons cette crainte que l'on avait du temps des d√©buts de Docker en 2013 : +
            "*Concern and hesitation* arose in the IT community regarding the *security of a shared OS kernel*" (https://searchitoperations.techtarget.com/feature/Dive-into-the-decades-long-history-of-container-technology)
                ***** *gVisor* impl√©mente son propre kernel, *Sentry*, et son composant pour les interactions avec le syst√®me de fichiers, *Gofer*
                ***** *Nabla containers* utilise la technique de *l‚Äôunikernel* qui consiste √† packager l‚Äôapplication avec une biblioth√®que d‚ÄôOS qui remplace un OS normal pour aboutir √† une image de machine virtuelle minimale et d√©di√©e √† l‚Äôapplication.
                ***** *Kata containers* lance les conteneurs dans une *micro-VM d√©di√©e*, optimis√©e pour d√©marrer vite et con√ßue pour cet usage. Un composant sur la machine h√¥te permet de faire le proxy et d‚Äôenvoyer les instructions √† l‚Äôagent Kata via l‚Äôhyperviseur. Les micro-VMs sont des VMs avec un minimum de fonctionnalit√©s, seulement le strict n√©cessaire pour faire fonctionner des conteneurs.
            **** Ces "sandbox runtimes" permettent d‚Äôisoler les conteneurs, mais au prix de *performances d√©grad√©es*, et parfois plus : 
                ***** *gVisor* n‚Äôest pas compatible avec toutes les applications, notamment celles qui n√©cessitent un acc√®s direct aux syst√®me de fichier, et il impactent aussi les performances.
                ***** *Nabla container* induit √©galement une baisse de performance et plus important encore, il n‚Äôest pas tout √† fait fini et *ne semble plus tr√®s maintenu*.
            **** *Kata containers* : lancement de la v1.0 le 2018/05/22 (https://techcrunch.com/2018/05/22/the-kata-containers-project-hits-1-0/)
            **** *gVisor* : release initiale en 2018/05/02 (https://en.wikipedia.org/wiki/GVisor)
                ***** blog de Google annon√ßant la sortie de gVisor le 2018/05/02 : https://cloud.google.com/blog/products/identity-security/open-sourcing-gvisor-a-sandboxed-container-runtime +
                "To that end, we‚Äôd like to introduce gVisor, a new kind of sandbox that helps provide secure isolation for containers, while being more lightweight than a virtual machine (VM). gVisor integrates with Docker and Kubernetes, making it simple and easy to run sandboxed containers in production environments."
                ***** https://www.zdnet.com/article/google-open-sources-gvisor-a-sandboxed-container-runtime/ (2018/05/03) : +
                "With gVisor, Google has introduced a new way to *sandbox containers*. These are containers that provide a *secure isolation boundary* between the host operating system and the application running within the container."
            **** *Nabla containers* : les Nabla containers ont √©t√© lanc√©s en 2018/07 https://blog.hansenpartnership.com/a-new-method-of-containment-ibm-nabla-containers/ 
            **** Le choix de ces nouveaux runtimes est expliqu√© par Justin Cormarck, le CTO de Docker, √† la KubeCon 2018 : https://static.sched.com/hosted_files/kccna18/c6/KubeCon_%20How%20to%20Choose%20a%20Kubernetes%20Runtime.pdf / https://www.youtube.com/watch?v=OZJkwvAnLb4 +
            Le choix de ces nouveaux containers runtimes est li√© √† l'usage de plus en plus massif de Kubernetes, et des containers qu'il fait tourner : de plus en plus de containers qui tournent impliquant une attention plus pouss√©e √† leur s√©curit√©

        *** *2019* : les cons√©quences de l'essor de Kubernetes (le d√©clin de Docker)
            **** 2019/04 : la CNCF archive le projet rkt, suite √† une adoption utilisateur en forte baisse
            **** 2019/11/13 : Docker se scinde en 2 : Mirantis rach√®te Docker Enterprise, et Docker Inc se recentre autour de Docker Desktop (et Docker Hub) et l√®ve 35 millions aupr√®s de ses pr√©c√©dents investisseurs Benchmark Capital et Insight Partners. +
            Voici l'explication officielle de Docker : +
            "Docker is ushering in a new era with a return to our roots by focusing on advancing developers‚Äô workflows when building, sharing and running modern applications. As part of this refocus, Mirantis announced it has acquired the Docker Enterprise platform business,‚Äù Docker said in a statement when asked about this change. ‚ÄúMoving forward, we will expand Docker Desktop and Docker Hub‚Äôs roles in the developer workflow for modern apps. Specifically, we are investing in expanding our cloud services to enable developers to quickly discover technologies for use when building applications, to easily share these apps with teammates and the community, and to run apps frictionlessly on any Kubernetes endpoint, whether locally or in the cloud." +
            Pour plus d'explication, voir : 
                ***** https://techcrunch.com/2019/11/13/mirantis-acquires-docker-enterprise/
                ***** https://www.nextinpact.com/lebrief/40573/10329-docker-se-scinde-en-deux--mirantis-rachete-la-branche---entreprise--
        *** *2020/02* : project rkt is ended (https://github.com/rkt/rkt/issues/4024), so same thing for appc

    ** https://searchitoperations.techtarget.com/feature/Dive-into-the-decades-long-history-of-container-technology (2020/04) (TRES BONNES EXPLICATIONS et bon graphique, complet r√©sumant l'histoire des containers avec ses grandes √©tapes)

        *** *1979* : d√©veloppement de chroot, dans la version 7 d'Unix +
        "Chroot marked the beginning of container-style process isolation by restricting an application's file access to a specific directory -- the root -- and its children. A key benefit of chroot separation was improved system security, such that an isolated environment could not compromise external systems if an internal vulnerability was exploited."
        *** *2003* : Google introduced Borg, the organization's container cluster management system. +
        "It relied on the *isolation mechanisms that Linux already had in place*. In those early days in the evolution of containers, *security wasn't much of a concern*. Anyone could see what was going on inside the machine, which enabled a system of accounting for who was using the most memory and how to make the system perform better."
        *** *2006* (et pas 2004, erreur du site) : control groups / cgroups +
        "Nevertheless, this kind of container technology could only go so far. This led to the development of process containers, which became control groups (cgroups) as early as 2004. Cgroups noted the relationships between processes and reined in user access to specific activities and memory volumes. *The cgroups concept was absorbed into the Linux kernel in January 2008*, after which the Linux container technology LXC emerged. Namespaces developed shortly thereafter to provide the basis for container network security -- to hide a user's or group's activity from others."
        *** *2013* : l'√©mergence de Docker +
        Docker floated onto the scene in 2013 with an easy-to-use GUI, and the ability to package, provision and run container technology. Because Docker enabled multiple applications with different OS requirements to run on the same OS kernel in containers, IT admins and organizations saw opportunity for simplification and resource savings. +
        *Unlike VMs*, containers have a significantly smaller resource footprint, are faster to spin up and down, and require less overhead to manage. VMs must also each encapsulate a fully independent OS and other resources, while *containers share the same OS kernel* and use a proxy system to connect to the resources they need, depending upon where those resources are located. +
        *Concern and hesitation* arose in the IT community regarding the *security of a shared OS kernel*. A vulnerable container could result in a vulnerable ecosystem without the right precautions baked into the container technology. Additional complaints early in the modern evolution of containers bemoaned the lack of data persistence, which is important to the vast majority of enterprise applications. Efficient networking also posed problems, as well as the logistics of regulatory compliance and distributed application management.
        *** *2017* : Kubernetes a le vent en poupe
        *** *2017/04* : Microsoft enabled organizations to run Linux containers on Windows Server. This was a major development for Microsoft shops that wanted to containerize applications and stay compatible with their existing systems.
        *** *2020* : Gartner predicts that by 2022, more than 75% of global organizations will be running containerized applications in production, up from less than 30% today. +
        Worldwide container management revenue will grow strongly from a small base of $465.8 million in 2020, to reach $944 million in 2024, according to a new forecast from Gartner, Inc. +
        For more details, see https://www.gartner.com/en/newsroom/press-releases/2020-06-25-gartner-forecasts-strong-revenue-growth-for-global-co 
        *** *2021* : +
        Gartner predicts that by 2022, more than 75% of global organisations will be running containerised applications in production, up from less than 30% today. The analyst‚Äôs figures are reflected in the latest Red Hat Enterprise Open Source Report 2021, which shows container adoption is already widespread. Of the 1,250 IT leaders surveyed, just under 50% said they use containers in production to at least some degree. A further 37% use containers for development only, while just 16% are still evaluating or researching container adoption, according to Red Hat. +
        Voir https://www.computerweekly.com/feature/Containers-for-a-post-pandemic-IT-architecture
            **** Red Hat Enterprise Open Source Report 2021 : https://www.redhat.com/rhdc/managed-files/rh-enterprise-open-source-report-f27565-202101-en.pdf

    ** https://oziie.medium.com/something-missed-history-of-container-technology-e978f202464a (2020/03/31) : TRES BONNE RESSOURCE (que de tr√®s bonnes explications), et bon graphique r√©sumant l'histoire des containers avec ses grandes √©tapes, et bonnes explications des techno impliqu√©es

        *** le graphique vient en fait du site www.plesk.com : +
        https://www.plesk.com/blog/business-industry/infographic-brief-history-linux-containerization/

        *** *2013* : Docker +
        "Docker was introduced in 2013 by an San Francisco company that offers PaaS cloud services named dotCloud as an open-source project, and its founder is Solomon Hykes. When it first came out, *it aimed to convert monolitich applications into image and container structure by using LXC* (Linux containers). Later on, it started to develop his own container runtime, *libcontainer*, and after this stage, libcontainer was started to be used."

        *** *2014/12* : rkt +
        Rkt is a secure and lightweight Docker alternative container system developed by CoreOS. It is built on a container standard known as *App Container* or *appc*. For this reason, rkt images can be run on container systems that support the ‚Äúappc‚Äù format. +
        "Unlike Docker, rkt runs containers with un-privileged users (unlike priority‚Ä¶ Unlike Docker‚Ä¶). Thus, even if there is a kernel level deficit and the user can get out of the container, this does not affect other containers and users."
            **** rkt venait r√©pondre √† certaines des *probl√®matiques de s√©curit√©* existant avec Docker : +
            "As it is known, containers are process groups that can be created by granting some rights to users on the system or by processing with root. In addition, the operation of a user in one container is not seen by the other container. Users are safe in this way as long as there is no abuse on the Linux kernel. However, in some systems such as Docker, *malicious users who can get out of the container through an abuse on the kernel can ruin everything*. Such a risk exists despite measures."

        *** *l'avenir* (et la multiplication des runtimes) : *podman* (avec *buildah* et *Skopeo*), et le passage aux *daemonless* runtimes

            **** "*Podman* works with the ‚ÄúrunC‚Äù we mentioned earlier so it works in accordance with the *daemonless* concept." It corrects some "daemon with" problems : 
                ***** At the point where no news is received from Daemon, there will be no access to the processes.
                ***** All Docker operations are performed by one or more users with the same root privileges. This could create a vulnerability.
            **** Pour une bonne pr√©sentation du pourquoi de podman (les probl√®mes de s√©curit√© de Docker et l'h√©g√©monie de Kubernetes) et une demo de son utilisation, voir https://www.redhat.com/en/blog/say-hello-buildah-podman-and-skopeo (2019/10) +
            "This excites some people who always saw the *monolith daemon that required root access for everything as a problem*. This brings us to the heart of this article ‚Äì the *daemon-less* and largely *rootless* suite of container management tools."
            **** *Podman ne build pas d'image OCI*, il d√©l√®gue cela √† buildah

            **** *Buildah* : Buildah is a common containerize tool for container systems that comply with the OCI (Open Container Initiative) standards, one of the most important reasons for its development being its power in building container images.
                ***** 1st release v0.11 2018/01/17
                ***** Buildah is a tool that facilitates building OCI images
                ***** The build commands in Podman are actually a subset of Buildah commands and they use the same codes.
                ***** Buildah also works as rootless and daemonless.
            **** Voir √©galement cet excellent article sur les daemonless container runtimes Podman et Buildah, ainsi que le lien qui les unit : https://developers.redhat.com/blog/2018/11/20/buildah-podman-containers-without-daemons : +
            "Kubernetes installations can be complex with multiple runtime dependencies and runtime engines. *CRI-O* was created to provide a lightweight runtime for Kubernetes which adds an *abstraction layer between the cluster and the runtime that allows for various OCI runtime technologies*. However you still have the *problem of depending on daemon*(s) in your cluster for builds - I.e. if you are using the cluster for builds you still need a Docker daemon. +
            Enter Buildah. Buildah allows you to have a Kubernetes cluster without any Docker daemon for both runtime and builds. Excellent. But what if things go wrong? What if you want to do troubleshooting or debugging of containers in your cluster? Buildah isn‚Äôt really built for that, what you need is a client tool for working with containers and the one that comes to mind is Docker CLI - but then you‚Äôre back to using the daemon. +
            This is where Podman steps in. Podman allows you to do all of the Docker commands without the daemon dependency. To see examples of Podman replacing the docker command, see Alessandro Arrichiello's Intro to Podman and Doug Tidwell's Podman‚ÄîThe next generation of Linux container tools. +
            With Podman you can run, build (it calls Buildah under the covers for this), modify and troubleshoot containers in your Kubernetes cluster. With the two projects together, you have a well rounded solution for your OCI container image and container needs."

            **** *Skopeo* : gestion d'image, au sens de t√©l√©chargement, push et signature (principalement)

    ** vid√©os sympas d√©taillant les d√©buts de l'histoire des  containers (jusqu'√† Docker), et r√©sumant bien l'usage des namespaces et cgroups : https://www.youtube.com/watch?v=9Egk9Tnc28E&list=PL5JFPVMx5WzXB-NlH13_G8R8dgfz564uo&index=2
        *** les vid√©os 2 et 3 de la s√©rie pr√©sentent (rapidement) l'histoire de la containerisation, et l'√©cosyst√®me Docker avec l'OCI et CRI (de plus, le speaker explique tr√®s rapidement comment installer correctement Docker sur Ubuntu en 2021)

    ** https://faun.pub/the-missing-introduction-to-containerization-de1fbb73efc5 (2019/03): l√† aussi, une bonne explication de l'histoire des containers
        *** avec une bonne explication de l'*architecture actuelle de Docker* (√† partir de la 1.11) : +
--
Prior to version 1.11, Docker engine was used to manage volumes, networks, containers, images, etc.. +
Now, Docker architecture is broken into four components:

    * Docker engine,
    * containerd,
    * containerd-shim
    * and runC.

The binaries are respectively called docker, docker-containerd, docker-containerd-shim, and docker-runc.

Let‚Äôs enumerate the step to run a container using the new architecture of docker:

    1. Docker engine creates the container (from an image) and passes it to containerd.
    2. Containerd calls containerd-shim
    3. Containerd-shim uses runC to run the container
    4. Containerd-shim allows the runtime (runC in this case) to exit after it starts the container

Using this new architecture we can run ‚Äú*daemon-less containers*‚Äù and we have two advantages:

    * runC can exit after starting the container and we don‚Äôt have to have the whole runtime processes running.
    * containerd-shim keeps the file descriptors like stdin, stdout, and stderr open even when Docker and/or containerd die.
--
        *** Pour un autre *tr√®s bon sch√©ma de l'architecture actuelle de Docker* : https://iximiuz.com/en/posts/implementing-container-runtime-shim/ (2021/08/24)
            **** L'article √©galement tr√®s bien le fonctionnement du shim containerd-shim

=== 2022 - WebAssembly (WASM), modules WASM, containers Javascript

Rappel : WebAssembly = *WASM*

* 2022/05 : https://javascript.developpez.com/actu/333398/Les-conteneurs-JavaScript-surpasseront-ils-les-conteneurs-Linux-Le-createur-de-Node-js-pense-que-les-conteneurs-JavaScript-pourraient-simplifier-l-ecriture-des-services-Web/
    ** "Selon *Dahl* (cr√©ateur de Node.js et de Deno), √©tant donn√© que les logiciels de serveur d√©pendent souvent de nombreuses ressources et configurations syst√®me, leur d√©ploiement √©tait difficile par le pass√©. *Les conteneurs Linux* ont alors r√©solu ce probl√®me. Cependant, Dahl estime qu'un *environnement herm√©tique similaire peut √™tre trouv√© dans le JavaScript du navigateur*, bien qu'√† un niveau d'abstraction plus √©lev√©."
    ** "Il est donc logique de consid√©rer JavaScript comme le langage de script universel" +
    "Selon le cr√©ateur de Node.js, le conteneur JavaScript n'est pas destin√© √† traiter la m√™me ampleur de probl√®mes que les conteneurs Linux."
    ** "En gros, le cr√©ateur de Node.js de Deno pense que l*'universalit√© de JavaScript favorise l'√©mergence d'une nouvelle abstraction de type conteneur*. Les *conteneurs Linux* ne vont pas dispara√Ætre, mais penser en mati√®re de conteneurs JavaScript pourrait simplifier de nombreux services Web."

    ** QUESTION : Parler de Deno, ou trop sp√©cifique / particulier pour une chronologie ?
        *** Semble trop sp√©cifique, de c√¥t√©

* 2022/11 : LCC 288 : https://lescastcodeurs.com/2022/11/21/lcc-288-l-episode-marathon-mastodonien/
    ** *Docker annonce une technical preview des conteneurs WASM* https://www.docker.com/blog/docker-wasm-technical-preview/ (2022/10)
        *** Nouveau packaging qui wrap un ex√©cutable WASM et le fait tourner avec le runtime *WasmEdge*.
        *** C'est un nouveau type de conteneur.
        *** Il y a beaucoup d'activit√© autour de WASM, et il y a eu de nombreuses annonces et d√©monstration lors de la conf√©rence CloudNativeCon et le jour sp√©cial sur WASM, lors de KubeCon.
        *** https://www.infoq.com/news/2022/11/cloud-native-wasm-day/.
        *** Docker utilise Docker Desktop et Docker engine pour d√©marrer des shims.
        *** Ces shims (processes) lancent soit runc (donc pour faire tourner un conteneur), soit wasmedge pour faire tourner des modules wasm.
        *** Donc docker s'√©loigne des conteneurs et essaie de toucher l'orchestration.

    ** 2022/10 - Docker Wasm Technical Preview (le *support par Docker des conteneurs WASM*) : https://www.docker.com/blog/docker-wasm-technical-preview/

        *** As part of this release, we‚Äôre also happy to announce that *Docker will be joining the Bytecode Alliance* as a voting member.

        *** *What is Wasm ?* +
        WebAssembly is a relatively new technology that allows you to *compile application code* written in over 40+ languages (including Rust, C, C++, JavaScript, and Golang) and *run it inside sandboxed environments*.
        *** The *original use cases* were focused on *running native code in web browsers*, such as Figma, AutoCAD, and Photoshop. In fact, fastq.bio saw a 20x speed improvement when converting their web-based DNA sequence quality analyzer to Wasm. And Disney built their Disney+ Application Development Kit on top of Wasm! The benefits in the browser are easy to see.
        *** But *Wasm is quickly spreading beyond the browser thanks to the WebAssembly System Interface (WASI)*. Companies like Vercel, Fastly, Shopify, and Cloudflare support using Wasm for running code at the edge, and Fermyon is building a platform to run Wasm microservices in the cloud.


        *** Plut√¥t que de parler de "containers Javascript", il est peut-√™tre pr√©f√©rable de parler de "Wasm modules" (comme dans l'article)
        *** Bon sch√©ma, simple et clair, du lancement par containerD ET de runC et des containers "classiques" ET de WasmEdge et des modules Wasm via les bons shim.

    ** Docker fait carr√©ment la promotion de son rapprochement avec Wasm, c'est sur la page d'accueil de leur site : 
+
.https://www.docker.com/
----    
WHAT‚ÄôS NEW
Docker + Wasm = Awesome!
Wasm is a new, FAST, and LIGHT alternative to the Linux/Windows containers you‚Äôre using in Docker today ‚Äî give it a try with the Docker+Wasm Beta.
----
        *** Dans cette derni√®re d√©finition, c'est surtout le "LIGHT" qui est important : *WASM* rime avec *SECURE*, *PERFORMANT* et *LIGHT*
            **** Et ne pas oublier la *portabilit√©*

    ** https://wasmlabs.dev/articles/docker-without-containers/ : Recently Docker announced support for WebAssembly in cooperation with WasmEdge (Wasm runtime)
    
    ** Cf MeetUp TechRocks "A la d√©couverte de WebAssembly", https://www.youtube.com/watch?v=-W2ze6tiTyk, il est indiqu√© que les 3 principales caract√©ristiques de WASM sont la *s√©curit√©*, les *performances* et la *portabilit√©*
        *** Faire le parall√®le entre "light" et portabilit√©
        *** *l'id√©e de base de WebAssembly* : "faire tourner un autre langage que Javascript dans le browser"
            **** m√™me si Wasm a √©t√© initi√© par la W3C, mais dans la r√©alit√© c'est surtout Mozilla qui a particip√© √† l'√©volution de Wasm c√¥t√© browser (avec Firefox)
                ***** Il y a 2 ans, grosse vague de licenciement chez *Mozilla*, toute l'√©quipe WASM a pris la porte, MAIS a √©t√© reprise telle quelle chez *Fastly*
                ***** Mozilla poussait surtout WASM sur le navigateur, et Fastly sur le edge computing (donc c√¥t√© backend)
                ***** Fastly - https://en.wikipedia.org/wiki/Fastly : Fastly is an American cloud computing services provider. It describes its network as an edge cloud platform, which is designed to help developers extend their core cloud infrastructure to the edge of the network, closer to users. +
                -> Donc fonction de CDN c√¥t√© Fastly.
                ***** ChatGPT : Fastly is a content delivery network (CDN) that has developed an edge computing platform called Compute@Edge that uses WebAssembly as a runtime for executing custom code at the edge of the network.

        *** Le r√¥le de *WASI* est un peu d'ouvrir la sandbox WASM aux appels I/O, car √† la base WASM ne peut PAS faire d'appels I/O
        *** https://youtu.be/-W2ze6tiTyk?t=2060 : *WASI va permettre de sandboxer WASM* : "quand toi WASM tu vas essayer dans ton langage de faire un file descriptor .open, √ßa va en fait appeler telle external function qui va appeler le runtime"
            **** le principe est que le module WASM va demander au runtime de faire quelque chose qu'il ne sait pas faire lui-m√™me / ne conna√Æt pas ("j'ai cet appel de fonction l√†, je ne sais pas ce que c'est, merci de t'en occuper toi runtime")

        *** Philippe Charri√®re : le langage le plus adapt√© pour faire du WASM doit actuellement √™tre RUST
        *** super explication du fonctionnement interne de WASM via sa stack √† ~52:10 (https://youtu.be/-W2ze6tiTyk?t=3130)

    ** https://www.infoq.com/news/2022/11/cloud-native-wasm-day/
        *** "Wasm was originally developed as a secure sandbox for the web browser. In recent years, it has found many applications on the server-side as a secure, lightweight, fast, and portable alternative to VMs and Linux containers (LXCs)"
        *** *WasmEdge* : "Major Wasm *runtimes* such as WasmEdge and Wasmtime are already committed to supporting and implementing the component model proposal."
        *** De nombreux liens vers les derniers articles sur les Wasm modules
        *** De nombreux langages sont maintenant support√©s (ou en voie de l'√™tre) par Wasm : PHP, Java (mais sans GC, donc pour short-lived Java programs), Python, .Net
        *** Wasm gagne m√™me la Data : +
        "Guba Sandor and Dubas Adam from Cisco presented a *Wasm-based plugin system* for the *Envoy Proxy* that is specifically designed for *customizing logging data pipelines*."

* TODO : rappeler rapidement *la force et les avantages de Wasm*
    ** https://www.linkedin.com/pulse/webassembly-un-nouveau-must-pour-le-d%C3%A9veloppement-web-arnaud/?originalSubdomain=fr
        *** "on peut r√©sumer trois grands objectifs pour Wasm : la *rapidit√©*, la *portabilit√©* et la *flexibilit√©*."
        *** One important WebAssembly advantage revolves around *edge computing*.
    
    ** "Why Containers and WebAssembly Work Well Together" : https://www.docker.com/blog/why-containers-and-webassembly-work-well-together/#:~:text=While%20Docker%20excels%20at%20building,creating%20their%20multi%2Darchitecture%20builds.
        *** "While Docker excels at building and deploying cross-platform cloud applications, Wasm is well-suited to portable, binary code compilation for browser-based applications."
        *** Bonne explication des diff√©rents types de compute infrastructure : ‚Äúthree different categories of compute infrastructure"
            **** *Virtual machines* (heavyweight class) - AKA the ‚Äúworkhorse‚Äù of the cloud, VMs package together an entire operating system ‚Äî kernels and drivers included, plus code or data ‚Äî to run an application virtually on compatible hardware. VMs are also great for OS testing and solving infrastructure challenges related to servers, but, they‚Äôre often multiple GB in size and consequently start up very slowly.
            **** *Containers* (middleweight class) - Containers make it remarkably easy to package all application code, dependencies, and other components together and run cross-platform. Container images measure just tens to hundreds of MB in size, and start up in seconds.
            **** *WebAssembly* (lightweight class) - A step smaller than containers, WebAssembly binaries are minuscule, can run in a secure sandbox, and start up nearly instantly since they were initially built for web browsers.

NOTE: D√©finition par Red Hat : *Edge computing* is computing that takes place at or near the physical location of either the user or the source of the data.

Cf l'abstract du meetup Tech Rocks du 23/02/2023 : 
+
.https://www.meetup.com/fr-FR/Meetup-CTO-Tech-Rocks/events/290691230/
--
Tech.Rocks est heureux de vous convier √† un meetup virtuel d√©di√© √† la d√©couverte de *Web Assembly*, une *technologie de virtualisation* pour impl√©menter des services *portables* plus *s√©curis√©s* et plus performants.

Ils nous aideront √©galement √† mieux comprendre pourquoi Web Assembly s'annonce comme la nouvelle *r√©volution* pour la *portabilit√©*, la *s√©curit√©* et la *performance* des applications et services en ligne.
--

* https://www.linkedin.com/pulse/rapport-tendances-2023-didier-girard tendandes 2023 par Didier Girard, avec un section consacr√©e √† *WebAssembly* (WASM)
    ** solution permettant d'ex√©cuter du code bas niveau directement dans le navigateur, offrant des *am√©liorations spectaculaires des performances*.
    ** solution pour l'ex√©cution, dans le navigateur, d'applications √©crites en C++, Rust ou Go.
    ** WebAssembly va aussi bien au-del√† du navigateur. +
    Cette technologie peut aussi √™tre utilis√©e dans les applications de cloud computing et d'Internet des objets (IoT) : *WebAssembly fournit un environnement de sandboxing s√©curis√© dans lequel le code peut s'ex√©cuter sans avoir d'impact sur les autres programmes*.
        *** Donc une notion proche de celle des conteneurs.

* *D√©finition de WebAssembly*
    ** https://fr.wikipedia.org/wiki/WebAssembly : 
        *** WebAssembly, abr√©g√© wasm, est un *standard* du World Wide Web (W3C) pour le d√©veloppement d‚Äôapplications +
        Le standard consiste en un *bytecode*, sa *repr√©sentation textuelle* et un *environnement d'ex√©cution* dans un *bac √† sable* compatible avec *JavaScript*. Il peut √™tre *ex√©cut√© dans un navigateur Web et en dehors*. +
        Comme WebAssembly ne sp√©cifie qu'un langage de bas niveau, le *bytecode est g√©n√©ralement produit en compilant un langage de plus haut niveau*. +
        De nombreux langages de programmation poss√®dent aujourd'hui un compilateur WebAssembly : Rust, C, C++, C#, Go, Java, Lua, Python, Ruby, Fortran ou Pascal2.+
        Les navigateurs Web compilent le bytecode wasm dans le langage machine de l'h√¥te sur lequel ils sont utilis√©s avant de l'ex√©cuter.

    ** https://medium.com/@gear_techs/what-is-the-webassembly-virtual-machine-why-should-you-use-it-5bfa521e7880
        *** WebAssembly is a way to run programming languages ‚Äî other than JavaScript ‚Äî in your web pages. Essentially, *Wasm is just a virtual machine* that runs on all modern browsers.
            *** https://wasmlabs.dev/articles/docker-without-containers/ : "Browser engines integrate a *Wasm virtual machine*, usually called *a Wasm runtime*, which can run the Wasm binary instructions."

* *Bytecode Alliance* (BCA) : un partenariat industriel poussant le d√©veloppement de Wasm, tout particuli√®rement en dehors du browser
    ** ChatGPT : Bytecode Alliance : a community-driven organization focused on advancing the use of WebAssembly beyond the web. The alliance includes members like Mozilla, Fastly, Intel, and Red Hat, and is committed to creating a more secure, efficient, and open web.

    ** https://bytecodealliance.org/#what-is-the-bytecode-alliance
        *** "The Bytecode Alliance is a nonprofit organization working to provide state-of-the-art foundations for the *development of runtime environments* and language toolchains where security, efficiency, and modularity can all coexist across a wide range of devices and architectures. We enable innovation in compilers, runtimes, and tooling, *focusing on fine-grained sandboxing*, capabilities-based security, modularity, and standards such as WebAssembly and WASI."

    ** *2019/11/12* - *cr√©ation de la Bytecode Alliance* +
    https://hacks.mozilla.org/2019/11/announcing-the-bytecode-alliance/ : "*Announcing* the Bytecode Alliance: Building a secure by default, composable future for WebAssembly"
        *** Le MEME article sur le site de BCA : https://bytecodealliance.org/articles/announcing-the-bytecode-alliance
        *** *EXCELLENTE RESSOURCE*, d√©taillant tr√®s bien les objectifs de WebAssembly, tout particuli√®rement en mati√®re de s√©curit√©, et expliquant son fonctionnement lui permettant d'atteindre ses objectifs.
        *** The founding members of the Bytecode Alliance are Mozilla, Fastly, Intel, and Red Hat.
        *** "Today we announce the formation of the Bytecode Alliance, a new industry partnership coming together to forge WebAssembly‚Äôs outside-the-browser future by collaborating on implementing standards and proposing new ones."
            **** D√®s 2019, il √©tait d√©j√† question de *sortir Wasm du "seul navigateur"*
        *** Objectif de *SECURITE avant tout* (le 2nd objectif de WASM √©tant les *PERFORMANCES*) : 
            **** we‚Äôre putting in solid, secure foundations that can *make it safe to use untrusted code*, no matter where you‚Äôre running it‚Äîwhether on the cloud, natively on someone‚Äôs desktop, or even on a tiny IoT device.
            **** This is a unique moment in time at the dawn of a new technology, where we have the opportunity to fix what‚Äôs broken and build new, *secure-by-default foundations for native development* that are *portable and scalable*

        *** Constat actuel : Now *80% of your average code base is built with modules downloaded from registries* like JavaScript‚Äôs npm, Python‚Äôs PyPI, Rust‚Äôs crates.io, and others.

    ** 2021/04/28 : *Transformation de la BCA en 1 fondation* : https://deislabs.io/posts/bytecode-alliance/
        *** "Today, the Bytecode Alliance (BCA) has officially launched as a foundation (with Microsoft as a founding member)"

* *Fonctionnement de WebAssembly* : https://bytecodealliance.org/articles/announcing-the-bytecode-alliance

    ** *Constat c√¥t√© s√©curit√©* AVANT WASM :
        *** "This memory isolation does make it much safer to run two programs at the same time. But this isn‚Äôt perfect security. A malicious program can still mess with certain other resources, like files in the file system."
        *** *VMs* and *containers* were *developed to fix this*. They ensure that something running in one VM or container can‚Äôt access the file system of another. And with *sandboxing*, it‚Äôs possible to take away access to APIs and syscalls.
        *** BUT, DRAWBACKS of VMs and containers : "All of these techniques are relatively heavyweight. If we wrap hundreds of packages into their own sandboxed process, we‚Äôd quickly run *out of memory*. We‚Äôd also make the function *calls between the different packages much slower and more complicated*."

    ** Ce qu'apporte WASM √† ce niveau : As we‚Äôre building out the WebAssembly ecosystem, we can design how the pieces fit together in a way that gives you the kind of isolation that you get with processes or containers, but without the downsides.
        *** *WebAssembly can provide the kind of isolation* that makes it safe to run untrusted code. We can have an architecture that‚Äôs like Unix‚Äôs many small processes, or *like containers and microservices*. +
        -> BUT this isolation is *much lighter weight*, and the *communication between them isn‚Äôt much slower* than a regular function call.
    
    ** Donc WAMS est LIGHT, FAST and SECURE, comment est-ce r√©alis√© ? 

        *** *each WebAssembly module is sandboxed by default* : By default, the module doesn‚Äôt have access to APIs and system calls. +
        If you want the module to be able to interact with anything outside of the module, you have to explicitly provide the module with the function or syscall.

        *** *memory model* : Unlike a normal binary compiled directly to something like x86, a WebAssembly module doesn‚Äôt have access to all of the memory in its process. It *only has access to the chunk of memory that has been assigned to it*.
            **** In theory, *scripting languages would also provide this kind of isolation*. Code in scripting languages can‚Äôt directly access the memory in the process. It can only access memory through the variables it has in scope. +
            * But in most scripting language ecosystems, code makes a lot of use of a shared global object*. That‚Äôs effectively the same as shared memory. So the conventions in the ecosystem make memory isolation a problem for scripting languages as well. +
            WebAssembly could have had this problem. In the early days, some wanted to establish a convention of passing a shared memory in to every module. But *the community group opted for the more secure convention of keeping memory encapsulated by default*.
            -> This gives us *memory isolation between the two modules*. That means that a malicious module can‚Äôt mess with the memory of its parent module.

        *** By default, WebAssembly only has a handful of numeric types, which means you *can only pass single digits across*.
            **** BUT, with *interface types*, modules can communicate using more complex values‚Äîthings like like strings, sequences, records, variants, and nested combinations of these. +
            That makes it easy for two modules to exchange data, but in a way that‚Äôs secure and fast. The WebAssembly engine can do *direct copies between the caller and the callee‚Äôs memories*, *without having to serialize and deserialize the data*. And this works even if the two modules aren‚Äôt compiled from the same language.

        *** "And those APIs or system calls might have access to shared resources, like the file system. And as we talked about in a previous post, the way that most operating systems handle access to the file system really falls down in providing the security we need here." +
        This is where comes in *WASI*, *the WebAssembly system interface* : +
        That gives us *a way to isolate these different modules from each other* and give them *fine-grained permissions to particular parts of the file system and other resources*, and also *fine grained permissions for different system calls*.

        *** Etude en cours pour "In technical terms, we‚Äôre *planning to use a fine grained form of per-module virtualization*  [...] and we‚Äôre working on bringing this to WebAssembly"

        *** Tous les points pr√©c√©dents pris en compte, these features make it possible for us to have similar isolation to that of a process, but with much lower overhead. This pattern of usage is what we‚Äôre calling a *WebAssembly nanoprocess*.
            **** These *nanoprocesses* ‚Äîthese *little container-like things*‚Äî can fit in all sorts of places that regular processes and containers and VMs can‚Äôt go.
            **** Ces nanoprocess semblent tout √† fait indiqu√©s pour *faire tourner des MICROSERVICES*
            **** "But these services can‚Äôt go all the places that libraries can go because they‚Äôre too big. They are often running inside a process, which is running inside of a container, which is running on a server. This means that you often have to use a coarse-grained approach when breaking your app apart into these services." +
            -> With wasm, we can *replace microservices with nanoprocesses* and get the same security and language independence benefits. It *gives us the composability of microservices without the weight*. This means we can use a microservices-style architecture and the language interoperability that provides, but with a finer-grained approach to defining the component services.

        *** Plusieurs exemples de soci√©t√©s et de use cases utilisant les nanoprocess sont donn√©es : 
            **** *Fastly* : They‚Äôve come up with an innovative architecture using WebAssembly nanoprocesses which makes it possible to securely host tens of thousands of simultaneously running programs in the same process. Their approach completely isolates the request from previous requests, ensuring full VM isolation.

    ** De bonnes explications du fonctionnement de WASM dans sa *page Wikipedia anglaise* : https://en.wikipedia.org/wiki/WebAssembly

        *** "WebAssembly implementations usually use either ahead-of-time (AOT) or just-in-time (JIT) *compilation*, but *may also use an interpreter*. While the first implementations have landed in web browsers, there are also non-browser implementations for general-purpose use, including Wasmer,[10] Wasmtime[40] or WAMR,[16] wasm3, WAVM, and many others."
        *** "Because *WebAssembly executables are precompiled*, it is possible to use a variety of programming languages to make them.[42] This is achieved *either through direct compilation to Wasm*, or *through implementation of the corresponding virtual machines in Wasm*. There have been around 40 programming languages reported to support Wasm as a compilation target"

        *** About *WASI* (WebAssembly System Interface) : +
        "WASI is a simple interface (ABI and API) designed by Mozilla intended to be portable to any platform.[79] It provides POSIX-like features like file I/O constrained by capability-based security."

            **** ChatGPT : WASI provides a standard interface between WebAssembly modules and the host environment, allowing WebAssembly to be used outside of the web, for example in serverless computing or edge computing.

            **** https://twitter.com/solomonstre/status/1111004913222324225 (Solomon Hykes / Lin Clark) : +
            *2019/03/27* - Announcing WASI : a system interface for running WebAssembly outside the web (and inside it too)

            **** *ABI* : *Application Binary Interface* : https://en.wikipedia.org/wiki/Application_binary_interface +
            "An application binary interface (ABI) is an *interface between two binary program modules*. Often, one of these modules is a library or operating system facility, and the other is a program that is being run by a user." +
            "An ABI *defines how data structures or computational routines are accessed in machine code*, which is a low-level, hardware-dependent format. In contrast, an API defines this access in source code, which is a relatively high-level, hardware-independent, often human-readable format."

            **** https://blog.jdriven.com/2022/08/wasi-capability-based-networking/ : tr√®s bon article sur le fonctionnement de WASI, avec de bons sch√©mas pouvant √™tre r√©utilis√©s
        
        *** 2019 : *Solomon Hykes*, a co-founder of Docker, wrote in 2019, "*If WASM+WASI existed in 2008, we wouldn't have needed to create Docker*. That's how important it is. WebAssembly on the server is the future of computing."[84] Wasmer, out in version 1.0, provides "software containerization, we create universal binaries that work anywhere without modification, including operating systems like Linux, macOS, Windows, and web browsers. Wasm automatically sandboxes applications by default for secure execution".[84]

        *** *Virtual Machine* : +
        Wasm code (binary code, i.e. bytecode) is intended to be run on a portable virtual stack machine (VM).[85] The VM is designed to be faster to parse and execute than JavaScript and to have a compact code representation.[50] An external functionality (like syscalls) that may be expected by Wasm binary code is not stipulated by the standard. It rather provides a way to deliver interfacing via modules by the host environment that the VM implementation runs in.
            **** https://wasmlabs.dev/articles/docker-without-containers/ : Browser engines integrate a Wasm virtual machine, usually called a Wasm runtime, which can run the Wasm binary instructions.

        *** *Wasm program* : +
        A *Wasm program* is designed to be a separate *module* containing collections of various Wasm-defined values and program type definitions. These are expressed in either binary or textual format that both have a common structure.

        *** *Code representation* : +
        In March 2017, the WebAssembly Community Group reached consensus on the initial (MVP) binary format, JavaScript API, and reference interpreter.[96] It defines a *WebAssembly binary format* (*.wasm*), which is not designed to be used by humans, as well as a human-readable *WebAssembly text format* (*.wat*) that resembles a cross between S-expressions and traditional assembly languages.
            **** Philippe Charri√®re : il faut voir le .wasm comme un .jar avec Java

    ** TRES BON ARTICLE r√©cent (2022/12) pr√©sentant WebAssembly, son fonctionnement, ses liens avec les containers et Docker +
    https://wasmlabs.dev/articles/docker-without-containers/
        *** Quelques infos sur Wasm Labs @ VMWare OCTO (https://wasmlabs.dev/) : +
        "Wasm Labs is a team inside VMware's Office of the CTO. We create and contribute to projects that showcase the possibilities of WebAssembly, and help developers adopt this new and exciting technology."
            **** Attention ! OCTO veut ici dire "Office of the CTO" chez VMWare

        *** avec en plus des sch√©mas complets et r√©utilisables
        *** reprend la fameuse d√©claration de Solomon "If WASM+WASI existed in 2008..."
        *** L'article explique aussi l'usage par Docker de WASM √† la place des containers Linux
            **** Pour les SLIDES, reprendre le tr√®s bon sch√©ma de https://www.docker.com/blog/docker-wasm-technical-preview/ faisant appara√Ætre le containerd-wasm-shim
        *** L'article retrace, tr√®s rapidement, l'√©volution de la conteneurisation avec son "future" que serait WASM (le successeur des containers)
        *** Pour l'usage d'un langage interpr√©t√© avec WASM, inclure le tr√®s sch√©ma √† la section "What about interpreted languages?"
        *** et des comparaisons de taille entre modules WASM et containers Docker
        *** Donne l'exemple de WordPress tournant dans le navigateur avec Wasm : https://wordpress.wasmlabs.dev/

    ** Autre article r√©cent (2023/01/21) donnant avec des demo de WASM (Hello World, appel de WASM depuis Javascript, etc.)

* Tr√®s bon article, r√©cent (2022/08) sur le fonctionnement des *WASM nanoprocess* et sur WASI : https://blog.jdriven.com/2022/08/wasi-capability-based-networking/

    ** On commence par une explication du "*secure capability based networking*" de WASI ET une *comparaison* du *network namespace* utilis√© par les containers avec le *network isolation model de WASI* : 

        *** "Namespaces are a feature of the linux kernel providing isolation of global resources. There are different kind of *namespaces*, like *cgroup*, mount, process and network namespaces. The *network namespace* is interesting to compare with *WASI‚Äôs network isolation model* because it‚Äôs the *standard for containerization technologies*."

        *** IMPORTANT : Network namespaces are great for isolating resources for different processes. But *WASI‚Äôs nano process model takes isolation a step further*: with WASI you also define the capabilities of guest Wasm modules loaded in as third party libraries. This way you can restrict a module to make a network call to only a certain host, while an other module can only call another host.
            **** Voici l'explication de la *diff√©rence entre isolation par les containers et par WASM / WASI*

    ** https://github.com/deislabs/bindle/blob/main/docs/webassembly.md +
    Redonne des d√©finitions issues de BCA (Bytecode Alliance)
        *** Nanoprocesses: We used to refer to this as "tianyan" as well, but have since adopted the language used by BCA. We believe our usage of the term is the same as BCAs: *A nanoprocess is a Wasm module that can execute on its own, but communicate to other Wasm modules via the component architecture* (Module Linking, Interface Types) and *WASI* (IO Streams, IO Arrays). +
        -> Le plus important ici: "*A nanoprocess is a Wasm module* that can execute on its own"            

* *forces / faiblesses de WASM* : 

    ** https://medium.com/@gear_techs/what-is-the-webassembly-virtual-machine-why-should-you-use-it-5bfa521e7880 +
    What are the main *benefits of WebAssembly Virtual Machine* ?
        *** Wasm is extremely fast, efficient and portable. Code can be executed at near-native speed across different platforms.
        *** It‚Äôs also very secure as it‚Äôs run in a safe, sandboxed environment and like other web code, it will enforce the browsers same-origin and permissionless security policies.

    ** Google Bard : 
        *** advantages of WASM : 
            **** *Speed* : WebAssembly is compiled to machine code, which makes it *faster than containers*, which are typically run in a virtual machine.
                **** Attention ! Je ne suis pas fan de la formulation, le code WASM √©tant √©galement ex√©cut√© sur une machine virtuelle
            **** *Portability*: WebAssembly is a cross-platform technology, which means that it can run on different operating systems and hardware architectures. Containers, on the other hand, are typically tied to a specific operating system.
            **** *Security*: WebAssembly is sandboxed, which means that code running in a WebAssembly container cannot access the host system or other containers. Containers, on the other hand, can potentially access the host system if they are not properly configured.
            **** *Size* (LIGHT) : WebAssembly is a *binary format*, which means that it can be compressed more efficiently than containers, which are typically based on text formats.
        *** drawbacks of WASM : 
            **** *Complexity*: WebAssembly is a complex technology, which can make it difficult to develop and debug applications. Containers, on the other hand, are relatively simple to use.
            **** *Lack of maturity*: WebAssembly is a relatively new technology, which means that it is not as mature as containers. This can make it more difficult to find support and resources for WebAssembly development.
            **** *Limited ecosystem*: The WebAssembly ecosystem is still in its early stages, which means that there are fewer tools and libraries available for WebAssembly development than for containers. 

    ** ChatGPT : 
        *** advantages of WASM
            **** *Portability* : WebAssembly modules are platform-agnostic and can run in any environment that supports WebAssembly, making it easy to run the same code on multiple platforms.
            **** *Performance* : WebAssembly code is compiled to machine code, which can result in faster execution compared to interpreted languages like JavaScript.
            **** *Security* : WebAssembly runs in a sandboxed environment, which makes it more secure compared to running code directly on a host system.
            **** *Smaller size* : WebAssembly modules are typically smaller in size compared to container images, making them faster to download and requiring less storage space.
        *** drawbacks of WASM
            **** *Limited ecosystem* : While the WebAssembly ecosystem is growing, it is still not as mature as the container ecosystem, which has a wide range of tools and services for managing and deploying applications.
            **** *Limited language support* : While WebAssembly supports multiple programming languages, it is still limited compared to the wide range of languages supported by containers.
            **** *Limited networking support* : WebAssembly modules currently have limited support for networking and communication with other modules, which can limit their usefulness for complex distributed applications.
            **** *Limited flexibility* : WebAssembly modules are designed to run within a sandboxed environment, which can limit their flexibility compared to containers, which provide a more complete runtime environment.

        *** advantages of containers
            **** *Flexibility* : Containers provide a complete runtime environment for applications, including the necessary libraries, dependencies, and configurations, which provides a high degree of flexibility for deploying and managing complex applications.
            **** *Mature ecosystem* : The container ecosystem is mature and has a wide range of tools and services for managing and deploying applications.
            **** *Large language support* : Containers support a wide range of programming languages and frameworks, which provides flexibility for building and deploying applications in multiple languages.
            **** *Distributed applications* : Containers are designed for distributed applications, which can run across multiple hosts and environments.
        *** drawbacks of containers
            **** *Larger size* : Container images can be large in size, which can result in slower deployment and require more storage space.
            **** *Security risks* : Containers can introduce security risks, particularly if they are not properly configured or managed.
            **** *Performance overhead* : Containers can introduce a performance overhead compared to running code directly on a host system.
            **** *Limited portability* : While containers are designed to be portable, there can be differences in the underlying host environment that can affect performance and behavior.
        
    ** Forces : 
        *** Philippe Charri√®re - l√©ger / *LIGHT* : +
        Dans le cadre de FaaS (Function as a Service), WASM est vraiment tout petit : tu peux cr√©er une fonction h√¥te / host, comme un petit ex√©cutable en Go, qui va le loader, l'appeler et le servir comme un microservice. + 
        Tu vas pouvoir le mettre dans une image Docker "from scratch" (https://codeburst.io/docker-from-scratch-2a84552470c8) qui va peser 20 Mo √† tout casser (contre plusieurs centaines de Mo ou Go avec les images habituelles). +
        Du coup, quand on va d√©ployer l'application dans du Kube, d√©j√† √ßa va se faire vite (l'image √©tant petite), et en plus quand tu vas la scaller et passer √† plusieurs dizaines de pod d'un coup, √ßa va se faire tr√®s rapidement. Et LA on gagne en efficacit√©.

            **** *Host function* : In the context of WebAssembly, a host function is a function that is implemented in the environment where the WebAssembly module is running, typically outside of the WebAssembly module itself. Host functions can be used to provide access to system resources and services that are not available within the WebAssembly sandbox, such as file I/O, network access, or user input.

        *** Philippe Charri√®re - *performance* : plus que les perf, c'est surtout l'*efficacit√©* de WASM qui est int√©ressante : ce qu'on arrive √† faire avec peu de ressources

        *** https://wasmlabs.dev/articles/docker-without-containers/ *portability* : +
        One of the best things about Wasm is its portability. Docker has made traditional containers the way to go when one wants a portable application. However, on top of the big image size, traditional containers are also bound to the architecture of the platform on which they run. Many of us have been through the ups and downs of having to build versions of our software that support different architectures and packaging those in different images for each architecture. +
        -> WebAssembly brings true portability to the picture. You can *build a binary once and run it everywhere*.
            **** Ce qui n'est pas sans rappeler l'une des promesses de Java

    ** Faiblesse importante : jeunesse du projet
        *** nombre de types limit√© : on ne peut globalement passer que des chiffres √† une fonction, ele ne peut te retourner qu'une seule valeur qui sera un chiffre. Si tu veux faire un Hello World en passant ton nom (donc une String) dedans, ce n'est pas √©vident.
        *** pas de debugging possible actuellement
        *** tout est √† faire c√¥t√© gestion des exceptions

* *Comparaison avec les containers*

    ** Bard : 
        *** WebAssembly is a binary instruction format for a stack-based virtual machine. It is designed to be portable across different platforms and browsers. It is also designed to be fast and efficient.
        *** Containers, on the other hand, are a way of isolating applications from each other on a shared operating system. They are typically used to deploy and manage microservices. Containers are often used in conjunction with orchestration tools like Kubernetes.

    ** ChatGPT : 
        *** WebAssembly is not a replacement for containerization, but it is a technology that has the potential to change the way we think about software development and deployment, especially in the context of the web and serverless computing.
        *** WebAssembly is not a replacement for containerization, which provides a more complete solution for managing and deploying applications across different environments.

        *** WebAssembly is a binary instruction format for a stack-based virtual machine that can run code in a wide variety of environments, including web browsers, serverless computing, edge computing, and even desktop and mobile applications. The goal of WebAssembly is to provide a portable, efficient, and safe way to run code in a sandboxed environment, regardless of the platform or programming language used to write the code.
        *** On the other hand, containers are a form of operating system virtualization that allow multiple applications or services to run in isolation on a single host machine. Containers provide a complete runtime environment for applications, including the necessary libraries, dependencies, and configurations. Containers are designed to be lightweight, portable, and easy to deploy and manage, and they are commonly used in cloud computing, DevOps, and microservices architectures.
        *** While WebAssembly and containers can both be used to deploy and run software, they serve different purposes and have different strengths and weaknesses. WebAssembly is best suited for running code in a portable and efficient way, particularly in the context of the web and edge computing. Containers, on the other hand, provide a more complete and flexible solution for managing and deploying complex applications across different environments, particularly in cloud computing and DevOps.

    ** https://blog.jdriven.com/2022/08/wasi-capability-based-networking/
    "The *network namespace* is interesting to compare with *WASI‚Äôs network isolation model* because it‚Äôs the *standard for containerization technologies*."

* *Usages de WASM* : 

    ** *Figma* est un des plus gros utilisateurs de WebAssembly
        *** https://fr.wikipedia.org/wiki/Figma : Figma est un √©diteur de graphiques vectoriels, un *outil de prototypage collaboratif* parmi les plus utilis√©s pour tout ce qui est conception d'interface utilisateur et UX (exp√©rience utilisateur)
        *** ChatGPT : Figma is a web-based collaborative design tool that allows teams to create and share designs in real-time. The company uses WebAssembly to improve the performance of its design editor, particularly for computationally-intensive tasks such as rendering complex vector graphics.
            **** Figma announced its use of WebAssembly in 2018/06
        *** 2022/09/15 - *Figma a √©t√© rachet√© par Adobe pour 20 milliards de dollars*
            **** https://news.adobe.com/news/news-details/2022/Adobe-to-Acquire-Figma/default.asp
            **** 2023/02/24 : rachat contest√© par les autorit√©s de concurrence am√©ricaine et britannique (Adobe √©tant d√©j√† ultra-dominant dans le secteur du desgin) +
            https://www.usine-digitale.fr/article/antitrust-le-gouvernement-americain-lance-une-action-en-justice-contre-le-rachat-de-figma-par-adobe.N2105011
                ***** Figma est utilis√© par 4 millions d‚Äôutilisateurs, notamment chez Microsoft, Google, Twitter ou encore Uber.

    ** *Google Earth* maintenant pr√©sent sur tous les navigateurs gr√¢ce √† WebAssembly
        *** 2020/02/26 : https://medium.com/google-earth/google-earth-comes-to-more-browsers-thanks-to-webassembly-1877d95810d6+
        After six months of a public beta, we are now making Google Earth accessible on Firefox, Edge and Opera browsers. This was made possible by *moving Google Earth for Chrome onto WebAssembly* (Wasm), the W3C web standard for bringing native code to the web.

    ** *Fastly* : Fastly is a content delivery network (CDN) that has developed an edge computing platform called Compute@Edge that uses WebAssembly as a runtime for executing custom code at the edge of the network.

    ** *Dropbox* : Dropbox has been exploring the use of WebAssembly as a way to improve the performance of its web-based file viewer, particularly for large files that can take a long time to load and render.
        *** Dropbox announced its use of WebAssembly in 2019/09
    
    ** https://youtu.be/-W2ze6tiTyk?t=1881 +
    *Shopify* : on peut embarquer un interpr√®teur javascript dans un module WASM, afin de pouvoir dire √† tes clients "fournissez moi des fonctions javascript, moi j'ai la garantie qu'elles sont sandbox√©es, qu'elles s'ex√©cutent dans mon module WASM, et je vous les lance comme √ßa" -> tu perds en rapidit√©, mais c'est plus secure
        *** ChatGPT :  the company uses WebAssembly to improve the performance of its online store editor, particularly for computationally-intensive tasks such as rendering product images. +
        In addition to its use of WebAssembly, Shopify has also been actively contributing to the development of the WebAssembly ecosystem, including through its involvement in the WebAssembly Interface Types (WIT) working group and its development of the Wasm-bindgen tool for creating WebAssembly bindings in Rust.

    ** *Adobe*: Adobe has been experimenting with using WebAssembly to improve the performance of its Creative Cloud suite of products.
        *** Adobe announced its use of WebAssembly in 2019/11

    ** *Slack*: Slack has used WebAssembly to improve the performance of its desktop application, particularly for computationally-intensive tasks such as encryption and compression.
        *** Slack announced its use of WebAssembly in 2020/09

    ** *GitHub*: GitHub has been exploring the use of WebAssembly as a way to enable high-performance code execution in its Actions platform for building and testing software.
        *** GitHub announced its support for WebAssembly in 2021/04
    
    ** https://youtu.be/-W2ze6tiTyk?t=1908 +
    Projet de *R&D Azure* : *Kruslet* +
    Dans Kubernetes, au lieu de lancer un container Docker, on va lancer un module WASM
        *** https://cloudblogs.microsoft.com/opensource/2020/04/07/announcing-krustlet-kubernetes-rust-kubelet-webassembly-wasm/
        *** 2020/04/07 : v0.1.0 de Kruslet
        *** C'est en devenir, mais cela montre dans quelle direction sont actuellement en train de creuser les plus grands.
    ** De mani√®re g√©n√©rale, Microsoft travaille √©norm√©ment sur WebAssembly

    ** Business : gagner des sous en d√©pensant moins de ressources (WASM est tr√®s fort √† ce niveau)
    ** Philippe Charri√®re : la spec n'est pas encore pr√™te, mais, surtout pour tout ce qui est FaaS (Function as a Service) on sent bien qu'il y a un int√©r√™t.

* *Dates importantes de WebAssembly* : 

    ** *2015/06/17* : *cr√©ation de WebAssembly*, https://fr.wikipedia.org/wiki/WebAssembly
        *** pr√©sentation officielle : *2015/06/17*
        *** 1ere d√©monstration : 2016/03/16
    ** *2017/03/06* : *1st release*, WebAssembly 1.0 is released, https://en.wikipedia.org/wiki/WebAssembly
        *** In March 2017, the design of the minimum viable product (MVP) was declared to be finished and the preview phase ended.
            **** https://www.infoworld.com/article/3176681/webassembly-is-now-ready-for-browsers-to-use.html +
            "WebAssembly, a portable code format that could make for a faster web, has moved to minimum viable product (MVP) status, with browser vendors now able to switch WebAssembly on by default."
            **** WebAssembly became a *World Wide Web Consortium recommendation* on 2019/12/05
    ** *2018/03/06* : The Rust programming language adds support for WebAssembly as a compilation target
        *** Rust added support for compiling to WebAssembly (WASM) as a compilation target with the release of wasm-bindgen. The *wasm-bindgen* project is designed to generate WASM binaries and target the wasm32-unknown-unknown target in Rust so as to build WebAssembly applications.
        *** https://rustwasm.github.io/wasm-bindgen/reference/rust-targets.html +
        The *wasm-bindgen* project is designed to target the wasm32-unknown-unknown target in Rust.
        *** Wasm-bindgen is a tool for creating WebAssembly bindings in Rust. 
            **** *WebAssembly bindings* are a mechanism for allowing programming languages to interoperate with WebAssembly modules. The idea is to provide a way for code written in one language to call functions defined in a WebAssembly module, and vice versa.

    ** *2019/03/27* : 1st version de WASI (WebAssembly System Interface), https://twitter.com/linclark/status/1110920999061594113
    ** *2019/11/12* : *cr√©ation de la Bytecode Alliance*, partenariat industriel poussant le d√©veloppement de Wasm, tout particuli√®rement en dehors du browser. Inclut parmis ses membres des soci√©t√©s comme Mozilla, Fastly, Intel, Red Hat, etc.
    ** *2019/12/05* : WebAssembly became a *World Wide Web Consortium recommendation*
    ** *2022/10/24* : Docker announces support for WebAssembly, in cooperation with WasmEdge, and joins the Bytecode Alliance as a voting member
        *** 2022 : l'ann√©e ou WASM pousse tr√®s fort

    -> A partir de 2019, cf les usages √† venir, Wasm pousse tr√®s fort et est de plus en plus adopt√©, jusqu'√† "l'apoth√©ose" avec le support par Docker en coop√©ration avec WasmEdge.

* *Wasmer*

    ** standalone runtime environment
    ** Bard : Wasmer is a fast and secure WebAssembly runtime that enables super lightweight containers to run anywhere: from Desktop to the Cloud, Edge and IoT devices. It is a complete WebAssembly stack, providing a runtime, compiler, and toolchain.
    ** ChatGPT : Wasmer is an open-source runtime for executing WebAssembly code on various platforms, including desktop and mobile devices, servers, and even in web browsers. It provides a lightweight and secure sandboxed environment for running WebAssembly code, and supports multiple programming languages, including Rust, C, C++, Python, and JavaScript.

* *Comparison between the size of a WebAssembly module and the one of a container image*
    ** ChatGPT : 
        *** The size of a WebAssembly (Wasm) module and a container image can vary widely depending on the specific application, libraries, and dependencies involved. However, in general, WebAssembly modules are typically much smaller than container images. +
        A typical *container image* might be *several hundred megabytes in size*, although they can range *from a few megabytes* to several gigabytes or more.

        *** M√™me avec des distroless container image, on restera √† hauteur de quelques Mo au minimum, et on montera facilement √† 100 ou 200 Mo
            **** gcr.io/distroless/static: This is a distroless container image that contains only the application binary and its runtime dependencies. It is suitable for running static binaries, such as Go binaries : *size of around 3-5 Mo*
            **** gcr.io/distroless/java: This is a distroless container image that contains only the Java Virtual Machine (JVM) and the application code. It is suitable for running Java applications : *size of around 100-200 Mo*

        ** Could you please give me the *size of some famous WebAssembly modules* ? +
        Les exemples donn√©s ont des tailles variant de *quelques dizaines de Ko √† moins d'une dizaine de Mo*
            *** *ffmpeg.wasm*: This is a WebAssembly port of the popular FFmpeg multimedia framework, which can be used to encode, decode, and process audio and video files. The size of the ffmpeg.wasm module is around *3.3 Mo*
            *** *protobuf.wasm*: This is a WebAssembly port of the Google Protocol Buffers serialization format, which can be used to encode and decode structured data. The size of the protobuf.wasm module is around *700 Ko*.

        ** Could you now give me the size of some famous container images ? +
        Les exemples donn√©s ont des tailles variant de *quelques dizaines de Mo √† plusieurs centaines de Mo*
            *** *nginx:latest*: This is the latest version of the popular Nginx web server, which is commonly used as a reverse proxy, load balancer, or static content server. The size of the nginx:latest image is around *133 Mo*.
            *** *openjdk:11-jre-slim*: This is a slimmed-down version of the OpenJDK 11 Java runtime environment, which can be used to run Java applications. The size of the openjdk:11-jre-slim image is around *282 Mo*.

        ** https://wasmlabs.dev/articles/docker-without-containers/
            ** image Docker "classique" pour PHP : 166 Mo
            ** image Docker pour PHP avec une Alpine : 30 Mo
            ** container "from scratch" contenant uniquement le module Wasm avec l'interpr√™teur PHP : 5.35 Mo

    ** Bard : 
        *** A typical *WebAssembly module* is around *100KB in size*, while a typical container image is around 1GB in size. This is because WebAssembly modules are compiled to machine code, while container images are typically based on text formats.

    ** 2016 : https://stackoverflow.com/questions/38597955/what-docker-image-size-is-considered-too-large +
    In my previous company, we adopted a micro-service architecture and used Docker to implement it. The average size of our Docker images were ~300MB - ~600MB. However my new company is using Docker mostly for development workflow, and the average image size is ~1.5GB - ~3GB. Some of the larger images (10GB+) are being actively refactored to reduce the image size.

* *Ressources sur WASM* : 
    ** Le livre √©crit par Philippe Charri√®re (GitLab) "WASM cooking with Golang" https://wasm.cooking/
        *** Int√©r√™t : package tout l'environnement avec la cha√Æne de production pour pouvoir √™tre hands-on extr√™ment rapidement avec Wasm
        *** MAIS de l'aveu de Philippe est d√©j√† obsol√®te ("ne l'achetez plus", Philippe pense √† une suite)

==== slides

* Commencer par une d√©finition de WebAssembly
* Puis pr√©senter les raisons de sa cr√©ation et ses principaux objectifs : la s√©curit√©, les performances et la l√©g√®ret√©
    ** cf les ressources ci-dessus, c'est vraiment la l√©g√®ret√© qui est la raison de la naissance de WASM par rapport aux VMs et aux containers
    ** "WebAssembly is a way to run programming languages ‚Äî other than JavaScript ‚Äî in your web pages"
* Expliquer le principe de WebAssembly nanoprocess
* Citer Solomon Hykes et son talk de 2019 sur "If WASM+WASI existed in 2008, we wouldn't have needed to create Docker"
    ** Voici son tweet de l'√©poque : https://twitter.com/solomonstre/status/1111004913222324225
* Parler ensuite du lien cr√©√© en Docker et WASM, de la cr√©ation de la Bytecode Alliance, puis de l'adh√©sion de Docker √† cette derni√®re

TO BE COMPLETED

== Distroless container images et "scratch" images

* 2023/02/02 : https://bell-sw.com/blog/distroless-containers-for-security-and-size/
    ** So distroless images (which should be called *"almost distroless"* but it won‚Äôt make a good selling point, will it?) do contain a Linux distribution, albeit an incredibly stripped-down one, without a package manager, shell, or other typical Linux components. This approach is in line with the modern practices of running an application in the immutable container, meaning that we have no need for introducing system changes or utilizing libraries not involved in app‚Äôs work
        *** Bonne image pour comparer les tailles des images distroless qui peuvent quand m√™me vite monter...

    ** Mais les images distroless ont leurs inconv√©nients : 
        *** Distroless containers are hard to debug due to the absence of shell access.
        *** Configuration of distroless images is anything but simple. To adjust Google's images, you need to know bazel, and adding new packages is complicated without a package manager. +
        A limited range of out-of-the-box distroless images increases the risk of the image being incompatible with your app.

* ChatGPT : 
    ** Distroless container images are container images that contain only the application and its runtime dependencies, but do not include any additional operating system components or package managers. This means that they are stripped down to the bare essentials needed to run the application, and do not include any unnecessary or potentially insecure software.
    ** The idea behind distroless containers is to create a more secure and lightweight environment for running applications in containers. By removing the operating system components and package managers, the attack surface of the container is reduced, making it less vulnerable to security threats. Additionally, the smaller size of the container can improve performance and reduce the amount of resources required to run the application.

    ** Strictly speaking, distroless container images do not exclude a Linux operating system entirely. Instead, they exclude the distribution-specific userland and package manager components that are traditionally included in container images. +
    In a distroless container image, only the essential libraries and dependencies required to run the application are included, rather than a full operating system with its own set of tools and utilities. This approach reduces the size of the image and limits the attack surface of the application. +
    So while distroless container images do still rely on a Linux operating system, they do not include the distribution-specific components that are often unnecessary and can introduce security risks or compatibility issues.

    ** Int√©r√™ts des images distroless : 
        *** *Microservices*: Distroless container images are a good fit for microservices architecture, where each microservice is packaged in its own container. By using distroless containers, the size and complexity of each microservice can be reduced, making it easier to deploy and manage.
        *** *Cloud-native applications*: Distroless containers are well-suited for cloud-native applications that are designed to run on distributed, dynamic infrastructure. These types of applications require containers that are lightweight, portable, and secure, and that can be quickly and easily deployed and scaled up or down as needed.
        *** *Security-conscious applications*: Applications that require a high level of security can benefit from using distroless containers because they reduce the attack surface of the application by eliminating unnecessary components and packages.
        *** *Kubernetes deployments*: Distroless containers are a good choice for running with Kubernetes because they are designed to be minimal and self-contained. This makes it easier to manage and scale containerized applications on a Kubernetes cluster.
        *** *Cross-platform compatibility*: Distroless containers are also a good choice for applications that need to run across different platforms or environments. Since they don't rely on distribution-specific packages or tools, they are more compatible with a wider range of systems and environments.

    ** Explication claire et concise : +
    Distroless container images are designed to provide a minimalistic and secure environment for running containerized applications, by excluding distribution-specific components that are often unnecessary and can introduce security risks or compatibility issues. They contain only the essential libraries and dependencies required to run the application, resulting in smaller images and improved performance. This approach also simplifies deployment and ensures compatibility across different systems and environments.

* Bard : 
    ** distroless container images do include a Linux operating system, albeit a very stripped-down one. The Linux kernel is included in all distroless container images, as it is necessary for the container to run. However, other components of the Linux operating system, such as the GNU C Library (glibc), are not included. This makes distroless container images smaller and more secure than traditional container images that include a full Linux operating system.

* https://twitter.com/tracymiranda/status/1636778664871313414?s=20 : Met en avant les Distroless container images cr√©√©es en 2017 par Google
    ** https://www.trendmicro.com/en_us/research/22/i/enhancing-cloud-security-by-reducing-container-images-through-di.html +
    "Google created Distroless container images, which are images that contain only the application and its runtime dependencies. Unlike images for standard Linux distributions, Distroless container images do not have package managers, shells, or other programs."

* *2017* - *Repo de Google sur les "Distroless" Container images* : https://github.com/GoogleContainerTools/distroless
    ** (almost) "Distroless" images contain only your application and its runtime dependencies. They do not contain package managers, shells or any other programs you would expect to find in a standard Linux distribution.
    ** To be precise, distroless container images do include a Linux operating system, albeit a very stripped-down one.
    ** Le 1er commit de ce repo date du *2017/04/12* : https://github.com/GoogleContainerTools/distroless/commit/02de2e7f738c27d8811003167355d357a146997a

* *"Scratch" images* : https://blog.baeke.info/2021/03/28/distroless-or-scratch-for-go-apps/
    ** Official "scratch" image from Docker : https://hub.docker.com/_/scratch
        *** "This image is most useful in the context of building base images (such as debian and busybox) or super minimal images (that contain only a single binary and whatever it requires, such as hello-world). +
        As of Docker 1.5.0 (specifically, docker/docker#8827), FROM scratch is a no-op in the Dockerfile, and will not create an extra layer in your image (so a previously 2-layer image will be a 1-layer image instead)."
    ** https://docs.docker.com/engine/userguide/eng-image/baseimages/
        *** "You can use Docker‚Äôs reserved, minimal image, scratch, as a starting point for building containers. Using the scratch ‚Äúimage‚Äù signals to the build process that you want the next command in the Dockerfile to be the first filesystem layer in your image. +
        While scratch appears in Docker‚Äôs repository on the hub, you can‚Äôt pull it, run it, or tag any image with the name scratch. Instead, you can refer to it in your Dockerfile. For example, to create a minimal container using scratch:"
+
[source, docker]
----
FROM scratch
COPY hello /
CMD ["/hello"]
----

=== 2022 - TO BE COMPLETED (TODO)

* 2023/03/17 : anniversaire, les 10 ans de Docker
* A brief history of minimal, secure container images : https://twitter.com/tracymiranda/status/1636778664871313414?s=20
    ** Regarder le Wolfi Community call du 2023/03/08 : https://www.youtube.com/watch?v=T6rOdF3ZeRs
        *** Wolfi is a lightweight GNU software distribution designed around minimalism

== Plan du talk

* Petite phrase introductive : "Pour comprendre le pr√©sent, et pouvoir envisager sereinement le futur, il est bon de conna√Ætre / pas avoir oubli√© le pass√© / d'o√π l'on vient"
* ou "Pour comprendre o√π l'on est, pouvoir envisager sereinement o√π l'on va, il est bon de savoir d'o√π l'on vient"

Commencez par un petit disclaimer pour le public ? "ce que ce talk n'est pas ?" (√† savoir une prez sur comment utiliser Docker, CRI-O ou Podman)

1. un rappel : qu'est-ce qu'un container ?

Julia EVANS en dit la chose suivante (https://jvns.ca/blog/2016/10/10/what-even-is-a-container/) : +
"The word ‚Äúcontainer‚Äù doesn‚Äôt mean anything super precise. Basically there are a few *new Linux kernel features* (‚Äúnamespaces‚Äù and ‚Äúcgroups‚Äù) that let you isolate processes from each other. When you use those features, you call it ‚Äúcontainers‚Äù." +

On trouve √©galement le rappel "tout b√™te" suivant : *A container is a group of processes*

-> D'o√π : a container is a "just" a group of processes that are isolated from each other by some means.

[start=2]
2. L'histoire de la containerisation d'hier √† aujourd'hui : La frise chronologique

    * cgroups et namespaces : les premisses des containers
    * Puis Docker
        ** pr√©voir sch√©ma de ce "Docker l'ancien"
    * Puis Docker 1.11 et le "split" avec containerd et runc
        ** nouveau sch√©ma, Docker Engine, server avec containerd et runc
        ** checker o√π en √©tait Kubernetes √† date du split (2016/04). CRI avait-il d√©j√† fait son apparition ?
    * Puis l'arriv√©e de Kubernetes qui a entra√Æn√© l'apparition de CRI
        ** nouveau sch√©ma avec Docker ET Kubernetes, et leurs container runtimes
    * Et maintenant toutes les alternatives possibles √† "Docker" (en fait, de nouveaux high et low container runtime)
        ** pour les alternatives, voir https://linoxide.com/docker-alternative-container-tools/
        ** Podman (gr√¢ce √† Kubernetes) est devenu un incontournable

    On commence par une *frise temporelle compl√®te* du d√©but des containers √† nos jours, puis on en propose *une 2nd* avec (opinionated point of view) uniquement les plus grandes √©tapes que je compte d√©tailler.

    A chaque d√©but de nouvelle section, reprendre o√π nous en sommes dans la frise temporelle

Les √©tapes majeures √† pr√©senter plus en d√©tails : 

* *Les cgroups et les namespaces*

    ** pourquoi a-t-on fait √ßa ? Principalement pour des besoins d'isolation
        *** retrouver les 1ers usages

* *L'arriv√©e de Docker, le d√©but des containers*

    ** et au d√©but, les containers, pour l'immense majorit√© des devs, c'√©tait Docker et rien que Docker.
        *** Maintenant, il y a Docker la compagnie, et Docker la technologie
    ** Docker la compagnie ? Les images, les containers, la ligne de commande ?
    ** A la base la compagnie Docker a cr√©√© un outil simple et ergonomique pour travailler avec les containers, outil appel√© "docker" (la CLI docker pour √™tre plus pr√©cis)
        *** cette CLI permet tr√®s facilement to build images, pull them from registries, create, start and manager containers
    ** et la grosse diff√©rence se fait avec le passage √† la version 1.11, et l'apparition de containerd et runc

    ** *Docker "√† l'ancienne" avant la 1.11 (2016/04)*
        *** https://jvns.ca/blog/2016/10/02/i-just-want-to-run-a-container/

* *Le split de Docker (v1.11.0) : l'apparition de containerd et runc (high level et low level container runtime)*

    On a maintenant le Docker engine
    ** Docker client (CLI, GUI, etc.) 
    ** parle √† un Docker Daemon 
    ** qui parle √† containerd : un autre daemon qui va aller surveiller vos containers, les red√©marrer
        *** containerd supervise les containers (start, stop, pause)
    ** qui parle √† runc : une librairie, un esp√®ce de wrapper qui va vous permettre de lancer plus facilement des processus isol√©s
        *** et c'est runc qui va lancer votre processus de fa√ßon isol√© via les features de votre kernel (namespaces & co, etc.)
        *** runC can help you avoid being strongly tied to specific technologies, hardware, or cloud service providers.

    ** *containerd* et *runc* ont commenc√© √† appara√Ætre √† partir de Docker *1.11.0* (2016/04) ?
        *** √† confirmer via https://jvns.ca/blog/2016/10/02/i-just-want-to-run-a-container/ (site de 2016)
            **** OUI, confirm√© via https://faun.pub/docker-containerd-standalone-runtimes-heres-what-you-should-know-b834ef155426 : +
            "Docker Engine 1.11 was the first release built on runC (a runtime based on Open Container Intiative technology) and containerd."
        *** Regarder avant tout le blog de Docker : https://www.docker.com/blog/docker-engine-1-11-runc/ (2016/04/13, sortie de Docker 1.11)
            **** "Over the last year (2015), Docker has helped advance the work of the OCI to make it more readily available to more users. It started in *December 2015*, when we *introduced containerd*, a daemon to control runC. This was part of our effort to *break out Docker into small reusable components*."
            **** *2017/03/15* Docker's donation of containerd to the CNCF
        *** voir https://containerd.io/ pour un bon sch√©ma de *containerd*, montrant les low-level runtimes qui gravitent aujourd'hui autour (2021) : https://containerd.io/img/architecture.png

    ** d√©tailler ici les low-level container runtimes, et les high-level container runtimes

* *l'arriv√©e de Kubernetes, et la d√©multiplication des runtimes*

    Kubernetes : fait naturellement tourner des containers dans des pods.

    * Donc l'ecosystem des containers est loin de se limiter au seul "Docker", c'est vraiment un *assemblage de diverses technos*, parmi lesquelles on peut citer : 
        ** pour builder des images OCI compliant : Kaniko (Google), buildah (RedHat), Makisu (Uber)
        ** pour lancer des containers depuis des images : CRI-o, rkt, containerd, Kata containers, gVisor, singularity, nabla, podman

    * Parler des confusions possible entre les diff√©rents "shim" : le deprecated docker-shim, et containerd-shim

== Frise temporelle compl√®te v1

* *1970s* : Le concept d'isolation √©merge du c√¥t√© des syst√®mes Unix. +

    "The *original idea* of a container has been around since the 1970s, when the concept was first employed on *Unix systems* to *better isolate application code*. While useful in certain application development and deployment scenarios, the *biggest drawback* to containers in those early days was the simple fact that they were *anything but portable*." +
    "Back in the 1970s, *early containers created an isolated environment where services and applications could run without interfering with other processes* ‚Äì producing something akin to a sandbox to test applications, services, and other processes. The original idea was to *isolate the container's workload from production systems* in way that *enabled developers to test their applications and processes on production hardware without risking disruption to other services*."

    "During the development of Unix version 7 in 1979, the *chroot* system call was introduced, changing the root directory of a process and its children to a new location in the filesystem." +
        "This advance was *the beginning process isolation*: segregating file access for each process. Chroot was added to BSD in 1982."

    ** Voir √©galement cet excellent article sur les d√©buts d'Unix (Unics √† l'√©poque, pour "Uniplexed Information and Computing Service") : +
    https://www.spiria.com/fr/blogue/breves-technos/unix-a-50-ans/

* *1979* : GRANDE ETAPE - *chroot* +

    ** "During the development of Unix version 7 in 1979, the *chroot* system call was introduced, changing the root directory of a process and its children to a new location in the filesystem."
    ** "This advance was *the beginning process isolation*: segregating file access for each process. Chroot was added to BSD in 1982."
    ** d√©veloppement de chroot, dans la version 7 d'Unix
    ** "Chroot marked the beginning of container-style process isolation by restricting an application's file access to a specific directory -- the root -- and its children. A key benefit of chroot separation was improved system security, such that an isolated environment could not compromise external systems if an internal vulnerability was exploited."

        *** system call - http://www.di.uevora.pt/~lmr/syscalls.html : +
        A system call is just what its name implies -- a request for the operating system to do something on behalf of the user's program. The system calls are functions used in the kernel itself.

    ** Pour des exemples de chroot "breakouts", voir https://securityqueens.co.uk/im-in-chroot-jail-get-me-out-of-here/ +
    L'article contient √©galement un bon sch√©ma illustrant le r√©sultat d'un chroot

* *2000/03* : *FreeBSD Jails* +

    ** "jails", an early implementation of container technology, was added to FreeBSD
    ** At that time, "a small shared-environment hosting provider came up with FreeBSD jails to achieve *clear-cut separation between its services and those of its customers* for *security* and *ease of administration*. FreeBSD Jails allows administrators to partition a FreeBSD computer system into several independent, smaller systems ‚Äì called ‚Äújails‚Äù ‚Äì with the ability to assign an IP address for each system and configuration."
    ** https://en.wikipedia.org/wiki/FreeBSD_jail : "Jails were first introduced in FreeBSD version 4.0, that was released on *March 14, 2000*"

    ** Pour un bon sch√©ma illustrant l'usage de Jails, voir https://www.admin-magazine.com/Archive/2013/13/How-to-configure-and-use-jailed-processes-in-FreeBSD/(offset)/6

* *2001* : *Linux VServer* +

    ** Container technology made it to the Linux side of the house +
    "Jacques G√©linas created the VServer project, which according to the 0.0 version‚Äôs change log allowed ‚Äúrunning several general purpose Linux server on a single box with a high degree of Independence and security.‚Äù"
    ** "Like FreeBSD Jails, Linux VServer is a jail mechanism that can partition resources (file systems, network addresses, memory) on a computer system. Introduced in 2001, this operating system virtualization that is implemented by *patching the Linux kernel*. Experimental patches are still available, but the last stable patch was released in 2006."

    ** Linux-VServer is a virtual private server implementation that was created by adding operating system-level virtualization capabilities to the Linux kernel. +
    https://en.wikipedia.org/wiki/Linux-VServer

* *2002/08* : Les 1er *Linux namespaces* (mount namespaces) sont ajout√©s au kernel Linux 2.4.19 (2002/08/03)

    ** *namespaces* : allow processes to have their own network / PIDs / users / hostname / mounts / and more !

    ** la vid√©o https://www.youtube.com/watch?v=sK5i-N34im8[cgroups, namespaces, and beyond: what are containers made from?] de J√©r√¥me PETAZZONI (Docker) explique en d√©tails les diff√©rentes fonctionnalit√©s des *cgroups*, *diff√©rents types de namespaces*. +
        ATTENTION ! Elle date de 2015 !
            **** Il est √©galement question des *container runtimes* qui sont bas√©s sur les cgroups et les namespaces. +
            Exemples de container runtimes bas√©s sur des namespaces et des cgroups : 
                ***** *LXC* (Linux Containers) : easy for sysadmins / OPS, hard for devs (requires significant elbow grease)
                ***** *systemd-nspawn*
                ***** *Docker*
                ***** *rkt*
                ***** *runC*
                ***** All those container runtimes use the same kernel features (at that time, 2015 ?)
            **** et maintenant des container runtimes qui ne sont PAS bas√©s sur les namespaces et les cgroups : 
                **** *OpenVZ* : by example Travis CI gives you root in OpenVZ
                **** *Jails* / *Zones*

    ** Bon un bon sch√©ma des diff√©rents Linux namespaces, voir https://8gwifi.org/docs/linux-namespace.jsp

    ** *namespaces* are a Linux kernel feature allowing your processes to be separated from the other processes on the computer. +
    You can have PID namespace, networking namespace, mount namespace. +
    Namespaces can be created using the `unshare` program.

    ** *namespaces* limit what you can see : https://youtu.be/sK5i-N34im8?t=1519[J√©r√¥me Petazzoni √† la DockerCon 2015]

    ** Pour les *dates* de cr√©ation des *cgroups* et *namespaces*, voir cet article : https://www.silicon.co.uk/software/open-source/linux-kernel-cgroups-namespaces-containers-186240

    ** *namespaces* were originally developed by *Eric Biederman*, and the final major namespace was merged into *Linux 3.8*. +
        Cf Wikipedia (https://en.wikipedia.org/wiki/Linux_namespaces) : 
        "The Linux Namespaces originated in *2002 in the 2.4.19 kernel* (2002/08/03) with work on the *mount namespace* kind. Additional namespaces were added beginning in 2006[2] and continuing into the future. +
        Adequate containers support functionality was finished in kernel *version 3.8* with the *introduction of User namespaces*."
            **** Et l'info tr√®s int√©ressante est ici : ce sont les user namespaces, introduit avec le kernel 3.8 de Linux qui ont chang√© la donne, et dont Solomon Hykes dit en 2013 (voir la conf ci-dessous, √† 16:19) que, √ßa y est, "les namespaces marchent maintenant".
            **** https://kernelnewbies.org/Linux_3.8 : "*Linux 3.8* was released on Mon, *18 Feb 2013*."

    ** Description des *mount namespaces* par J√©r√¥me Petazzoni de Docker durant la DockerCon 2015 : https://youtu.be/sK5i-N34im8?t=1666

    ** *Namespaces* let you virtualize system resources, like the file system or networking for each container.
        *** Namespaces are *"what you can see"*

    ** At their core, low-level container runtimes are responsible for setting up these namespaces and cgroups for containers, and then running commands inside those namespaces and cgroups.

    ** fin 2007 : ajout des 1eres briques de l'impl√©mentation des user namespaces dans le kernel Linux 2.6.23 par Eric Biederman (Red Hat) +
        "Red Hatter Eric W. Biederman‚Äôs 2008 user namespaces patches being arguably the most complex and one of the most important namespaces in the context of containers. The implementation of user namespaces allows a process to have it‚Äôs own set of users and in particular to *allows a process root privileges inside a container, but not outside*."

    ** "Nevertheless, this kind of container technology (speaking of Borg) could only go so far. This led to the development of process containers, which became control groups (cgroups) as early as 2004. Cgroups noted the relationships between processes and reined in user access to specific activities and memory volumes. The cgroups concept was absorbed into the Linux kernel in January 2008, after which the Linux container technology LXC emerged. *Namespaces developed shortly thereafter to provide the basis for container network security* -- to hide a user's or group's activity from others."

    ** vid√©os sympas d√©taillant les d√©buts de l'histoire des  containers (jusqu'√† Docker), et r√©sumant bien l'usage des namespaces et cgroups : https://www.youtube.com/watch?v=9Egk9Tnc28E&list=PL5JFPVMx5WzXB-NlH13_G8R8dgfz564uo&index=2

* *2003* : Google introduced *Borg*, the organization's container cluster management system. +

    ** "It relied on the *isolation mechanisms that Linux already had in place*. In those early days in the evolution of containers, *security wasn't much of a concern*. Anyone could see what was going on inside the machine, which enabled a system of accounting for who was using the most memory and how to make the system perform better."
    ** *Borg* is Google's cluster manager that runs hundreds of thousands of jobs, from many thousands of different applications, across a number of clusters each with up to tens of thousands of machines. +
    See https://research.google/pubs/pub43438/ for more details

* *2004* : *Solaris Containers* +

    "In 2004, the first public beta of Solaris Containers was released that combines system resource controls and boundary separation provided by zones, which were able to leverage features like snapshots and cloning from ZFS."

    ** https://en.wikipedia.org/wiki/Solaris_Containers : +
    Zones act as completely isolated virtual servers within a single operating system instance

    ** ZFS (https://fr.wikipedia.org/wiki/ZFS) : +
    Les caract√©ristiques de ce syst√®me de fichiers sont sa *tr√®s haute capacit√© de stockage*, l'int√©gration de beaucoup de concepts que l'on trouve sur d'autres syst√®mes de fichiers, et la *gestion de volume*. Il utilise pour cela des structures de donn√©es comme les B-tree "On-Disk", et un adressage des secteurs disque logique au lieu d'un adressage physique. +
    Produit par Sun Microsystems (soci√©t√© rachet√©e par Oracle en 2009) pour *Solaris 10* et au-del√†, il a √©t√© con√ßu par l'√©quipe de Jeff Bonwick (en). Annonc√© pour septembre 2004, il a √©t√© int√©gr√© √† Solaris le 31 octobre 2005 et le 16 novembre 2005 en tant que caract√©ristique du build 27 d'OpenSolaris. Sun a annonc√© que ZFS √©tait int√©gr√© dans la mise √† jour de Solaris dat√©e de juin 2006, soit un an apr√®s l'ouverture de la communaut√© OpenSolaris.

* *2005* : *Open VZ* +

    "This is an operating system-level virtualization technology for Linux which uses a patched Linux kernel for virtualization, isolation, resource management and checkpointing. The code was not released as part of the official Linux kernel."

    ** voir https://fr.wikipedia.org/wiki/OpenVZ +
    OpenVZ permet √† un serveur physique d'ex√©cuter de multiples instances de syst√®mes d'exploitation isol√©s, qualifi√©es de serveurs priv√©s virtuels (VPS) ou environnements virtuels (VE).

    ** pour un bon sch√©ma de l'architecture d'OpenVZ, voir http://www.virtualizationsoftwares.com/openvz-open-virtualization/

* *2006* : d√©but des travaux sur les *Process Containers* chez Google (later renamed *cgroups* / Control Groups)

        ** D√©but des travaux sur les cgroups par Paul Menage and Rohit Seth chez Google

        ** Paul Menage (Google) travaille sur les "process containers", plus tard renomm√© en cgroups (control groups) +
        "Cgroups allow processes to be grouped together, and ensure that each group gets a share of memory, CPU and disk I/O; preventing any one container from monopolizing any of these resources"

        ** "Process Containers (launched by Google in 2006) was designed for limiting, accounting and isolating resource usage (CPU, memory, disk I/O, network) of a collection of processes. It was renamed ‚ÄúControl Groups (cgroups)‚Äù a year later and eventually merged to Linux kernel 2.6.24."

        ** "Nevertheless, this kind of container technology (speaking of Borg) could only go so far. This led to the development of process containers, which became control groups (cgroups) as early as 2004. Cgroups noted the relationships between processes and reined in user access to specific activities and memory volumes. *The cgroups concept was absorbed into the Linux kernel in January 2008*, after which the Linux container technology LXC emerged. Namespaces developed shortly thereafter to provide the basis for container network security -- to hide a user's or group's activity from others."

        ** Cf wikipedia (https://en.wikipedia.org/wiki/Cgroups), *cgroups* : +
        "cgroups (abbreviated from control groups) is a *Linux kernel feature* that limits, accounts for, and isolates the resource usage (CPU, memory, disk I/O, network, etc.) of a collection of processes."
        ** la vid√©o https://www.youtube.com/watch?v=sK5i-N34im8[cgroups, namespaces, and beyond: what are containers made from?] de J√©r√¥me PETAZZONI (Docker) explique en d√©tails les diff√©rentes fonctionnalit√©s des *cgroups*, *diff√©rents types de namespaces*. +
        ATTENTION ! Elle date de 2015 !
            *** Il est √©galement question des *container runtimes* qui sont bas√©s sur les cgroups et les namespaces. +
            Exemples de container runtimes bas√©s sur des namespaces et des cgroups : 
                **** *LXC* (Linux Containers) : easy for sysadmins / OPS, hard for devs (requires significant elbow grease)
                **** *systemd-nspawn*
                **** *Docker*
                **** *rkt*
                **** *runC*
                **** All those container runtimes use the same kernel features (at that time, 2015 ?)
            *** et maintenant des container runtimes qui ne sont PAS bas√©s sur les namespaces et les cgroups : 
                *** *OpenVZ* : by example Travis CI gives you root in OpenVZ
                *** *Jails* / *Zones*

        ** le travail sur les *cgroups* a commenc√© en 2006 chez Google sous le nom "process containers", avant d'√™tre renomm√© en "control groups" pour √©viter toute confusion avec le terme "container" dans un contexte Linux Kernel.
            **** cf Wikipedia (https://en.wikipedia.org/wiki/Cgroups) : +
            "A control group (abbreviated as cgroup) is a *collection of processes that are bound by the same criteria* and associated with a set of parameters or limits. These groups can be *hierarchical*, meaning that *each group inherits limits from its parent group*. The kernel provides access to multiple controllers (also called subsystems) through the cgroup interface;[2] for example, the "memory" controller limits memory use, "cpuacct" accounts CPU usage, etc."

        ** Pour les *dates* de cr√©ation des *cgroups* et *namespaces*, voir cet article : https://www.silicon.co.uk/software/open-source/linux-kernel-cgroups-namespaces-containers-186240

            *** *cgroups* were originally developed by Paul Menage and Rohit Seth of Google, and their first features were merged into *Linux 2.6.24* (*2008/01*) +
            Cf Wikipedia (https://en.wikipedia.org/wiki/Cgroups) : 
            "Engineers at Google (primarily *Paul Menage* and *Rohit Seth*) *started the work on this feature in 2006* under the name "*process containers*".[1] In late 2007, the nomenclature changed to "control groups" to avoid confusion caused by multiple meanings of the term "container" in the Linux kernel context, and the control groups functionality was merged into the Linux kernel mainline in *kernel version 2.6.24*, which was *released in January 2008*."

        ** *cgroups* : limit memory / CPU usage for a group of processes

        ** *cgroups* provide a way to limit the amount of resources, such as CPU and memory, that each container can use.
            *** control groups are "what you can use"
        ** At their core, low-level container runtimes are responsible for setting up these namespaces and cgroups for containers, and then running commands inside those namespaces and cgroups.

        ** vid√©os sympas d√©taillant les d√©buts de l'histoire des  containers (jusqu'√† Docker), et r√©sumant bien l'usage des namespaces et cgroups : https://www.youtube.com/watch?v=9Egk9Tnc28E&list=PL5JFPVMx5WzXB-NlH13_G8R8dgfz564uo&index=2

        ** *cgroups* limit how much you can use : https://youtu.be/sK5i-N34im8?t=1519[J√©r√¥me Petazzoni √† la DockerCon 2015]

        ** sch√©ma sur les cgroups : https://www.lightnetics.com/topic/17326/what-are-control-groups-in-linux

* *2007/10* : ajout des 1eres briques de l'impl√©mentation des *user namespaces* dans le kernel Linux 2.6.23 par Eric Biederman (Red Hat) +

    "Red Hatter Eric W. Biederman‚Äôs 2008 user namespaces patches being arguably the most complex and one of the most important namespaces in the context of containers. The implementation of user namespaces allows a process to have it‚Äôs own set of users and in particular to *allows a process root privileges inside a container, but not outside*."
        ** le kernel Linux 2.6.23 est releas√© le 2007/10/10 (https://lwn.net/Articles/253813/)

    ** Les meilleurs explications et sch√©mas que j'ai trouv√©s sur les user namespaces : 
        *** https://blog.quarkslab.com/digging-into-linux-namespaces-part-2.html
        *** julia-evans_how-containers-work_14_user-namespaces.jpg

* *2008/01* : GRANDE ETAPE - ajout de la fonctionnalit√© des *cgroups* dans le kernel Linux 2.6.24

* *2008/08* : GRANDE ETAPE - cr√©ation du projet *Linux Containers (LXC)* par des ing√©nieurs d'IBM. +

    "It layered some userspace tooling on top of cgroups and namespaces"
        ** https://fr.wikipedia.org/wiki/LXC : initial release 2008/08/06
        ** "LXC (LinuX Containers) was the first, most complete implementation of Linux container manager. It was implemented in 2008 using cgroups and Linux namespaces, and it works on a single Linux kernel *without requiring any patches*."

* *2011* : Warden +

    "CloudFoundry started Warden in 2011, using LXC in the early stages and later replacing it with its own implementation. Warden can isolate environments on any operating system, running as a daemon and providing an API for container management. It developed a client-server model to manage a collection of containers across multiple hosts, and Warden includes a service to manage cgroups, namespaces and the process life cycle."

    ** Voir √©galement https://github.com/cloudfoundry-attic/warden
    The project's primary goal is to provide a simple API for managing isolated environments. These isolated environments -- or containers -- can be limited in terms of CPU usage, memory usage, disk usage, and network access. As of writing, the only supported OS is Linux.

    ** Autre tr√®s bonne explication sur Warden : http://underlap.blogspot.com/2014/06/warden-meets-libcontainer.html
    ** Voir √©galement https://tanzu.vmware.com/content/blog/cloud-foundrys-container-technology-a-garden-overview
        *** Warden creates a root process, called "wshd" for Warden shell daemon, in each container.
        *** The Warden protocol is defined using Google protocol buffer definitions

* *2013/02* : GRANDE ETAPE - ajout des *user namespaces* au kernel Linux 3.8

    ** A ce moment, on a les cgroups et les namespaces (que Solomon HYKES pr√©sente comme si importants), mais il y a √©galement d'autres Linux Kernel features utilis√©s par les containers que l'on va pr√©senter rapidement.

* *2013/03/20* : ETAPE MAJEURE - *1ere release de Docker*

    ** "When Docker emerged in 2013, containers exploded in popularity. It‚Äôs no coincidence the growth of Docker and container use goes hand-in-hand." +
    "Just as Warden did, Docker also used LXC in its initial stages and later replaced that container manager with its own library, libcontainer. But there‚Äôs no doubt that Docker separated itself from the pack by offering an entire ecosystem for container management."
    ** Docker floated onto the scene in 2013 with an easy-to-use GUI, and the ability to package, provision and run container technology. Because Docker enabled multiple applications with different OS requirements to run on the same OS kernel in containers, IT admins and organizations saw opportunity for simplification and resource savings. +
    *Unlike VMs*, containers have a significantly smaller resource footprint, are faster to spin up and down, and require less overhead to manage. VMs must also each encapsulate a fully independent OS and other resources, while *containers share the same OS kernel* and use a proxy system to connect to the resources they need, depending upon where those resources are located. +
    *Concern and hesitation* arose in the IT community regarding the *security of a shared OS kernel*. A vulnerable container could result in a vulnerable ecosystem without the right precautions baked into the container technology. Additional complaints early in the modern evolution of containers bemoaned the lack of  data persistence, which is important to the vast majority of enterprise applications. Efficient networking also posed problems, as well as the logistics of regulatory compliance and distributed application management.
    ** "Docker was introduced in 2013 by an San Francisco company that offers PaaS cloud services named dotCloud as an open-source project, and its founder is Solomon Hykes. When it first came out, *it aimed to convert monolitich applications into image and container structure by using LXC* (Linux containers). ater on, it started to develop his own container runtime, *libcontainer*, and after this stage, libcontainer was started to be used."

* *2013/10* : LMCTFY +

    "Let Me Contain That For You (LMCTFY) kicked off in 2013 as an open-source version of Google's container stack (based on Borg internals), providing Linux application containers. Applications can be made ‚Äúcontainer aware,‚Äù creating and managing their own subcontainers. Active deployment in LMCTFY stopped in 015 after Google started contributing core LMCTFY concepts to libcontainer, which is now part of the Open Container Foundation."
    ** initial release 2013/10/13, et final release (0.4.5) 2014/03/28

* *2014/02/20* : release de la 1ere version 1.0 de LXC

* *2014/06/07* : GRANDE ETAPE - toute premi√®re release de *Kubernetes* par Google (1er commit GitHub), qui le pr√©sente comme une version open source de Borg (Google‚Äôs *internal* container cluster-management system)

    ** Kubernetes en peu de mots : un gestionnaire de cluster de conteneurs open source
    ** pour cette date du 06/06, voir https://techcrunch.com/2018/06/06/four-years-after-release-of-kubernetes-1-0-it-has-come-long-way/
        *** Pour plus de d√©tails sur l'histoire de Kubernetes, voir https://blog.risingstack.com/the-history-of-kubernetes/

* *2014/11* : 1ere release de rkt (prononcer "rocket"), le container runtime cr√©√© par les √©quipes de CoreOS.

    ** https://blog.wescale.fr/2017/01/23/introduction-a-rkt/
    ** Rkt is a secure and lightweight Docker alternative container system developed by CoreOS. It is built on a container standard known as *App Container* or *appc*. For this reason, rkt images can be run on container systems that support the ‚Äúappc‚Äù format. +
    "Unlike Docker, rkt runs containers with un-privileged users (unlike priority‚Ä¶ Unlike Docker‚Ä¶). Thus, even if there is a kernel level deficit and the user can get out of the container, this does not affect other containers and users."
    ** rkt venait r√©pondre √† certaines des *probl√®matiques de s√©curit√©* existant avec Docker : +
    "As it is known, containers are process groups that can be created by granting some rights to users on the system or by processing with root. In addition, the operation of a user in one container is not seen by the other container. Users are safe in this way as long as there is no abuse on the Linux kernel. However, in some systems such as Docker, *malicious users who can get out of the container through an abuse on the kernel can ruin everything*. Such a risk exists despite measures."

* *2015/06* : GRANDE ETAPE - *Docker Inc donne la codebase du projet Docker √† l'OCI*, projet de la Linux Fondation, *cr√©√© pour cette occasion*. +

    "In June 2015, Docker the company, the largest contributor to Docker the project (Red Hat is the second), donated the project‚Äôs existing codebase to the *Open Container Initiative*, a lightweight governance structure under the auspices of the Linux Foundation created to *prevent fragmentation* and promote open standards by ‚Äúcloud giants‚Äù including Red Hat."
        ** ce "*prevent fragmentation*" est tr√®s probablement la principal raison du "split" de Docker op√©r√© par Docker Inc

* *2015/07* : GRANDE ETAPE - *runc* est publi√© pour la 1ere fois, et son code a *tout de suite donn√© par Docker √† l'OCI* (runc est l'impl√©mentation de r√©f√©rence de la runtime-spec)

* *2015/07/21* : ETAPE MAJEURE - *release de la 1ere version de Kubernetes* par Google, et *cr√©ation de la CNCF*, comme umbrella projet de la Linux Foundation, √† laquelle Kubernetes sera donn√© comme √©l√©ment fondateur. +

    Google versera / contribuera cette v1.0 de Kubernetes √† la CNCF en tant que tout 1er projet et √©l√©ment fondateur. +
    Pour rappel, la CNCF se d√©finit comme "a Linux Foundation project that was founded in 2015 to help advance container technology and align the tech industry around its evolution" (voir https://en.wikipedia.org/wiki/Cloud_Native_Computing_Foundation et https://fr.wikipedia.org/wiki/Cloud_Native_Computing_Foundation)

-> DONC EN 2015/06 et 2015/07, on a la cr√©ation √† la fois de la CNCF et de l'OCI

* *2015/12* : GRANDE ETAPE - Docker introduce *containerd*

* *2016/03/14* : 1ere apparition des cgroups v2 dans le Linux Kernel 4.5

    ** plus tard utilis√© par runc et crun
        *** *runc* : runc fully supports cgroup v2 (unified mode) since v1.0.0-rc93, 2021/02/04 (https://github.com/opencontainers/runc)
        *** *crun* : dans la sp√©cification d'origine en 2019/03, et "add support for OCI unified cgroups v2 in v0.15, 2020/09/23" (https://github.com/containers/crun/releases?q=cgroup+v2&expanded=true)

* *2016/04/13* : ETAPE MAJEURE - sortie de la *version 1.11 de Docker* : 1ere release de Docker bas√©e sur containerd et runC (OCI)
    ** cette release marque le passage √† l'architecture "moderne" de Docker

-> A partir de 2017, Kubernetes a de plus en plus le vent en poupe

* *2017/03/15* : GRANDE ETAPE - *Docker's donation of containerd project to the CNCF* AND *CoreOS's donation of rkt to the CNCF* 

    ** Cette donation a eu le *2017/03/15*, voir l'annonce de Solomon Hykes https://www.docker.com/blog/docker-donates-containerd-to-cncf/ +
    Cet article explique √©galement que containerd a √©t√© cr√©√© en 2016/12 : +
    "Back in December 2016, Docker spun out its core container runtime functionality into a standalone component, incorporating it into a separate project called *containerd*, [...]"
    ** Les 2 annonces ont eu lieu le m√™me jour, durant le m√™me meeting du CNCF TOC (Technical Oversight Committee, o√π Docker et CoreOS √©taient d√©j√† repr√©sent√©s) +
    Voir https://www.cncf.io/blog/2017/03/29/rkt-pod-native-container-engine-launches-cncf/ : +
    "On March 15, 2017, at the CNCF TOC meeting, CoreOS and Docker made proposals to add rkt and containerd as new projects for inclusion in the CNCF. During the meeting, we as rkt co-founders, proposed rkt, and Michael Crosby, a containerd project lead and co-founder, proposed containerd."

* *2017/04* : Microsoft enabled organizations to run Linux containers on Windows Server. This was a major development for Microsoft shops that wanted to containerize applications and stay compatible with their existing systems.

* *2017/04/12* - apparition des "Distroless" container images chez Google
    ** repo Google : https://github.com/GoogleContainerTools/distroless
    ** 1er commit : https://github.com/GoogleContainerTools/distroless/commit/02de2e7f738c27d8811003167355d357a146997a
    ** (almost) Distroless container images are designed to provide a minimalistic and secure environment for running containerized applications, by excluding distribution-specific components that are often unnecessary and can introduce security risks or compatibility issues. They contain only the essential libraries and dependencies required to run the application, resulting in smaller images and improved performance. This approach also simplifies deployment and ensures compatibility across different systems and environments.
    ** To be precise, distroless container images do include a Linux operating system, albeit a very stripped-down one.

* *2017/07/19* : Release of the OCI v1.0 runtime and image specifications

* *2017/10* : DockerCon 2017, Docker announced they will support the Kubernetes container orchestrator, and Azure and AWS fell in line, with AKS (Azure Kubernetes Service) and Amazon EKS (Amazon Elastic Kubernetes Service)

(* 2018 beginnning : CoreOS was acquired by Red Hat at the beginning of 2018)

-> 2018, l'av√®nement de Kubernetes, tous les plus grands Cloud providers en proposant une offre packag√©, ET la d√©multiplication des container runtimes de types diff√©rents (sandbox runtimes, daemonless runtimes)

-> *2018* : ETAPE MAJEURE - *L'av√®nement de Kubernetes*, o√π tous les Cloud providers propose leur offre de Kubernetes manag√© +
"The massive adoption of Kubernetes pushed cloud vendors such as AWS, Google with GKE (Google Kubernetes Engine), Azure, and Oracle with Container Engine for Kubernetes, to offer managed Kubernetes services. Furthermore, leading software vendors such as VMWare, RedHat, and Rancher started offering Kubernetes-based management platforms." +
L'usage croissant de Kubernetes a d√©multipli√© l'usage des containers (1 seul cluster Kubernetes pouvant en faire tourner jusqu'√† un maximum de 300 000), rendant la *s√©curisation* de ces derniers d'autant plus importante. +
Ce besoin accrue de s√©curisation a amen√© √† l'apparition de nouveaux types de runtimes : les sandbox runtimes, ainsi que les daemonless / rootless runtimes 

* √©mergences des "*sandbox runtimes*" : *Kata containers*, *gVisor*, *Nabla* : +

    "We also witnessed emerging hybrid technologies that combine *VM-like isolation with container speed*. Open source projects such as Kata containers, gVisor, and Nabla attempt to provide *secured container runtimes* with lightweight virtual machines that perform the same way container do, but provide *stronger workload isolation*." +
    Voir cet article https://www.agaetis.fr/blogpost/les-runtimes-oci qui expliquent bien ce que sont les "*sandbox runtimes*" comme gVisor, Nabla containers et Kata containers : +
    "Les sandbox runtimes, des runtimes qui *isolent un peu plus les conteneurs de la machine h√¥te* en limitant les interactions entre le kernel et les conteneurs." +
    L'accent est donc mis sur la *SECURITE* : il faut combler les failles de s√©curit√© des containers popularis√©s par Docker, c'est la raison d'√™tre des sandbox runtimes. +
    "Les sandbox runtimes *limitent les interactions entre le conteneur et le kernel* pour *r√©duire au maximum la surface d‚Äôattaque*, permettant ainsi une plus grande isolation. Dans cette cat√©gorie nous allons voir gVisor,  Nabla containers et Kata containers. Chacun utilisent une m√©thode diff√©rente pour y arriver". +
    Rappelons cette crainte que l'on avait du temps des d√©buts de Docker en 2013 : +
    "*Concern and hesitation* arose in the IT community regarding the *security of a shared OS kernel*" (https://searchitoperations.techtarget.com/feature/Dive-into-the-decades-long-history-of-container-technology)
        *** *gVisor* impl√©mente son propre kernel, *Sentry*, et son composant pour les interactions avec le syst√®me de fichiers, *Gofer*
        *** *Nabla containers* utilise la technique de *l‚Äôunikernel* qui consiste √† packager l‚Äôapplication avec une biblioth√®que d‚ÄôOS qui remplace un OS normal pour aboutir √† une image de machine virtuelle minimale et d√©di√©e √† l‚Äôapplication.
        *** *Kata containers* lance les conteneurs dans une *micro-VM d√©di√©e*, optimis√©e pour d√©marrer vite et con√ßue pour cet usage. Un composant sur la machine h√¥te permet de faire le proxy et d‚Äôenvoyer les instructions √† l‚Äôagent Kata via l‚Äôhyperviseur. Les micro-VMs sont des VMs avec un minimum de fonctionnalit√©s, seulement le strict n√©cessaire pour faire fonctionner des conteneurs.
    ** Ces "sandbox runtimes" permettent d‚Äôisoler les conteneurs, mais au prix de *performances d√©grad√©es*, et parfois plus : 
        *** *gVisor* n‚Äôest pas compatible avec toutes les applications, notamment celles qui n√©cessitent un acc√®s direct aux syst√®me de fichier, et il impactent aussi les performances.
        *** *Nabla container* induit √©galement une baisse de performance et plus important encore, il n‚Äôest pas tout √† fait fini et *ne semble plus tr√®s maintenu*.

    ** *2018/05/22* : *Kata containers* : lancement de la v1.0 le 2018/05/22 (https://techcrunch.com/2018/05/22/the-kata-containers-project-hits-1-0/)

    ** *2018/05/02* : *gVisor* : release initiale en 2018/05/02 (https://en.wikipedia.org/wiki/GVisor)
        *** blog de Google annon√ßant la sortie de gVisor le 2018/05/02 : https://cloud.google.com/blog/products/identity-security/open-sourcing-gvisor-a-sandboxed-container-runtime +
        "To that end, we‚Äôd like to introduce gVisor, a new kind of sandbox that helps provide secure isolation for containers, while being more lightweight than a virtual machine (VM). gVisor integrates with Docker and Kubernetes, making it simple and easy to run sandboxed containers in production environments."
        *** https://www.zdnet.com/article/google-open-sources-gvisor-a-sandboxed-container-runtime/ (2018/05/03) : +
        "With gVisor, Google has introduced a new way to *sandbox containers*. These are containers that provide a *secure isolation boundary* between the host operating system and the application running within the container."

    ** *2018/07* : *Nabla containers* : les Nabla containers ont √©t√© lanc√©s en 2018/07 https://blog.hansenpartnership.com/a-new-method-of-containment-ibm-nabla-containers/ 
    ** Le choix de ces nouveaux runtimes est expliqu√© par Justin Cormarck, le CTO de Docker, √† la KubeCon 2018 : https://static.sched.com/hosted_files/kccna18/c6/KubeCon_%20How%20to%20Choose%20a%20Kubernetes%20Runtime.pdf / https://www.youtube.com/watch?v=OZJkwvAnLb4 +
    Le choix de ces nouveaux containers runtimes est li√© √† l'usage de plus en plus massif de Kubernetes, et des containers qu'il fait tourner : de plus en plus de containers qui tournent impliquant une attention plus pouss√©e √† leur s√©curit√©.

* √©mergence des *daemonless runtimes* et du *rootless* : *podman* (avec *buildah* et *Skopeo*)

    ** *2018/04* : 1ere release de Podman sur le repo https://github.com/containers/podman/releases +
        "*Podman* works with the ‚ÄúrunC‚Äù we mentioned earlier so it works in accordance with the *daemonless* concept." It corrects some "daemon with" problems : 
            *** At the point where no news is received from Daemon, there will be no access to the processes.
            *** All Docker operations are performed by one or more users with the same root privileges. This could create a vulnerability.
            *** Pour une bonne pr√©sentation du pourquoi de podman (les probl√®mes de s√©curit√© de Docker et l'h√©g√©monie de Kubernetes) et une demo de son utilisation, voir https://www.redhat.com/en/blog/say-hello-buildah-podman-and-skopeo (2019/10) +
            "This excites some people who always saw the *monolith daemon that required root access for everything as a problem*. This brings us to the heart of this article ‚Äì the *daemon-less* and largely *rootless* suite of container management tools."
            *** *Podman ne build pas d'image OCI*, il d√©l√®gue cela √† buildah

        *** *Buildah* : Buildah is a common containerize tool for container systems that comply with the OCI (Open Container Initiative) standards, one of the most important reasons for its development being its power in building container images.
            **** 1st release v0.11 2018/01/17
            **** The build commands in Podman are actually a subset of Buildah commands and they use the same codes.
            **** Buildah also works as rootless and daemonless.

        *** Voir √©galement cet excellent article sur les *daemonless container runtimes* Podman et Buildah, ainsi que le lien qui les unit : https://developers.redhat.com/blog/2018/11/20/buildah-podman-containers-without-daemons : +
        "Kubernetes installations can be complex with multiple runtime dependencies and runtime engines. *CRI-O* was created to provide a lightweight runtime for Kubernetes which adds an *abstraction layer between the cluster and the runtime that allows for various OCI runtime technologies*. However you still have the *problem of depending on daemon*(s) in your cluster for builds - I.e. if you are using the cluster for builds you still need a Docker daemon. +
        Enter Buildah. Buildah allows you to have a Kubernetes cluster without any Docker daemon for both runtime and builds. Excellent. But what if things go wrong? What if you want to do troubleshooting or debugging of containers in your cluster? Buildah isn‚Äôt really built for that, what you need is a client tool for working with containers and the one that comes to mind is Docker CLI - but then you‚Äôre back to using the daemon. +
        This is where Podman steps in. Podman allows you to do all of the Docker commands without the daemon dependency. To see examples of Podman replacing the docker command, see Alessandro Arrichiello's Intro to Podman and Doug Tidwell's Podman‚ÄîThe next generation of Linux container tools. +
        With Podman you can run, build (it calls Buildah under the covers for this), modify and troubleshoot containers in your Kubernetes cluster. With the two projects together, you have a well rounded solution for your OCI container image and container needs."
        
        *** *Skopeo* : gestion d'image, au sens de t√©l√©chargement, push et signature (principalement)

-> *2019* : GRANDE ETAPE - les cons√©quences de l'essor de Kubernetes (le d√©clin de Docker et d'autres container runtimes)

* *2019/04* : la CNCF archive le projet rkt, suite √† une adoption utilisateur en forte baisse

* *2019/11/13* : Docker se scinde en 2 : Mirantis rach√®te Docker Enterprise, et Docker Inc se recentre autour de Docker Desktop (et Docker Hub) et l√®ve 35 millions aupr√®s de ses pr√©c√©dents investisseurs Benchmark Capital et Insight Partners. +

    Voici l'explication officielle de Docker : +
    "Docker is ushering in a new era with a return to our roots by focusing on advancing developers‚Äô workflows when building, sharing and running modern applications. As part of this refocus, Mirantis announced it has acquired the Docker Enterprise platform business,‚Äù Docker said in a statement when asked about this change. ‚ÄúMoving forward, we will expand Docker Desktop and Docker Hub‚Äôs roles in the developer workflow for modern apps. Specifically, we are investing in expanding our cloud services to enable developers to quickly discover technologies for use when building applications, to easily share these apps with teammates and the community, and to run apps frictionlessly on any Kubernetes endpoint, whether locally or in the cloud." +
    Pour plus d'explication, voir : 
        ** https://techcrunch.com/2019/11/13/mirantis-acquires-docker-enterprise/
        ** https://www.nextinpact.com/lebrief/40573/10329-docker-se-scinde-en-deux--mirantis-rachete-la-branche---entreprise--

* *2020/02* : project rkt is ended (https://github.com/rkt/rkt/issues/4024), so same thing for appc

* *2020/12* : dockerd qui est d√©pr√©ci√© pour Kubernetes 1.20 (2020/12), et remplac√© par containerd +

    Kubernetes community announced it is deprecating Docker as a container runtime after v1.20. +
    Pour √™tre plus pr√©cis, c'est l'usage du docker daemon (dockerd), au travers de Dockershim, qui est d√©pr√©ci√© par Kubernetes. +
    Pour se "connecter √† Docker", Kubernetes passera √† partir de sa v1.20 par containerd : kubelet appelera directement containerd via son CRI-plugin

* *2020/06* : Gartner predicts that by 2022, more than 75% of global organizations will be running containerized applications in production, up from less than 30% today. +

    Worldwide container management revenue will grow strongly from a small base of $465.8 million in 2020, to reach $944 million in 2024, according to a new forecast from Gartner, Inc. +
    For more details, see https://www.gartner.com/en/newsroom/press-releases/2020-06-25-gartner-forecasts-strong-revenue-growth-for-global-co 

* *2021/01* : Red Hat Enterprise Open Source Report 2021 shows container adoption is already widespread +

    Gartner predicted in 2020 that, by 2022, more than 75% of global organisations will be running containerised applications in production, against 30% in 2020. +
    The analyst‚Äôs figures are reflected in the latest Red Hat Enterprise Open Source Report 2021, which shows container adoption is already widespread. +
    Of the 1,250 IT leaders surveyed, just under 50% said they use containers in production to at least some degree. A further 37% use containers for development only, while just 16% are still evaluating or researching container adoption, according to Red Hat. +
    Voir https://www.computerweekly.com/feature/Containers-for-a-post-pandemic-IT-architecture

* *2022/10* : Pouss√©e des modules Wasm ("containers Javascript") et technical preview Docker PLUS Wasm

*Informations globales √† donner* : 

    * insister sur les Linux Kernel features, pas que les cgroups et namespaces
        ** montrer un exemple de code expliquant que l'on peut "coder un container" uniquement avec ces features (Gist en GO de *Julien Friedman*)
    * parler des "user namespaces qui marchent vraiment" de la version 3.8 du kernel Linux qui ont, cf Solomon Hykes lui-m√™me, permis la sortie de Docker

    * bien d√©finir ce qu'est un container runtime
        ** et d√©finir ce que sont les low-level et high-level container runtimes
    * sch√©ma de l'architecture de Docker A PARTIR DE LA 1.11
    * sch√©ma des relations entre Docker ET Kubernetes avec les container runtimes qu'ils utilisent
    
    * parler des probl√®mes de s√©curit√© qui faisaient peur √† la sortie de Docker avec les containers : "shared OS kernel"

    * pour parler du daemonless, bien rappeler que tous les noms d'outils se terminant par "d" indiquent qu'il s'agit de daemon
        ** https://en.wikipedia.org/wiki/Daemon_(computing) : a daemon is a computer program that runs as a background process, rather than being under the direct control of an interactive user. +
        For example, syslogd is a daemon that implements system logging facility, and sshd is a daemon that serves incoming SSH connections.

    * red√©finir rapidement ce qu'est un "shim" +
    "In tech terms, a shim is a component in a software system, which acts as a *bridge between different APIs*, or as a compatibility layer. A shim is sometimes added when you want to use a third-party component, but you need a little bit of glue code to make it work."

*Ressources √† donner dans le talk* : 

    * la s√©rie d'articles de Ian Lewis
    * Le talk durant lequel Solomon Hykes a introduit Docker, le 2013/06/07 : https://www.youtube.com/watch?v=3N3n9FzebAA[Why Docker ?]
    Le talk a √©t√© donn√© √† la conf√©rence dotScale 2013, juste apr√®s la 1ere publication de Docker.

* TODO : parler du grand probl√®me de Docker, le "monolith daemon that required root access for everything" +
Ce qui a conduit √† l'√©mergence des daemonless et autre rootless comme Podman et Buildah
* TODO : parler des confusions possibles en dockershim et containerd-shim

* TODO : pour Docker, bien dire que quoi est constitu√© Docker aujourd'hui, et faire appara√Ætre un sch√©ma des √©l√©ments du Docker Engine : +
Docker server (avec dockerd, containerd, containerd-shim, runc), l'API (Docker Engine API) et la CLI (ligne de commande "docker") +
S'inspirer par exemple de https://iximiuz.com/en/posts/implementing-container-runtime-shim/
    ** et en ajouter un de plus pour faire appara√Ætre dockershim et le lien avec Kubernetes avant la v1.20
        *** et en ayant sur le m√™me sch√©ma dockershim et dockerd ce serait fantastique
    ** faire un mix avec le sch√©ma du docker engine disponible ici : https://kubernetes.io/blog/2018/05/24/kubernetes-containerd-integration-goes-ga/ (qui fait appara√Ætre dockerd)
    ** Voici un sch√©ma faisant appara√Ætre tous les √©l√©ments de Docker, avec dockershim et dockerd (faire juste attention au sch√©ma indiquant le docker engine, on a l'impression que ce dernier utilise PLUTOT QUE contient dockerd, la CLI et l'API)

=== Outils √† tester pour cr√©er un chronologie en ligne 

Voir le https://elearningindustry.com/top-10-free-timeline-creation-tools-for-teachers. +
Les outils suivants ont l'air bien adapt√©s : 

    * https://timeline.knightlab.com/
    * https://www.timetoast.com/ : je trouve cet outil tr√®s bien, √† tester

== Frise temporelle r√©duite

* *1979* : d√©veloppement de *chroot*, dans la version 7 d'Unix, Le concept d'isolation √©merge. +
-> "le point de d√©part, les pr√©mices de la conteneurisation"
* *2000/03* : *FreeBSD Jails* 
* *2001* : *Linux VServer*
* *2002/08* : Les 1er *Linux namespaces* (mount namespaces) sont ajout√©s au kernel Linux 2.4.19 (2002/08/03)
* *2003* : Google introduced *Borg*, the organization's container cluster management system.
* *2004* : *Solaris Containers*
* *2005* : *Open VZ* (Open Virtuzzo)
* *2006* : d√©but des travaux sur les *Process Containers* chez Google (later renamed *cgroups* / Control Groups)
* *2007/10* : ajout des 1eres briques de l'impl√©mentation des *user namespaces* dans le kernel Linux 2.6.23 par Eric Biederman (Red Hat)
* *2008/01* : GRANDE ETAPE - ajout de la fonctionnalit√© des *cgroups* dans le kernel Linux 2.6.24
* *2008/08* : GRANDE ETAPE - cr√©ation du projet *Linux Containers (LXC)* par des ing√©nieurs d'IBM. +
* *2011* : Warden
* *2013/02* : GRANDE ETAPE - ajout des *user namespaces* au kernel Linux 3.8
* *2013/03/20* : ETAPE MAJEURE - *1ere release de Docker* +
-> "Le monde d√©couvre les containers"
* *2013/10* : LMCTFY
* *2014/02/20* : release de la 1ere version 1.0 de LXC
* *2014/06/07* : GRANDE ETAPE - toute premi√®re release de *Kubernetes* par Google (1er commit GitHub)
* *2014/11* : 1ere release de rkt (prononcer "rocket"), le container runtime cr√©√© par les √©quipes de CoreOS.
* *2015/06/17* : cr√©ation de *WebAssembly*
* *2015/06* : GRANDE ETAPE - *Docker Inc donne la codebase du projet Docker √† l'OCI*, projet de la Linux Fondation, *cr√©√© pour cette occasion*.
* *2015/07* : GRANDE ETAPE - *runc* est publi√© pour la 1ere fois, et son code a *tout de suite donn√© par Docker √† l'OCI* (runc est l'impl√©mentation de r√©f√©rence de la runtime-spec)
* *2015/07/21* : ETAPE MAJEURE - *release de la 1ere version de Kubernetes* par Google, et *cr√©ation de la CNCF*, comme umbrella projet de la Linux Foundation, √† laquelle Kubernetes sera donn√© comme √©l√©ment fondateur.
* *2015/12* : GRANDE ETAPE - Docker introduce *containerd*
* *2016/03/14* : 1ere apparition des cgroups v2 dans le Linux Kernel 4.5
* *2016/04/13* : ETAPE MAJEURE - sortie de la *version 1.11 de Docker* : 1ere release de Docker bas√©e sur containerd et runC (OCI) +
-> cette √©tape marque l'apparition des low-level et des high-level containers runtimes
* *2017/03/06* : GRANDE ETAPE - release de la v1.0 de WebAssembly 1.0 (MVP)
* *2017/03/15* : GRANDE ETAPE - *Docker's donation of containerd project to the CNCF*, and *CoreOS's donation of rkt to the CNCF*
* *2017/04* : Microsoft enabled organizations to run Linux containers on Windows Server.
* *2017/04/12* : apparition des "Distroless" container images chez Google
* *2017/07/19* : Release of the OCI v1.0 runtime and image specifications
* *2017/10* : DockerCon 2017, Docker announced they will support the Kubernetes container orchestrator, same thing for Azure and Amazon.
* *2018* : ETAPE MAJEURE - *L'av√®nement de Kubernetes*, o√π tous les Cloud providers propose leur offre de Kubernetes manag√© +
-> A partir de cette date, les containers deviennent pr√©sents partout, et d√®s lors, la *s√©curit√© des containers* devient un sujet primordial.
    ** 2018 marque √©galement l'apparition des "*sandbox runtimes*" : *Kata containers*, *gVisor*, *Nabla*
        *** *2018/05/02* : release initiale de *gVisor* par Google
        *** *2018/05/22* : lancement de la v1.0 de *Kata containers*
        *** *2018/07* : lancemnet des *Nabla containers*
    ** ainsi que celles des *daemonless runtimes* et du *rootless* : *podman* (avec *buildah* et *Skopeo*)
        *** *2018/04* : 1ere release de Podman
* *2019* : GRANDE ETAPE - les cons√©quences de l'essor de Kubernetes (le d√©clin de Docker et d'autres container runtimes)
* *2019/03/27* : 1st version de WASI (WebAssembly System Interface)
* *2019/04* : la CNCF archive le projet rkt, suite √† une adoption utilisateur en forte baisse
* *2019/11/12* : GRANDE ETAPE - *cr√©ation de la Bytecode Alliance*, partenariat industriel poussant le d√©veloppement de Wasm, tout particuli√®rement en dehors du browser (va marquer le d√©but de l'essort rapide de WebAssembly)
* *2019/11/13* : Docker se scinde en 2 : Mirantis rach√®te Docker Enterprise, et Docker Inc se recentre autour de Docker Desktop (et Docker Hub) 
* *2020/02* : le project rkt est arr√™t√© (https://github.com/rkt/rkt/issues/4024)
* *2020/12* : le daemon docker (dockerd) est d√©pr√©ci√© pour Kubernetes 1.20 (2020/12), et est remplac√© par containerd
* *2021* : les containers sont maintenant partout, et le constat suivent tir√© des √©tudes de Gartner : +
"Gartner predicted in 2020 that, by 2022, more than 75% of global organisations will be running containerised applications in production, against 30% in 2020."
* *2021/05/05* : release de la v1.0.0 (version stable) de la distribution-spec de l'OCI
* *2022/10/24* : Technical Preview de Docker, announces support for WebAssembly, in cooperation with WasmEdge, and joins the Bytecode Alliance as a voting member

~46 dates √† pr√©senter dans les 45 min de ce format conf√©rence.

    * si l'on veut garder 5 min pour les questions, cela laisse 40 min pour le talk, soit ~52 par date
    * 52 sec ne seront pas suffisante pour certaines grandes dates (docker, l'apparition des high et low level container runtimes, WebAssembly, etc.) +
    Il faudra passer rapidement certaines dates pour se concentrer sur les plus importantes

== Points √† aborder durant le talk : 

* Intro : se pr√©senter LinkedIn et GitHub
    ** et redonner l'article dans le blog de Devoxx France depuis 2022 pour la pr√©paration du salon ET la base de connaissance.
    ** et reparler rapidement du talk de veille donn√© √† Devoxx (√† la base DevFest Nantes)
    ** dire que je donnerai l'URL vers mes notes au final, car on ne pourra pas tout aborder

* Commencer par un disclaimer de ce que sera et ne sera pas le talk
    ** on ne va PAS vous apprendre ici √† utiliser tous les outils et techno cit√©es
    ** et je ne suis pas un expert de toutes les techno cit√©es...

* expliquer que certaines technos deviennent des superstars et d'autres meurent...(cf Rocket)
* ins√©rer la courbe de hype du Gartner pour certaines dates importantes de la conteneurisation (release de Docker par exemple)

* Bien d√©finir ce qu'est un runtime
* Donner la phrase introductive "Pour comprendre le pr√©sent..."
* Les PC en 1979 (rares, chers, peu puissants) et bien red√©finir ce qu'est un runtime

* bien r√©expliqer cgroups et namespaces
    ** et redonner les docs de Julia Evans

* Docker par Solomon et parall√®le avec les conteneurs maritimes et le "shipping"
    ** si les conteneurs avaient tous des tailles diff√©rentes, ce serait un fiche lego / tetris que de les charger sur un porte-conteneur (trouver une photo d'un porte-conteneur)
* Sortie de Docker, insister sur les 1ers probl√®mes de s√©curit√©
    ** cycle de hype du Gartner et "pr√©sence oppressante" des OPS dans votre dos quand vous avez pens√© "il faut qu'on pousse √ßa en PROD !"
    ** redonner le bout de code Docker qui permet en 1 ligne une √©l√©vation de privil√®ges, le fameux "Docker root please"
        *** https://github.com/VonC/blog/blob/201711_docker/PITCHME.md
        *** https://github.com/chrisfosterelli/dockerrootplease
            **** https://fosterelli.co/privilege-escalation-via-docker
            **** "A tous ceux qui sont OPS dans la salle, attention, souvenir douloureux..."
            **** "s'il y a des OPS dans la salle, pardonnez-moi, je vais rappeler un souvenir douloureux..." +
            "Ca rappelle des souvenirs √† certains ?" et slide avec "Docker Root Please"

* cr√©ation "√† la main" d'un conteneur en quelques lignes de codes

* donner des d√©tails sur les diff√©rents low and high level container runtimes
* bien r√©expliquer ce qu'est un shim

* reprendre le tr√®s bon sch√©ma de https://www.docker.com/blog/docker-wasm-technical-preview/ faisant appara√Ætre le containerd-wasm-shim
* WASM : la phrase de Solomon sur WASM, "si WASM / WASI avait exist√© en 2008, il n'y aurait pas eu Docker..."
* Donner la ressource https://wasmlabs.dev/articles/docker-without-containers/ et les infos sur Wasm Labs
    ** et donner l'exemple de Wordpress qui tourne dans le browser : https://wordpress.wasmlabs.dev/

* Remerciements : bien indiquer que j'ai utilis√© un tr√®s grand nombre de doc pour ma curation de contenus, et que j'en remercie tous les auteurs, m√™me si je ne les ai pas tous cit√©s.

== Cr√©ation des slides

1. clone du repo avec r√©cup√©ration du contenu du submodule reveal.js :

    git clone --recursive https://github.com/Ardemius/history-of-containerization.git

2. lancement du container :

    docker run -it -v <path-to-the-cloned-repo>/docs:/documents/ asciidoctor/docker-asciidoctor

    docker run -it -v D:\resources\my-talks-and-trainings\history-of-containerization:/documents/ asciidoctor/docker-asciidoctor

3. g√©n√©ration des slides :

    asciidoctor-revealjs history-of-containerization-slides.adoc -o docs/slides.html

[NOTE]
====
Le template de slides pour ce devoxx est disponible sur le repo : https://github.com/quantixx/template-presentation
====

== Lexique

[glossary]
ghcr:: GitHub Container registry



