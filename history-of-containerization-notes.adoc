= L'évolution de la conteneurisation, de 1979 à aujourd'hui
Thomas SCHWENDER <icon:github[] https://github.com/Ardemius/[GitHub] / icon:twitter[role="aqua"] https://twitter.com/thomasschwender[@thomasschwender]>
// Handling GitHub admonition blocks icons
ifndef::env-github[:icons: font]
ifdef::env-github[]
:status:
:outfilesuffix: .adoc
:caution-caption: :fire:
:important-caption: :exclamation:
:note-caption: :paperclip:
:tip-caption: :bulb:
:warning-caption: :warning:
endif::[]
:imagesdir: ./images
:resourcesdir: ./resources
:source-highlighter: highlightjs
:highlightjs-languages: asciidoc
// We must enable experimental attribute to display Keyboard, button, and menu macros
:experimental:
// Next 2 ones are to handle line breaks in some particular elements (list, footnotes, etc.)
:lb: pass:[<br> +]
:sb: pass:[<br>]
// check https://github.com/Ardemius/personal-wiki/wiki/AsciiDoctor-tips for tips on table of content in GitHub
:toc: macro
:toclevels: 4
// To number the sections of the table of contents
//:sectnums:
// Add an anchor with hyperlink before the section title
:sectanchors:
// To turn off figure caption labels and numbers
:figure-caption!:
// Same for examples
//:example-caption!:
// To turn off ALL captions
// :caption:

toc::[]

== Overview

Repo de travail sur l'histoire de la conteneurisation, de ses débuts jusqu'à aujourd'hui.

Talk associé présenté à Devoxx France 2023

== RESSOURCES

* A : https://www.tutorialworks.com/difference-docker-containerd-runc-crio-oci/ : excellente ressource
    ** Jeter un oeil au site parent, qui est vraiment très bien : https://www.tutorialworks.com/
    ** Attention, les schémas ne font pas suffisamment apparaître le *Docker Daemon* (*dockerd*) selon moi

* B : https://blog.engineering.publicissapient.fr/2019/12/23/docker-est-mort-vive-docker/
    ** Dans les 1eres minutes, le Docker Daemon est mieux présenté
        *** reprendre le schéma en 2:01, il est complet AVEC le docker daemon
            **** pour un schéma complet, voir également : http://sysblog.informatique.univ-paris-diderot.fr/wp-content/uploads/2020/03/Docker-2.3.png
        *** on pourrait également faire apparaître le *docker registry* sur le schéma

    ** Très bonne présentation des différents éléments de "Docker", qui est un fork de *Moby*
        *** https://mobyproject.org/ : Moby is an open framework created by Docker to assemble specialized container systems without reinventing the wheel.
            **** Moby permet de pratiquer avec la plomberie de Docker "Docker internals", il n'est pas conseillé si l'on souhaite simplement un moyen simple et rapide de lancer des containers

* Schéma de Docker en 2019 (récent) : https://www.codetd.com/en/article/6502770
    ** montre les 3 parties de *Docker engine*, à savoir : Docker Daemon (dockerd), ContainerD, RunC
        *** NON ! Préférer l'explication fournie plus bas : +
        Docker Engine = Docker Server (implémenté à l'aide de dockerd, qui lui même utilise containerd, qui lui même utilise runc) + API + CLI
        *** Docker Engine est qualifié de *container runtime* par Docker même (https://www.docker.com/products/container-runtime) +
        Je donne cette précision car en parlant de container runtimes, on parle plutôt de containerd et runc
    ** pour des définitions de *ContainerD* et *RunC*, voir https://jfrog.com/knowledge-base/the-basics-7-alternatives-to-docker-all-in-one-solutions-and-standalone-container-tools/
        *** voir également https://docs.docker.com/engine/api/, où il est écrit : +
            "Docker provides an API for interacting with the Docker daemon (called the Docker Engine API), as well as SDKs for Go and Python"
        *** NON ! Plus clair, site même de Docker : https://docs.docker.com/engine/ : 
+
----
Docker Engine acts as a client-server application with:

- A server with a long-running daemon process dockerd.
- APIs which specify interfaces that programs can use to talk to and instruct the Docker daemon.
- A command line interface (CLI) client docker.
----
            **** A l'aide de cette dernière explication, on se rend compte que *Docker Engine* regroupe en fait la *CLI*, la Docker Engine *API* et le *Docker daemon*. +
            Ce dernier est peut-être considéré ici comme *"englobant" containerd et runc*, étant donné que le *schéma d'architecture* https://docs.docker.com/get-started/overview/#docker-architecture montre le docker daemon en lien avec la gestion des images, elle-même liée aux containers
            **** concernant la *Docker Engine API* permettant l'interaction avec le Docker Daemon, voir https://docs.docker.com/engine/api/

    ** autre bon schéma : https://www.aquasec.com/cloud-native-academy/docker-container/docker-architecture/ +
    Ce dernier indique également que le Docker Engine englobe la CLI, l'API de comm avec le docker daemon, et le docker daemon lui-même +
    PAR CONTRE, est-ce toujours totalement d'actualité ? Aucune mention à runc et containerd, ce qui me pose un petit problème...
        *** OUI, c'est bien toujours d'actualité. Vu plus bas, le docker server (implémenté à l'aide de docker daemon) contient bien / utilise bien containerd et runc.
    ** réponse finale ici : https://www.studytrails.com/2018/12/04/docker-architecture-engine-containerd-runc/ +
    *Docker Engine* est bien composé de : 
        *** *Docker Server*, qui est implémenté à l'aide de *docker daemon (dockerd)*, et qui est responsable de la création des images, containers, networks et volumes
            **** Et on considère que le *Docker Server contient containerd et runc*
        *** a *RESTFul API* to talk to the docker server -> donc une API pour parler à dockerd, c'est à dire *Docker Engine API*
        *** une *CLI* (the docker command)
    
    ** *dockerd* is the thing that helps you *work with volumes*, *networking* or even *orchestration*. +
    And of course it *can launch containers* or *manage images* as well, *but containerd is listening on linux socket* and this is *just translated to calls to its GRPC API*. +
    see https://alenkacz.medium.com/whats-the-difference-between-runc-containerd-docker-3fc8f79d4d6e

    ** Une bonne comparaison, rapide et efficace entre Docker et Kubernetes : (https://www.threatstack.com/blog/diving-deeper-into-runtimes-kubernetes-cri-and-shims) +
    "*Docker* is a technology for automating the process of deploying containers. *Kubernetes* is orchestration software that gives us an API to manage how the containers will run." +
    "In a broad sense, Docker runs on nodes, and Kubernetes runs clusters of nodes. To run containers in pods, Kubernetes uses runtimes. Considering what we know about runtimes and how they are defined, Docker can be considered a runtime for Kubernetes, and is a high-level runtime as defined in our last post."

    ** On pourrait également définir Docker très simplement ainsi : *Docker allows to run containerized apps*
        *** Au final, les composants de Docker ont pour but de : *build des images*, et *run des containers*
    ** Une autre très bonne comparaison entre Kubernetes et Docker, Docker Composer et Docker Swarm : https://dzone.com/articles/kubernetes-vs-docker-differences-explained
        *** "Docker, which is the container engine solution, its container orchestration solution Docker Compose, and Docker Swarm, which is a cluster-container orchestration solution."
        *** Kubernetes, the alternative cluster-container solution
        *** *Docker Compose* : Managing multi-containerized applications on the same host is a complicated and time-consuming task. Docker Compose, the orchestration tool for *a single host*, manages multi-containerized applications defined on one host using the Compose file format. 
        *** *Docker Swarm* : Developers can design an application to run on *multiple containers on different hosts*, which creates the need for an orchestration solution for a cluster of containers across different hosts. For this reason, Docker Inc. introduced Docker Swarm.
        *** Kubernetes is more widely used than Swarm in large environments because it provides high availability, load balancing, scheduling, and monitoring to provide an always-on, reliable, and robust solution.
        *** Une TRES BONNE DEFINITION de ce que sont Docker, Docker Composer et Docker Swarm, à quoi ils servent :
        {lb}
        "Docker is an open-source platform to package and *run applications in standard containers* that can run across different platforms in the same behavior. With Docker, *containerized applications are isolated from the host*, which offers the flexibility of delivering applications to any platform running any OS. Furthermore, the Docker engine manages containers and allows them to run simultaneously on the same host.""
        {lb}
        Due to the client-server architecture, Docker consists of client- and server-side components (*Docker client* and *Docker daemon*). The client and the daemon (*Dockerd*) can run on the same system, or you can connect the client to a remote daemon. *The daemon processes the API requests sent by the client* in addition to managing the other Docker objects (containers, networks, volumes, images, etc.).
        {lb}
        *Docker Desktop is the installer of Docker client and daemon* and includes other components like Docker Compose, Docker CLI (Command Line Interface), and more. It can be installed on different platforms: Windows, Linux, and macOS.
        {lb}
        Developers can design an application to run on multiple containers on the same host, which creates *the need to manage multiple containers at the same time*. For this reason, Docker Inc. introduced *Docker Compose*. Docker vs Docker Compose can be summarized as follows: Docker can manage a container, while Compose can manage multiple containers *on one host*.
        {lb}
        *Docker Swarm* or Docker in Swarm mode is *a cluster of Docker engines* that can be enabled after installing Docker. Swarm allows *managing multiple containers on different hosts*, unlike Compose, which allows managing multiple containers on the same host only.

* dockerd vs containerd vs runc : https://stackoverflow.com/questions/46649592/dockerd-vs-docker-containerd-vs-docker-runc-vs-docker-containerd-ctr-vs-docker-c
    ** on y trouve aussi une bonne explication sur *shim* : +
    "(docker-)containerd-shim - After runC actually runs the container, it exits (allowing us to not have any long-running processes responsible for our containers). The shim is the component which sits between containerd and runc to facilitate this."

    ** toujours concernant shim (*docker-containerd-shim*), voir pour une bonne explication : https://www.threatstack.com/blog/diving-deeper-into-runtimes-kubernetes-cri-and-shims +
    Le point essentiel de shim est de permettre "It allows for *daemon-less containers*." +
    "It basically sits as the parent of the container’s processes to facilitate communications, and eliminates the long running runtime processes for containers." +
    "The processes of the *shim and the container* are bound tightly; however, they are *totally separated from the process of the container manager*" +
    "Shim allows a runtime (runC) to exit after the container is started. Without this we would still be subject to long runtime processes."
        *** cet article décrit également très bien Kubernetes et Docker, et les liens entre Kubelet, implémentation de CRI (CRI-O) et un low-level container runtime (très souvent runc)
    ** autre bon article sur le sujet : https://alenkacz.medium.com/whats-the-difference-between-runc-containerd-docker-3fc8f79d4d6e
        *** *containerd-shim* is the *parent process of every container started* and it *also allows daemon-less containers* (meaning you can upgrade docker daemon without restarting all your containers, which was a big pain)
    ** voir également https://oziie.medium.com/something-missed-history-of-container-technology-e978f202464a :
        *** It provides container operation by using runC. It also provides a “*Daemonless container*” environment. This means that there is no need for a long-running runtime process for containers. There are 2 benefits of running a Daemonless container :
            **** *runC* stops after container starts and it doesn’t have to work during the working container process.
            **** *containerd-shim* :  It keeps file information such as stdin (standard input), stdout (standard output), stderr (standard error), even if Docker or containerd becomes inoperable for any reason.

    ** *dockershim* est également très bien expliqué dans https://www.tutorialworks.com/difference-docker-containerd-runc-crio-oci/ : +
    "In tech terms, a shim is a component in a software system, which acts as a *bridge between different APIs*, or as a compatibility layer. A shim is sometimes added when you want to use a third-party component, but you need a little bit of glue code to make it work."

* autre *FANTASTIQUE ressource*, la série d'articles de *Ian Lewis* (2017/12) : https://www.ianlewis.org/en/container-runtimes-part-1-introduction-container-r
    ** en fait, toutes les différentes facettes de l'écosystème des containers y sont présentées (docker, dockerd, containerd, runc)
    ** et une fois lu, voir également https://alenkacz.medium.com/whats-the-difference-between-runc-containerd-docker-3fc8f79d4d6e, qui cite la série d'articles de Ian Lewis

* pour une explication de ce qui a amené aux containers, avec les *namespaces*, les *cgroups* (control groups), l'isolation des appels (*seccomp-bpf*), et finalement les "containers Docker", voir l'excellent article https://jvns.ca/blog/2016/10/10/what-even-is-a-container/
    ** Docker a fourni un wrapping simple et facile d'utilisation de ces fonctionnalités du kernel Linux (et en a également apporté d'autres également)
    ** Regarder absolument le super Zine "How Containers work" de *Julia Evans* (2020) : https://wizardzines.com/zines/containers/ / https://jvns.ca/blog/2020/04/27/new-zine-how-containers-work/
        *** Ce Zine contient une description sympa des *container Kernel features* : 
            **** *pivot_root* : set a process's root directory to a directory with the contents of the container image
                ***** difference between pivot_root and *chroot* : chroot is easy to escape from if you're root and pivot root isn't +
                -> so containers use pivot_root instead of chroot
            **** *cgroups* : limit memory / CPU usage for a group of processes
            **** *namespaces* : allow processes to have their own network / PIDs / users / hostname / mounts / and more !
            **** *seccomp-bpf* : security: prevent dangerous system calls
                ***** seccomp means "secure computing"
                ***** bpf, pour Berkeley Packet Filter, est une extension de seccomp
            **** *capabilities* : security: avoid giving root access +
            Capabilities allow to reduce the privileges of an active process
            **** *overlay filesystems* : optimization to reduce disk space used by containers which are using the same image
            **** quand on utilise *toutes les fonctionnalités précédentes*, on a un *container*


            **** Et un GROS reminder de la définition d'un CONTAINER / CONTENEUR : *A container is a group of processes* (cf julia-evans_containers-vs-VMs.jpg)
                ***** Cette définition est donnée dans le zine de Julia : julia-evans_containers-vs-VMs.jpg +
                Une très bonne ressource, très synthétique et claire sur les différences containers vs VMs
                ***** voir également ce site pour une définition similaire d'un container : https://jessicagreben.medium.com/what-is-the-difference-between-a-process-a-container-and-a-vm-f36ba0f8a8f7
                ***** d'où la définition : *a container is a "just" a group of processes that are isolated from the system (the host) by some means*.
                ***** J'aime bien la définition donné par Jessica : +
                "My personal definition of a container is a group of processes with some cool kernel features sprinkled on top that allow the processes to pretend that they’re running on their own separate machine. While the host machine knows that the container is actually a process, the container thinks that it is a separate machine. These awesome kernel features that make this possible are: namespaces / cgroups and capabilities"


                ***** le site précédent redonne également la définition d'un PROCESS : +
                "*A process represents a running program; it is an instance of an executing program*. A process consists of memory and a set of data structures. The kernel uses these data structures to store important information about the state of the program."

    ** LCC (Les Cast Codeurs) 270 : interview de *Nicolas De Loof* sur Docker et Docker Compose 
        *** Définition de Docker : "Docker est un moyen de lancer des applications, des process, mais on va prendre le process Linux, celui que tu veux faire tourner sur ta machine de PROD, et on va te donner un moyen simple de le faire tourner chez toi tout pareil"
            **** L'idée c'est vraiment, cf Nicolas, "moyen de lancer des applications"

    ** Cf wikipedia (https://en.wikipedia.org/wiki/Cgroups), *cgroups* : +
    "cgroups (abbreviated from control groups) is a Linux kernel feature that limits, accounts for, and isolates the resource usage (CPU, memory, disk I/O, network, etc.) of a collection of processes."
        *** la vidéo https://www.youtube.com/watch?v=sK5i-N34im8[cgroups, namespaces, and beyond: what are containers made from?] de Jérôme PETAZZONI (Docker) explique en détails les différentes fonctionnalités des *cgroups*, *différents types de namespaces*. +
        ATTENTION ! Elle date de 2015 !
            **** Il est également question des *container runtimes* qui sont basés sur les cgroups et les namespaces. +
            Exemples de container runtimes basés sur des namespaces et des cgroups : 
                ***** *LXC* (Linux Containers) : easy for sysadmins / OPS, hard for devs (requires significant elbow grease)
                ***** *systemd-nspawn*
                ***** *Docker*
                ***** *rkt*
                ***** *runC*
                ***** All those container runtimes use the same kernel features (at that time, 2015 ?)
            **** et maintenant des container runtimes qui ne sont PAS basés sur les namespaces et les cgroups : 
                **** *OpenVZ* : by example Travis CI gives you root in OpenVZ
                **** *Jails* / *Zones*
            **** la vidéo de Jérôme se termine par un live demo d'une création de container *à la main* (un début de container)
            **** autre très bonne vidéo de container complètement créé à la main en Go, https://www.youtube.com/watch?v=Utf-A4rODH8, de *Liz RICE* (2016/10)
                **** Voir également le Gist en GO de *Julien Friedman* dont Liz s'est inspirée : https://gist.github.com/julz/c0017fa7a40de0543001 (au final on build un container en ~55 lignes de Go)

        *** le travail sur les *cgroups* a commencé en 2006 chez Google sous le nom "process containers", avant d'être renommé en "control groups" pour éviter toute confusion avec le terme "container" dans un contexte Linux Kernel.
            **** cf Wikipedia (https://en.wikipedia.org/wiki/Cgroups) : +
            "A control group (abbreviated as cgroup) is a *collection of processes that are bound by the same criteria* and associated with a set of parameters or limits. These groups can be *hierarchical*, meaning that *each group inherits limits from its parent group*. The kernel provides access to multiple controllers (also called subsystems) through the cgroup interface;[2] for example, the "memory" controller limits memory use, "cpuacct" accounts CPU usage, etc."

        *** Development and maintenance of cgroups was then taken over by Tejun Heo. Tejun Heo redesigned and rewrote cgroups. This rewrite is now called version 2, the documentation of *cgroups v2* first appeared in Linux kernel 4.5 released on 14 March 2016. +
        Unlike v1, cgroups v2 has only a *single process hierarchy* and discriminates between processes, not threads.

    ** *namespaces* are a Linux feature allowing your processes to be separated from the other processes on the computer. +
    You can have PID namespace, networking namespace, mount namespace. +
    Namespaces can be creates using the `unshare` program.

    ** Pour les *dates* de création des *cgroups* et *namespaces*, voir cet article : https://www.silicon.co.uk/software/open-source/linux-kernel-cgroups-namespaces-containers-186240

        *** *cgroups* were originally developed by Paul Menage and Rohit Seth of Google, and their first features were merged into *Linux 2.6.24* (*2008/01*) +
        Cf Wikipedia (https://en.wikipedia.org/wiki/Cgroups) : 
        "Engineers at Google (primarily *Paul Menage* and *Rohit Seth*) *started the work on this feature in 2006* under the name "*process containers*".[1] In late 2007, the nomenclature changed to "control groups" to avoid confusion caused by multiple meanings of the term "container" in the Linux kernel context, and the control groups functionality was merged into the Linux kernel mainline in *kernel version 2.6.24*, which was *released in January 2008*."

        *** *user namespaces* were originally developed by *Eric Biederman*, and the final major namespace was merged into *Linux 3.8*. +
        Cf Wikipedia (https://en.wikipedia.org/wiki/Linux_namespaces) : 
        "The Linux Namespaces originated in *2002 in the 2.4.19 kernel* (2002/08/03) with work on the *mount namespace* kind. Additional namespaces were added beginning in 2006[2] and continuing into the future. +
        Adequate containers support functionality was finished in kernel *version 3.8* with the *introduction of User namespaces*."
            **** Et l'info très intéressante est ici : ce sont les user namespaces, introduit avec le kernel 3.8 de Linux qui ont changé la donne, et dont Solomon Hykes dit en 2013 (voir la conf ci-dessous, à 16:19) que, ça y est, "les namespaces marchent maintenant".
            **** https://kernelnewbies.org/Linux_3.8 : "*Linux 3.8* was released on Mon, *18 Feb 2013*."

Une bonne définition d'un *container runtime* : +
.https://www.quora.com/What-is-container-runtime-in-Kubernetes/answer/John-Sundarraj
----
A container runtime is a library or software which has the ability to create, deploy and manage containers on its own. Basically, container runtimes are responsible for container lifecycle. It provides simple API layer to create, deploy and manage containers.
----

* *Définition d'un runtime "classique" (ou exécution d'environnement ou Runtime Environment (RTE) ) :* 

    ** https://fr.wikipedia.org/wiki/Environnement_d%27ex%C3%A9cution +
    Un *environnement d'exécution* ou *runtime* est un *logiciel responsable de l'exécution des programmes informatiques* écrits dans un langage de programmation donné. Un runtime offre des services d'exécution de programmes tels que les entrées-sorties, l'arrêt des processus, l'utilisation des services du système d'exploitation, le traitement des erreurs de calcul, la génération d'événements, l'utilisation de services offerts dans un autre langage de programmation, le débogage, le profilage et le ramasse-miette. +
    Contrairement à un logiciel de développement permettant de programmer et développer son application, *un runtime ne permet QUE l'exécution d'un programme*. Un runtime *peut être vu comme une machine virtuelle* : de la même manière qu'un code natif est exécuté par le processeur, un code objet est exécuté par le runtime. Le runtime sert alors à exécuter du code objet en mettant le code natif ad hoc à disposition du processeur pour exécution

    ** *BONNE DEFINITION d'un RUNTIME* : https://www.ionos.fr/digitalguide/sites-internet/developpement-web/definition-environnement-dexecution/
        *** "Dans un environnement d'exécution (Runtime Environment), les logiciels sont exécutés indépendamment du système d'exploitation."
        *** "Un environnement d’exécution charge les applications et leur permet d’être exécutées sur une plateforme. Cette plateforme dispose de toutes les ressources nécessaires pour permettre au programme de fonctionner indépendamment du système d’exploitation."
        *** "Un environnement d’exécution met à la disposition un certain nombre de fonctions de base au service de la mémoire, du réseau ou du matériel. Le runtime environment exécute ces fonctions à la place de l’application, et indépendamment du système d’exploitation."

        *** l'article donne également des EXEMPLES de runtimes bien connus : 
            **** un comble, l'article ne parle pas du plus connu de tous, le *JRE* ! +
            un environnement d’exécution virtuel pour des applications Java, capable d’interpréter le bytecode Java.
            **** *Node.js* : l’environnement d’exécution de JavaScript qui permet d’interpréter le script de programmation sur un serveur. Le concepteur lui-même a émis quelques réserves sur Node.js, et a décidé de créer Deno, un nouvel environnement d’exécution Javascript plus moderne et sécurisé.

            **** *Javascript runtime environment*
            **** *Cygwin* : un environnement d’exécution pour les applications Linux leur permettant de fonctionner aussi sur Windows, macOS et d’autres systèmes d’exploitation.

    ** On peut reparler ici des différences entre JRE et JDK (et JVM) : https://www.digitalocean.com/community/tutorials/difference-jdk-vs-jre-vs-jvm
        *** *JRE* is the implementation of JVM. It provides *a platform to execute java programs*. JRE consists of JVM, Java binaries, and other classes to execute any program successfully.
            **** *JRE = JVM + Java Class Library (JCL)*
            **** JCL : https://en.wikipedia.org/wiki/Java_Class_Library +
            "A comprehensive set of standard class libraries, containing the functions common to modern operating systems" +
            "Almost all of JCL is stored in a single Java archive file called "rt.jar" which is provided with JRE and JDK distributions."
        *** un schéma simple et clair montrant les différences entre JDK, JRE et JVM : https://www.boardinfinity.com/blog/understanding-the-difference-between-jdk-jre-and-jvm/

    ** *Google* parlant de l'*environnement d'exécution Node.js* : https://cloud.google.com/appengine/docs/standard/nodejs/runtime?hl=fr
        *** "L'environnement d'exécution Node.js est la *pile logicielle* chargée d'installer le code de votre service Web et ses dépendances, et d'exécuter votre service."
        *** "Pendant le déploiement, l'environnement d'exécution installe vos dépendances à l'aide de la commande npm install ou, si un fichier yarn.lock existe, de la commande yarn install."
    ** Du même genre : https://www.infoworld.com/article/3210589/what-is-nodejs-javascript-runtime-explained.html
        *** "*Node.js* is a lean, fast, cross-platform JavaScript runtime environment that is useful for both servers and desktop applications."
    ** https://fr.quora.com/Qu%E2%80%99est-ce-que-Node-js-Je-souhaite-une-explication-claire-%C3%A0-ce-sujet
        *** "Node.js lui-même n’est qu’un programme (écrit essentiellement en C/C++) qui est capable de lire du code JavaScript, de le compiler en JIT et d’exécuter les instructions correspondantes. Un tel programme s’appelle techniquement un environnement d’exécution."
    ** *V8 JavaScript engine* pour le Node.js et parallèle avec la JVM pour le JRE : https://www.geeksforgeeks.org/explain-v8-engine-in-node-js/
        *** Bon schéma à reprendre
        *** "V8 is a C++-based open-source JavaScript engine developed by Google. It was originally designed for Google Chrome and Chromium-based browsers (such as Brave) in 2008, but it was later utilized to create Node.js for server-side coding." +
        "V8 is known to be a JavaScript engine because it takes JavaScript code and executes it while browsing in Chrome."
        *** https://www.geeksforgeeks.org/explain-v8-engine-in-node-js/ : *Node.js is referred to as a runtime environment* since it contains everything you need to run a JavaScript program.
        *** https://nodejs.dev/en/learn/the-v8-javascript-engine/ : V8 is the name of the JavaScript engine that powers Google Chrome. It's the thing that takes our JavaScript and executes it while browsing with Chrome. V8 provides the runtime environment in which JavaScript executes. The DOM and the other Web Platform APIs are provided by the browser.

    ** ChatGPT : 
        *** In software development, a runtime (also called runtime environment or runtime system) is a *software layer that provides a platform or framework for running and executing code*. It is responsible for managing the execution of code, including loading, interpreting, and executing program instructions, as well as providing the necessary support for accessing system resources and external libraries. +
        A runtime is typically associated with a specific programming language or technology, and provides the necessary environment for executing code written in that language or technology. For example, a Java runtime environment (JRE) provides the platform for running Java applications, while a Node.js runtime provides the environment for running JavaScript code on a server.

    ** Mes propositions de *DÉFINITION D'UN RUNTIME* : 
        *** un runtime est une couche logicielle permettant l'exécution de programmes (UNIQUEMENT l'exécution)
        *** un runtime est une pile logicielle offrant les services nécessaires à l'exécution d'applications (et UNIQUEMENT l'exécution) indépendamment du système d'exploitation.
            **** le runtime met à la disposition un certain nombre de fonctions de base au service de la *mémoire*, du *réseau* ou du *matériel* ET exécute ces fonctions à la place de l’application, indépendamment du système d’exploitation (le runtime fait donc le lien entre l’application et le système d’exploitation)

* *Docker was released for the 1st time the 2013/03/20*

* *Why we built Docker ?* by Solomon Hykes (foundateur de dotCloud à l'époque, puis Docker) : https://www.youtube.com/watch?v=3N3n9FzebAA (2013/06/07, EXCELLENTE conf, toujours d'actualité). +
Le talk a été donné à la conférence dotScale 2013, juste après la 1ere publication de Docker.
    ** "The automation for carrying coffee beans across the world is better and more reliable than the kind of tools that we use to ship software between computers. That’s pretty embarrassing." +
    -> En résumé : *Livrer est galère, il faut industrialiser les choses*
        *** https://www.linkedin.com/pulse/why-docker-container-built-ankit-utekar/

* Pour d'autres explications par Solomon sur la création de Docker et ses débuts, voir : https://www.youtube.com/watch?v=KF9Awj74dMw

La grande raison de l'époque : *shipping software from A to B, reliably and automatically* (https://youtu.be/3N3n9FzebAA?t=102 01:42)
    ** It has to behave the same way on both machine, and this with technological stack behind applications being more and more complex
    ** and your shipping place can be different depending on developer environment, servers, etc etc. (a lot of possible combinations that result finally in different environments)
    ** 08:39 (https://youtu.be/3N3n9FzebAA?t=519), to avoid all those shipping problems in the (shipping) industry, one day in the 1950s, people agreed on using a standard box, with standard dimensions, weight, way to open the doors, etc etc. AND it resulted with the creation on the container we know today. +
    This "ugly box" allows *separation of concerns* (séparation des responsabilités) : je crée un outil / soft, je veux le shipper, je le mets dans le container, et ma responsabilité pour le shipping s'arrête là. Je ne m'intéresse QU'A mon produit, et PAS au container. +
    De la même façon, pour les personnes en charge du shipping, elles n'ont *pas besoin de s'intéresser à ce qu'il y a dans le container* : elles savent que le container a une taille, un poids, des dimensions données, et que TOUS ces containers peuvent être utilisés via les mêmes moyens standards.

        *** ces "boîtes" ont réellement changé le monde à cette époque : *AVANT, c'était une galère de livrer* du fait de toutes les combinaisons possibles de packaging des produits à livrer.

            **** pour info, article sur *l'histoire des shipping containers* (conteneurs maritimes) : https://mccontainers.com/blog/the-history-of-containers/ +
            "A couple of ISO standards were set to determine terminology, dimensions, classifications, identifiers and so on. Thanks to these standards we nowadays have the 20’ and 40’ containers, the 20’ container (Twenty-foot Equivalent Unit, or TEU) being the standard volume."
            **** la standardisation des containers dans il est fait mention ci-dessus arriva en 1967 (https://fr.wikipedia.org/wiki/Conteneur)
            **** autre article sur la normalisation des conteneurs maritimes : http://geoconfluences.ens-lyon.fr/glossaire/conteneur-conteneurisation +
            "Les conteneurs sont des boîtes métalliques de *dimensions normalisées*, de 20 ou 40 pieds de long (6 ou 12 m) dont l'utilisation pour le groupage de marchandises a été *imaginée en 1956* par un ancien camionneur américain, *Malcolm McLean*, fondateur de l'entreprise Sea Land. Des conteneurs en bois existaient en Europe avant la Seconde guerre mondiale, mais l'utilisation de l'acier, la standardisation des dimensions par l'ISO et l'adoption des conteneurs par l'armée américaine pour sa logistique pendant la guerre du Vietnam ont favorisé sa généralisation à partir des années 1970."
            **** autre article sur le sujet : https://netbox-containers.fr/de-la-boite-qui-servait-au-transport-de-marchandise-a-la-maison-container/
                ***** "26 avril 1956, le premier « porte-conteneurs », nommé l’Ideal-X, parti du port de Newark, avec à son bord 58 containers maritimes chargés par des grues"

        *** We finally wanted to do the same in our IT world for our own shipping needs.
    ** Pour une explication du parallèle de Solomon entre le shipping logiciel et le shipping Software à cette dotScale : https://www.linkedin.com/pulse/why-docker-container-built-ankit-utekar/
        *** "What does a container like Docker do? It allows us, developers, to pack software inside a container image with its dependencies - its own code with required runtimes, libraries, and other configurations. Release engineers don’t need to know about these details now as the software comes packaged with its required dependencies."

    ** Avant, on avait bien déjà des archives comme des jars, rvms, etc. MAIS ce *sandboxing n'était pas complet*

    ** Il y avait bien *les VMs* : cette fois-ci, on a l'appli et on livre finalement toute la machine avec. On est maintenant sûr qu'on a bien le même "contexte" à chaque livraison.
        *** C'est la seule façon de s'assurer de share software in a truly reliable and repeatable way : to *ship the WHOLE system with the application* (because, truly, the system is PART OF the application)
        *** *le souci* avec les VMs est que l'*on ship trop de choses* : hard drives, network interfaces, le total de RAM, le type de processeur, etc. 
            **** Et il ne faut pas que ce soit le développeur qui décide comment l'on va faire fonctionner son application sur toutes les infrastructures possibles, ce n'est pas son rôle (on brise la "separation of concerns" précédente)
                ***** Pour reprendre l'analogie avec les "vrais" containers, cela reviendrait à imposer le modèle de grue avec lequel les décharger, et le modèle de bateau avec lequel les transporter.
                ***** In our IT world, the infrastructure provider is NOT free to make those choices just because you give them to him with your application.
        *** autre souci, *les VMs sont volumineuses* : est-ce facile d'en faire tourner 10 en parallèle ? Non.
            **** En fait, les VMs ont certains des "défauts" des machines classiques : elles mettent du temps à booter, consomment beaucoup de RAM, etc etc. Pas le plus pratique pour un dev dans son travail quotidien.
        
    ** Pour avoir le *meilleur des 2 mondes*, archives et VMs, il faudrait : 
        *** Sandbox the entire system
        *** without machine details
        *** and without the performance hit
        *** Et tout ceci est rendu *possible grâce aux fonctionnalités du kernel Linux*, tout particulièrement le *namespacing* qui a été rendu "réellement" fonctionnel dernièrement
            **** avec ce nouveau namespacing (2013), on peut maintenant isoler n'importe quel process des autres, et faire "croire" à ce process qu'il a sa propre VM (alors qu'il ne l'a pas)
                ***** mais utiliser ces fonctionnalités d'isolation du kernel Linux n'est pas évident, ce qu'il manque est une façon standard de les utiliser (un container standard pour cela) : c'est ce qu'est Docker +
                Docker est avant tout : 
                ***** un standard container format
                ***** simple tools that enable people running the infrastructure to take that container (without knowing what is inside), and then run it

    ** Donc, pour résumer, on a fait Docker dans le but de *shipper*. +
    Il fallait donc que Docker ne soit pas "trop infâme" à utiliser.
        *** on avait déjà les Linux Containers (LXC) avant, mais ce type de Operating System (OS) Containers n'est pas des plus simples à utiliser. Ces derniers sont plutôt à destination des sysadmin, pas des équipes qui "ship"


* https://www.ianlewis.org/en/container-runtimes-part-1-introduction-container-r

    ** developers who want to run apps in containers will need more than just the features that low-level runtimes provide, they need APIs and features around image formats, image management, and sharing images, which are provided by high-level runtimes.
    ** Developers who implement low-level runtimes will say that higher level runtimes like *containerd* and *cri-o* are not actually container runtimes, as from their perspective they outsource the implementation of running a container to *runc*.

* https://www.ianlewis.org/en/container-runtimes-part-2-anatomy-low-level-contai : *LOW LEVEL CONTAINER RUNTIME*

    ** le concept de *low-level container runtime* est mis en avant
    ** Low-level runtimes have a limited feature set and typically perform the low-level tasks for *running a container* (ex : runC)
        ** low-level runtimes are responsible for the mechanics of actually running a container
        ** raison pour laquelle de nombreux low-level container runtime s'appellent "run<quelque chose>"
    ** *Namespaces* let you virtualize system resources, like the file system or networking for each container.
        *** Namespaces are "what you can see"
    ** *cgroups* provide a way to limit the amount of resources, such as CPU and memory, that each container can use.
        *** control groups are "what you can use"
    ** At their core, low-level container runtimes are responsible for setting up these namespaces and cgroups for containers, and then running commands inside those namespaces and cgroups.

    ** Examples of low-level container runtimes : 

        *** *lmctfy* (Let Me Contain That For You) : projet by Google, based on the internal container runtime that *Borg* uses. +
        It supports container hierarchies that use cgroups hierarchies via the container names (a root container called "busybox" could create sub-containers under the name "busybox/sub1" or "busybox/sub2") +
        While lmctfy provides some interesting features and ideas, other runtimes were more usable so Google decided it would be better for the community to focus worked on Docker's "libcontainer" instead of lmctfy.

            *** *libcontainer* : voir http://igm.univ-mlv.fr/~dr/XPOSE2014/Docker/fonctionnement.html +
            "Libcontainer est une bibliothèque écrite en Go pour la création de conteneurs avec des espaces de noms, les groupes de contrôle, les capacités et les contrôles d'accès du système de fichiers. Cette librairie a été développée pour faire le travail de lxc tout en simplifiant l'installation de docker. Elle vous permet de gérer le cycle de vie du conteneur, effectuer des opérations supplémentaires après que le container soit créé."

                **** Libcontainer était une bibliothèque open source développée par Docker pour fournir une interface standardisée pour les fonctionnalités de conteneurisation du noyau Linux.
                **** Libcontainer a permis à Docker de devenir indépendant de la bibliothèque LXC et de fournir une interface *plus flexible* et *plus sûre* pour la *création*, la *gestion* et l'*orchestration* de conteneurs.

            *** *Borg* is Google's cluster manager that runs hundreds of thousands of jobs, from many thousands of different applications, across a number of clusters each with up to tens of thousands of machines. +
            See https://research.google/pubs/pub43438/ for more details
            *** https://faun.pub/the-missing-introduction-to-containerization-de1fbb73efc5 : The libcontainer repository has been archived now. +
            Voir le repo https://github.com/docker-archive/libcontainer, et l'article de blog http://blog.docker.com/2015/06/open-container-project-foundation/. +
            Ce dernier, datant du 2015/06/15 annonce la création de l'*Open Container Projet* (OCP, plus tard *rebaptisé OCI*) et la donation de *runc* par Docker à ce projet. +
            Il y est expliqué que *libcontainer* a été la base de *runc* : +
            "Docker has taken the entire contents of the libcontainer project, including [nsinit], and all modifications needed to make it run independently of Docker, and donated it to this effort. This codebase, called runC, can be found at github/opencontainers/runc. libcontainer will cease to operate as a separate project."

        *** *runC* : most widely used container runtime
            **** originally developed as part of Docker, then extracted as a separate tool and library.
                ***** Il s'agit en fait de l'extraction de libcontainer, voir ci-dessus

                ***** So runC is the low-level runtime that was broken off from Docker.
            **** runC implements the *OCI runtime spec* (Open Container Initiative)
                ***** Pour plus détails, lire l'OCI runtime spec : https://github.com/opencontainers/runtime-spec
            **** https://www.tutorialworks.com/difference-docker-containerd-runc-crio-oci/ : runc is responsible for creating and running the container process.
            **** pour une très bonne ressource sur runc, voir https://www.agaetis.fr/blogpost/les-runtimes-oci
                ***** il est question de *runc* et de *crun* comme des "native runtimes", auxquels on va comparer les "*sandbox runtimes*" que *gVisor*, *Nabla containers* et *Kata containers* +
                Ces derniers sont présentés comme "limitant les interactions entre le conteneur et le kernel pour réduire au maximum la surface d’attaque, permettant ainsi une plus grande isolation. Dans cette catégorie nous allons voir gVisor,  Nabla containers et Kata containers." Donc un accent mis sur la *sécurité*.
                ***** concernant plus précisément runc et crun, il est expliqué que : +
                "Ensuite viens crun, un runtime en C développé par Red Hat. Il est supposé plus performant que runc et est le runtime par défaut de Podman. Même si crun a supporté *cgroups v2* avant runc, ce dernier a rattrapé son retard depuis."

        *** *rkt* (CoreOS *Rocket*):
            **** developed by CoreOS, which was later acquired by Red Hat
            **** provides all features provided by low-level container runtimes, PLUS some high-level ones
            **** As said by Docker : "rkt is CoreOS’s pod-native container engine"
            **** *projet ended / discontinued on 2020/02* and is not maintained anymore.
                ***** for more details on the reasons, see https://github.com/rkt/rkt/issues/4024 +
                The main ones seem to be : 
                ***** the previous development team at CoreOS got dismantled, and post Red Hat acquisition there are no plan to push the development forward
                ***** no more have development plans for rkt (from the new development team)
                ***** a declining engagement from the community

* https://www.ianlewis.org/en/container-runtimes-part-3-high-level-runtimes : *HIGH LEVEL CONTAINER RUNTIMES*

    ** *high-level runtimes* are responsible for *transport and management of container images*, unpacking the image, and *passing off to the low-level runtime* to *run the container*.
    ** Typically, high-level runtimes provide a *daemon* application and an *API* that remote applications can use to logically run containers and monitor them but they sit on top of and *delegate to low-level runtimes* or other high-level runtimes for the actual work. +
    High-level runtimes can also provide *features* that sound low-level, but are *used across individual containers on a machine*. For example, one feature might be the management of network namespaces, and allowing containers to join another container's network namespace.
    ** Exemples of high-level container runtime : 

        *** *Docker*
            **** Originally built as a monolithic daemon, *dockerd*, and the *docker client (Docker CLI)* application. +
            The daemon provided most of the logic of building containers, managing the images, and running containers, along with an API. +
            The command line client could be run to send commands and to get information from the daemon.
            **** It really was *the first* popular runtime to incorporate all of the features needed during the lifecycle of building and running containers, hence its success.
            **** A la base Docker faisait tout, les low et les high level features, mais cela a depuis (v1.11) été scindé en différentes briques, dont containerd et runC. +
            Docker se compose donc maintenant (2021) de docker CLI, dockerd, docker-containerd et docker-runc (les 2 derniers étant simplement des versions packagées de containerd et runc) ainsi que la Docker Engine API
                ***** *dockerd* provides features such as *building images*, and dockerd uses docker-containerd to provide features such as image management and running containers. For instance, Docker's build step is actually just some logic that interprets a Dockerfile, runs the necessary commands in a container using containerd, and *saves the resulting container file system as an image*.

        *** *ContainerD* 
            **** final "d" for daemon, containerd is a daemon
            **** is the high-level runtime that was split off from Docker.
            **** implements downloading images, managing them, and running containers from images. +
            When it needs to *run a container* it unpacks the image into an OCI runtime bundle and *shells out to runc* to run it.
            **** Containerd also provides an API and client application that can be used to interact with it. The *containerd command line client* is *ctr*.
            ****  In contrast with Docker, containerd is *focused solely on running containers*, so it *does NOT provide a mechanism for building containers*.
                ***** Docker was focused on end-user and developer use cases, whereas containerd is focused on operational use cases, such as running containers on servers. Tasks such as building container images are left to other tools.
                ***** traduction simple : containerd can't build images (c'est le travail du daemon dockerd par exemple)
            **** containerd is made *compliant with CRI* through its *CRI plugin* "cri-containerd" (as coming from Docker, it is NOT natively compliant with CRI which comes from Kubernetes)
                ***** see https://github.com/containerd/cri for more details

        *** *rkt*
            **** CAREFUL ! See above, *projet ended in 2020/02* !
            **** rkt is a runtime that has both low-level and high-level features
            **** rkt allows you to *build container images*, *fetch* and *manage container images* in a local repository, and *run them* all from a single command

* https://www.ianlewis.org/en/container-runtimes-part-4-kubernetes-container-run : *KUBERNETES CONTAINER RUNTIMES & CRI*

    ** *Kubernetes* runtimes are *high-level container runtimes* that support the *Container Runtime Interface* (*CRI*) (mandatory to integrate with Kubernetes)

        *** CRI was introduced in Kubernetes 1.5 and acts as a *bridge* between the *kubelet* and the *container runtime*.
            **** *kubelet* : https://kubernetes.io/docs/concepts/overview/components/#kubelet (or https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/) +
            "An *agent* that runs on each node in the cluster. It *makes sure that containers are running in a Pod*. +
            The kubelet takes a set of PodSpecs that are provided through various mechanisms and ensures that the containers described in those PodSpecs are running and healthy. The *kubelet doesn't manage containers which were not created by Kubernetes*"
            **** The kubelet is responsible for managing the container workloads for its node. +
            When it comes to actually run the workload, the kubelet uses CRI to communicate with the container runtime running on that same node. +
            In this way *CRI is simply an abstraction layer* or API that allows you to switch out container runtime implementations instead of having them built into the kubelet.
                ***** *CRI évite donc de coupler kubelet avec le container runtime* (logique, c'est une interface)

    ** The runtime is expected to handle the *management of images* and to *support Kubernetes pods*, as well as *manage the individual containers*. As a consequence, a Kubernetes runtime must be a high-level runtime per our definition in part 3.

    ** *containerd*
        *** implements CRI as a plugin, which is enabled by default
        *** it *supports multiple low-level runtimes* via something called a "runtime handler" starting in version 1.2. The runtime handler is passed via a field in CRI and based on that runtime handler containerd runs an application called a *shim* to start the container. This can be used to run containers using low-level runtimes other than runc, like *gVisor*, *Kata Containers*, or *Nabla Containers*.
            **** *gVisor*, *Kata Containers* et *Nabla Containers* sont souvent comparés car mettant tous en avant une *isolation très forte vis à vis de l'host*
            **** https://alenkacz.medium.com/whats-the-difference-between-runc-containerd-docker-3fc8f79d4d6e : +
            kata containers "is claiming to be all the isolation you love from VMs but that can be easily plugged into all the tooling we have around containers. This means you can spin up these VMs (or kata containers if you wish) through docker or Kubernetes."

    ** *Docker*
        *** Nowadays, Docker itself isn't necessary to support CRI, which is done through the use of containerd

    ** *cri-o*
        *** cri-o is a lightweight *CRI runtime* made as a *Kubernetes specific high-level runtime*.
        *** It supports the management of OCI compatible images and pulls from any OCI compatible image registry.
        *** It *supports runc* and *Clear Containers* as low-level runtimes. +
        It supports other OCI compatible low-level runtimes in theory, but relies on compatibility with the runc OCI command line interface, so in practice it isn't as flexible as containerd's shim API.
        *** *CRI-O* was created to provide a lightweight runtime for Kubernetes which adds an *abstraction layer between the cluster and the runtime that allows for various OCI runtime technologies* (https://developers.redhat.com/blog/2018/11/20/buildah-podman-containers-without-daemons#)

    ** the *CRI Specification*
        *** CRI is a *protocol buffers* and *gRPC* API.
        *** CRI *defines several remote procedure calls* (RPCs) and *message types*. The RPCs are for operations like "pull image" (ImageService.PullImage), "create pod" (RuntimeService.RunPodSandbox), "create container" (RuntimeService.CreateContainer), "start container" (RuntimeService.StartContainer), "stop container" (RuntimeService.StopContainer), etc.
        *** We can interact with a CRI runtime directly using the crictl tool. crictl lets us send gRPC messages to a CRI runtime directly from the command line.

*OCI* : *Image spec* ET *Runtime spec*

    * https://fr.wikipedia.org/wiki/Open_Container_Initiative : L'*Open Container Initiative* (OCI) est un projet de la Fondation Linux visant à *concevoir des normes ouvertes* pour la virtualisation au niveau du système d'exploitation, surtout les *conteneurs Linux*. Il existe actuellement deux spécifications en cours de développement et en cours d'utilisation: la spécification d'exécution (runtime-spec) et la spécification d'image (image-spec).

    * https://www.docker.com/blog/oci-release-of-v1-0-runtime-and-image-format-specifications/ (TRES BONNE RESSOURCE) : +
    "the *Open Container Project* (OCP) was formed to create a set of container standards and was launched under the auspices of the Linux Foundation in *June 2015 at DockerCon*. It became the Open Container Initiative (*OCI*) as the project evolved that Summer."
        ** cet article du blog de Docker, écrit par Patrick CHANEZON le 19/07/2017, contient également le *détail de toutes les contributions de Docker à l'OCI* jusqu'à cette date.
        ** Voici également l'article du blog de Docker annonçant la création de l'OCP (plus tard renommé OCI) : https://www.docker.com/blog/open-container-project-foundation/
            *** Docker will be donating both our base container format and runtime, runC, to this project, to help form the cornerstone for the new technology.  And, in a particularly exciting recent development, the talented people behind *appc* are now joining us as *co-founders*.
                **** Behing appc (App containers) is the people of rkt, and so CoreOS

    * https://faun.pub/docker-containerd-standalone-runtimes-heres-what-you-should-know-b834ef155426 : +
    "Formed in June 2015, the Open Container Initiative (OCI) aims to establish common standards for software containers in order to avoid a potential fragmentation and divisions inside the container ecosystem."

    * https://opencontainers.org/ : +
    "The Open Container Initiative is an open governance structure for the express purpose of *creating open industry standards around container formats and runtimes*." +
    "Established in *June 2015* by Docker and other leaders in the container industry, the OCI currently contains two specifications: the Runtime Specification (*runtime-spec*) and the Image Specification (*image-spec*). The Runtime Specification outlines how to run a “filesystem bundle” that is unpacked on disk. At a high-level an OCI implementation would download an OCI Image then unpack that image into an OCI Runtime filesystem bundle. At this point the OCI Runtime Bundle would be run by an OCI Runtime."

    * cf "https://www.tutorialworks.com/difference-docker-containerd-runc-crio-oci/" : the Open Container Initiative (OCI) which publishes specifications for images and containers.
        *** cf https://faun.pub/docker-containerd-standalone-runtimes-heres-what-you-should-know-b834ef155426, il est bien question de specifications pour des image-spec et runtime-spec
            **** Dans le schéma de https://www.tutorialworks.com/difference-docker-containerd-runc-crio-oci/, il est expliqué que : +
            "OCI provides specifications for container images and running containers."

    * "https://blog.engineering.publicissapient.fr/2019/12/23/docker-est-mort-vive-docker/" voir en 2:06
    * *runc* est une implémentation de la runtime-spec de l'OCI 
        ** runC a été publié pour la première fois en 2015/07 (https://fr.wikipedia.org/wiki/Open_Container_Initiative)
    * image-spec (OCI image spec) : https://github.com/opencontainers/image-spec
    * runtime-spec (OCI runtime spec) : https://github.com/opencontainers/runtime-spec

    * NEWS : 2023 ! Now the *OCI now contains 3 specifications* : runtime-spec, image-spec AND NOW *distribution-spec*
        ** https://opencontainers.org/
        ** https://opencontainers.org/about/overview/ : pour plusieurs définitions récentes, concises et claires, pour les 3 spécifications.
            *** *Runtime Specification* : The Runtime Specification outlines how to run a “filesystem bundle” that is unpacked on disk. At a high-level an OCI implementation would download an OCI Image then unpack that image into an OCI Runtime filesystem bundle. At this point the OCI Runtime Bundle would be run by an OCI Runtime.
            *** *image-spec* : The OCI Image Format contains sufficient information to launch the application on the target platform (e.g. command, arguments, environment variables, etc). This specification defines how to create an OCI Image, which will generally be done by a build system, and output an image manifest, a filesystem (layer) serialization, and an image configuration. +
            At a high level the image manifest contains metadata about the contents and dependencies of the image including the content-addressable identity of one or more filesystem serialization archives that will be unpacked to make up the final runnable filesystem. The image configuration includes information such as application arguments, environments, etc. The combination of the image manifest, image configuration, and one or more filesystem serializations is called the OCI Image.
            *** *distribution specification* : The distribution specification reached v1.0 in May 2020 (2020/05) and was introduced to OCI as an effort to standardize the API to distribute container images. However, the specification is designed generically enough to be leveraged as a distribution mechanism for any type of content.
                **** ERREUR DE DATE DANS LA DOC OFFICIELLE !!!! +
                La v1.0.0 de la 3e spec n'a été rajoutée en 2020/05 mais en 2021/05 ! +
                Cf l'announcement de l'OCI : https://opencontainers.org/posts/announcements/2021-05-04-oci-dist-spec-v1/ +
                L'annoucement tout comme le commit date du *2021/05/05*.
                "*Reaching v1.0 means the OCI Distribution Spec is stable* and ready to serve as the baseline for the distribution of container images across platforms"
                    ***** https://github.com/opencontainers/distribution-spec/releases
                **** ChatGPT : This specification defines how container images are transferred and stored. It specifies the format of the image manifest, the metadata about the image, and the protocol for distributing and fetching images from a registry. It also defines the API for interacting with container registries. +
                The Distribution Specification of the OCI provides a common format for container image metadata and a standard protocol for interacting with container registries. This makes it easier for developers to create and share container images that can be run on any OCI-compliant runtime, while also improving the security and reliability of container image distribution.

        ** *2021/05* : The distribution specification reached v1.0 in May 2021 and was introduced to OCI as an effort to standardize the API to distribute container images. However, the specification is designed generically enough to be leveraged as a *distribution mechanism* for any type of content.
        ** https://github.com/opencontainers/distribution-spec : +
        The OCI Distribution Spec project defines an API protocol to facilitate and standardize the distribution of content.

* Attention ! Fin 2020 (décembre) *deprecation de docker/docker-shim* (dockershim)
    ** oui, c'est bien confirmé : "the Kubernetes community announced it is deprecating Docker as a container runtime after v1.20". +
    Donc, il s'agit bien de la deprecation de *docker-shim*, ET *NON* de containerd-shim, qui n'a rien à voir sinon le "shim" dans le nom. +
    "Docker-shim was a temporary solution proposed by the Kubernetes community to add support for Docker so that it could serve as its container runtime." +
    Pour plus de détails, voir : 
        *** https://kubesphere.io/blogs/dockershim-out-of-kubernetes/
        *** https://linoxide.com/docker-alternative-container-tools/
        *** https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/ (2020/12/02) : l'annonce officielle sur le blog de Kubernetes
        *** voir également ce site de 2018, https://kubernetes.io/blog/2018/05/24/kubernetes-containerd-integration-goes-ga/, qui a de bons *schémas faisant apparaître dockershim*, ainsi que le CRI-plugin de containerd (le tout en lien avec kubelet)
            **** dockershim is "Docker's CRI implementation"
        *** et pour un schéma montrant bien l'avant et l'après dockershim, voir https://medium.com/nttlabs/docker-20-10-59cc4bd59d37 (2020/12/09)

A VOIR / FACULTATIF : 

* Attention ! 2021/09, changement de licence Docker Desktop, on ne peut plus l'utiliser sur Windows en entreprise.
* Parler de Docker Desktop qui conseille maintenant de passer, avec WSL 2, aux Linux Containers ?

=== Les daemonless et rootless containers et podman

* La 1ere release sur le repo https://github.com/containers/podman/releases date du 2018/04/05

* Pour information, pourquoi podman a pour logo un groupe de phoques ("seal" en anglais) ? Parce que, justement, un groupe de phoques est appelé "a seal POD" en anglais... ;)

* Une présentation de *Podman*, à Devoxx France 2021 (2021/10), par Benjamin Vouillaume : https://www.youtube.com/watch?v=pUFIG2AMDhg
    ** Podman est écrit en Go et supporté massivement par RedHat
    ** Podman utilise *crun*, runtime concurrent de *runc* (également OCI), développé pour Podman
        *** crun semble (beaucoup) plus performant que runc
        *** et la raison d'être, le pourquoi avoir eu besoin de créé *crun* sont les *cgroups v2*
        *** que permettent les cgroups v2 ? 
            **** Faire marcher les containers en *rootless*, c'est à dire *sans que nous soyons root* pour démarrer nos conteneurs +
            C'est un peu la *raison d'être de Podman* : fournir une interface semblable à Docker, tout en étant plus sécure avec le rootless (*on ne démarre pas les containers en root*)
    ** Podman est *daemonless*, contrairement à Docker, qui, à partir de la 1.11, fait :
        *** systemd -> 
        *** commande Docker run qui va démarrer le container -> 
        *** le Docker engine qui tourne pour interpréter cette commande -> 
        *** containerd qui tourne pour interpréter les informations que l'Engine va lui envoyer ->
        *** qui lui-même va appeler runc ->
        *** qui lui même va faire tourner votre application
    ** ALORS que Podman va directement appeler crun, et il n'y a pas de daemon. +
        Donc *pas* de processus qui tourne en arrière plan pour gérer nos containers.
        *** L'intérêt du daemonless est la sécurité. +
        Via de l'Audit Log sur Docker, on se rend compte que tout est en root, tout passe par le daemon (dockerd), donc on ne sait pas qui a fait quoi avec le container
    ** *application container* vs *system container*
        *** *application container* : ceux qu'on utilise le plus fréquemment, on met 1 process dans 1 container (ce que recommande Docker)
        *** *system container* : on va démarrer plein de process dans un container, ce dernier étant au final davantage une "micro-VM" mais containerisée. +
        On peut faire des system container avec Docker, mais il n'a pas réellement été fait pour, alors que c'est supporté par Podman. +
        Dans Podman, il est possible de démarrer directement systemd, le process parent d'une arborescence d'un OS, dans un container.
    ** Podman est très adapté à Kubernetes. +
    Podman sait gérer les pods kubernetes, ce que ne sait pas faire Docker
        *** pods : plusieurs containers isolés mais avec des éléments communs (souvent la partie network)
        *** On va pouvoir jouer un fichier Kubernetes existant directement sur podman pour démarrer vos pods

* https://podman.io/ : What is Podman? Podman is a *daemonless* container engine for developing, managing, and running OCI Containers on your Linux System. Containers can either be run as root or in *rootless* mode.

* Pour un bon schéma de l'architecture de Podman : https://ios.dz/from-docker-to-podman/

=== Les tests containers

* Regarder ce que les containers peuvent faire pour les tests d'intégration (*Testcontaineurs*)

=== Histoire des containers

Alors, ce n'est pas une chronologie à proprement parler, mais cet article de Baeldung décrit très bien les débuts de la containerization, avec les namespaces et les cgroups, jusqu'à Docker : +
https://www.baeldung.com/linux/docker-containers-evolution

En fait, on trouve plus d'infos que je ne le pensais via les recherches Google "evolution of containers" et "history of containers", surtout en passant par la recherche images de Google

    ** https://www.redhat.com/en/blog/history-containers (2015/08) TRES BIEN

        *** *2000* : "jails", an early implementation of container technology, was added to FreeBSD
        *** *2001* : container technology made it to the Linux side of the house +
        "Jacques Gélinas created the VServer project, which according to the 0.0 version’s change log allowed “running several general purpose Linux server on a single box with a high degree of Independence and security.”" +
        The Linux-VServer solution was the first effort on Linux to “separate the user-space environment into distinct units (Virtual Private Servers) in such a way that each VPS looks and feels like a real server to the processes contained within.”
        *** *2006* : Paul Menage (Google) travaille sur les "process containers", plus tard renommé en cgroups (control groups) +
        "Cgroups allow processes to be grouped together, and ensure that each group gets a share of memory, CPU and disk I/O; preventing any one container from monopolizing any of these resources"
        *** *fin 2007* : ajout des 1eres briques de l'implémentation des user namespaces dans le kernel Linux 2.6.23 par Eric Biederman (Red Hat) +
        "Red Hatter Eric W. Biederman’s 2008 user namespaces patches being arguably the most complex and one of the most important namespaces in the context of containers. The implementation of user namespaces allows a process to have it’s own set of users and in particular to *allows a process root privileges inside a container, but not outside*."
        *** *2008* : création du projet Linux Containers (LXC) par des ingénieurs d'IBM. +
        "It layered some userspace tooling on top of cgroups and namespaces"
            **** https://fr.wikipedia.org/wiki/LXC : initial release 2008/08/06
        *** *2014/02/20* : release de la 1ere version 1.0 de LXC
        *** *2014/06/07* : toute première release de *Kubernetes* par Google (1er commit GitHub), qui le présente comme une version open source de Borg (Google’s *internal* container cluster-management system)
            **** Kubernetes en peu de mots : un gestionnaire de cluster de conteneurs open source
            **** pour cette date du 06/06, voir https://techcrunch.com/2018/06/06/four-years-after-release-of-kubernetes-1-0-it-has-come-long-way/
            **** Pour plus de détails sur l'histoire de Kubernetes, voir https://blog.risingstack.com/the-history-of-kubernetes/
        *** *2015* : Docker Inc donne la codebase du projet Docker à l'OCI. +
        "In June 2015, Docker the company, the largest contributor to Docker the project (Red Hat is the second), donated the project’s existing codebase to the Open Container Initiative, a lightweight governance structure under the auspices of the Linux Foundation created to *prevent fragmentation* and promote open standards by “cloud giants” including Red Hat."
            **** ce "prevent fragmentation" est très probablement la principal raison du "split" de Docker opéré par Docker Inc
        *** *2015/07/21* : release de la 1ere version de Kubernetes par Google, et création de la CNCF comme umbrella projet de la Linux Foundation. +
        Google versera / contribuera cette v1.0 de Kubernetes à la CNCF en tant que tout 1er projet et élément fondateur. +
        Pour rappel, la CNCF se définit comme "a Linux Foundation project that was founded in 2015 to help advance container technology and align the tech industry around its evolution" (voir https://en.wikipedia.org/wiki/Cloud_Native_Computing_Foundation et https://fr.wikipedia.org/wiki/Cloud_Native_Computing_Foundation)

    ** https://d2iq.com/blog/brief-history-containers (2018/07)

        *** *1970s* : +
        "The *original idea* of a container has been around since the 1970s, when the concept was first employed on *Unix systems* to *better isolate application code*. While useful in certain application development and deployment scenarios, the *biggest drawback* to containers in those early days was the simple fact that they were *anything but portable*." +
        "Back in the 1970s, *early containers created an isolated environment where services and applications could run without interfering with other processes* – producing something akin to a sandbox to test applications, services, and other processes. The original idea was to isolate the container's workload from production systems in way that *enabled developers to test their applications and processes on production hardware without risking disruption to other services*."

    ** *1970s* : Voir également cet EXCELLENT article sur les débuts d'Unix (Unics à l'époque, pour "Uniplexed Information and Computing Service") : +
    https://www.spiria.com/fr/blogue/breves-technos/unix-a-50-ans/
        ** L'article inclut la fameuse photo de *Ken Thompson* et *Dennis Ritchie* à côté d'un PDP-11 chez Bell Labs (vers 1972).
        Pour rappel, ce sont les créateurs respectifs d'Unix et du langage C, et Ken Thompson est également le créateur du premier shell Unix en 1971, sur la 1ere version d'Unix.
            *** https://en.wikipedia.org/wiki/Unix_shell : The first Unix shell was the Thompson shell, sh, written by Ken Thompson at Bell Labs and distributed with Versions 1 through 6 of Unix, from 1971 to 1975.

        ** "Dans les années 60, les Laboratoires Bell participaient à un projet avec le MIT et General Electric ayant pour objectif de mettre au point un *système de temps partagé*. Les ordinateurs de l’époque étant *très coûteux*, il s’agissait *de partager les ressources entre différents utilisateurs*. Insatisfaite de l’avancement du projet, appelé Multiplexed Information and Computing Service (Multics), la direction des Laboratoires Bell s’est finalement retirée en mars 1969. Ken Thompson, un programmeur de Bell qui avait travaillé sur Multics, a alors décidé d’écrire son propre système. Assisté de Dennis Ritchie, le futur créateur du langage C, il imagine un système de hiérarchie de fichiers, les concepts de processus et de fichiers de périphérique, une interface en ligne de commande et de petits utilitaires aux fonctionnalités correspondant à celles de Multics."
            *** *time-sharing* operating system : In computing, time-sharing is the sharing of a computing resource among many users at the same time by means of multiprogramming and multi-tasking

        ** Voir également https://en.wikipedia.org/wiki/Unix, section "History" pour les raisons et le comment de la création d'Unix
            *** "The origins of Unix date back to the mid-1960s when the Massachusetts Institute of Technology, Bell Labs, and General Electric were developing Multics, a time-sharing operating system for the GE-645 mainframe computer.[15] Multics featured several innovations, but also presented severe problems. *Frustrated by the size and complexity of Multics*, but *not by its goals*, individual researchers at Bell Labs started withdrawing from the project. The last to leave were Ken Thompson, Dennis Ritchie, Douglas McIlroy, and Joe Ossanna,[11] who decided to *reimplement their experiences in a new project of smaller scale*. This new operating system was initially without organizational backing, and also without a name." +
            "The new operating system was a single-tasking system"

                *** ChatGPT : À partir des années 1970, Unix a évolué pour permettre l'exécution simultanée de plusieurs programmes par différents utilisateurs, grâce à l'introduction du "time sharing system" (ou système de partage de temps). +
                Le time sharing a été rendu possible grâce à l'utilisation de terminaux, qui permettaient à plusieurs utilisateurs de se connecter à un même ordinateur et d'interagir avec lui en temps réel. Chaque utilisateur avait accès à une "tranche" de temps d'utilisation du processeur et de la mémoire, qui était partagée de manière équitable entre tous les utilisateurs connectés.

    ** https://blog.aquasec.com/a-brief-history-of-containers-from-1970s-chroot-to-docker-2016 (2020/01) (TRES BIEN)

        *** *1979* : "During the development of Unix version 7 in 1979, the *chroot* system call was introduced, changing the root directory of a process and its children to a new location in the filesystem." +
        "This advance was *the beginning process isolation*: segregating file access for each process. Chroot was added to BSD in 1982."
        *** *2000* : FreeBSD Jails +
        At that time, "a small shared-environment hosting provider came up with FreeBSD jails to achieve *clear-cut separation between its services and those of its customers* for *security* and *ease of administration*. FreeBSD Jails allows administrators to partition a FreeBSD computer system into several independent, smaller systems – called “jails” – with the ability to assign an IP address for each system and configuration."
            **** https://en.wikipedia.org/wiki/FreeBSD_jail : "Jails were first introduced in FreeBSD version 4.0, that was released on *March 14, 2000*"
        *** *2001* : Linux VServer +
        "Like FreeBSD Jails, Linux VServer is a jail mechanism that can partition resources (file systems, network addresses, memory) on a computer system. Introduced in 2001, this operating system virtualization that is implemented by patching the Linux kernel. Experimental patches are still available, but the last stable patch was released in 2006."
        *** *2004* : Solaris Containers +
        "In 2004, the first public beta of Solaris Containers was released that combines system resource controls and boundary separation provided by zones, which were able to leverage features like snapshots and cloning from ZFS."
            **** Cf Wikipedia, les principales caractéristiques du système de fichier ZFS pour Solaris sont, entre autres, sa très haute capacité de stockage, et la gestion de volume.
        *** *2005* : Open VZ (Open Virtuzzo) +
        "This is an operating system-level virtualization technology for Linux which uses a patched Linux kernel for virtualization, isolation, resource management and checkpointing. The code was not released as part of the official Linux kernel."
        *** *2006* : Process Containers (later renamed cgroups / Control Groups) +
        "Process Containers (launched by Google in 2006) was designed for limiting, accounting and isolating resource usage (CPU, memory, disk I/O, network) of a collection of processes. It was renamed “Control Groups (cgroups)” a year later and eventually merged to Linux kernel 2.6.24."
        *** *2008* : LXC +
        "LXC (LinuX Containers) was the first, most complete implementation of Linux container manager. It was implemented in 2008 using cgroups and Linux namespaces, and it works on a single Linux kernel *without requiring any patches*."
        *** *2011* : Warden +
        "CloudFoundry started Warden in 2011, using LXC in the early stages and later replacing it with its own implementation. Warden can isolate environments on any operating system, running as a daemon and providing an API for container management. It developed a client-server model to manage a collection of containers across multiple hosts, and Warden includes a service to manage cgroups, namespaces and the process life cycle."
        *** *2013* : LMCTFY +
        "Let Me Contain That For You (LMCTFY) kicked off in 2013 as an open-source version of Google's container stack (based on Borg internals), providing Linux application containers. Applications can be made “container aware,” creating and managing their own subcontainers. Active deployment in LMCTFY stopped in 2015 after Google started contributing core LMCTFY concepts to libcontainer, which is now part of the Open Container Foundation."
            **** initial release 2013/10/13, et final release (0.4.5) 2014/03/28
            **** Open Container Foundation doit être assimilé à l'Open Container Project qui deviendra après l'OCI. +
            -> CONFIRME, c'est bien la même chose
        *** *2013* : Docker +
        "When Docker emerged in 2013, containers exploded in popularity. It’s no coincidence the growth of Docker and container use goes hand-in-hand." +
        "Just as Warden did, Docker also used LXC in its initial stages and later replaced that container manager with its own library, libcontainer. But there’s no doubt that Docker separated itself from the pack by offering an entire ecosystem for container management."
        *** *2014/11* : 1ere release de rkt (https://blog.wescale.fr/2017/01/23/introduction-a-rkt/)
        *** *2017* : *Docker's donation of containerd project to the CNCF*
            **** Cette donation a eu le *2017/03/15*, voir l'annonce de Solomon Hykes https://www.docker.com/blog/docker-donates-containerd-to-cncf/ +
            Cet article explique également que containerd a été créé en 2016/12 : +
            "Back in December 2016, Docker spun out its core container runtime functionality into a standalone component, incorporating it into a separate project called containerd, [...]"
        *** 2017/03 : versement / contribution de rkt à la CNCF
        *** 2017/10 : DockerCon 2017, Docker announced they will support the Kubernetes container orchestrator, and Azure and AWS fell in line, with AKS (Azure Kubernetes Service) and Amazon EKS (Amazon Elastic Kubernetes Service)
        *** *2018* : *L'avènement de Kubernetes*, où tous les Cloud providers commencent à proposer leur offre de Kubernetes managé +
        "The massive adoption of Kubernetes pushed cloud vendors such as AWS, Google with GKE (Google Kubernetes Engine), Azure, and Oracle with Container Engine for Kubernetes, to offer managed Kubernetes services. Furthermore, leading software vendors such as VMWare, RedHat, and Rancher started offering Kubernetes-based management platforms."
        
            **** émergences des "*sandbox runtimes*" : *Kata containers*, *gVisor*, *Nabla* : +
            "We also witnessed emerging hybrid technologies that combine *VM-like isolation with container speed*. Open source projects such as Kata containers, gVisor, and Nabla attempt to provide *secured container runtimes* with lightweight virtual machines that perform the same way container do, but provide *stronger workload isolation*." +
            Voir cet article https://www.agaetis.fr/blogpost/les-runtimes-oci qui expliquent bien ce que sont les "*sandbox runtimes*" comme gVisor, Nabla containers et Kata containers : +
            "Les sandbox runtimes, des runtimes qui *isolent un peu plus les conteneurs de la machine hôte* en limitant les interactions entre le kernel et les conteneurs." +
            L'accent est donc mis sur la *SECURITE* : il faut combler les failles de sécurité des containers popularisés par Docker, c'est la raison d'être des sandbox runtimes. +
            "Les sandbox runtimes *limitent les interactions entre le conteneur et le kernel* pour *réduire au maximum la surface d’attaque*, permettant ainsi une plus grande isolation. Dans cette catégorie nous allons voir gVisor,  Nabla containers et Kata containers. Chacun utilisent une méthode différente pour y arriver". +
            Rappelons cette crainte que l'on avait du temps des débuts de Docker en 2013 : +
            "*Concern and hesitation* arose in the IT community regarding the *security of a shared OS kernel*" (https://searchitoperations.techtarget.com/feature/Dive-into-the-decades-long-history-of-container-technology)
                ***** *gVisor* implémente son propre kernel, *Sentry*, et son composant pour les interactions avec le système de fichiers, *Gofer*
                ***** *Nabla containers* utilise la technique de *l’unikernel* qui consiste à packager l’application avec une bibliothèque d’OS qui remplace un OS normal pour aboutir à une image de machine virtuelle minimale et dédiée à l’application.
                ***** *Kata containers* lance les conteneurs dans une *micro-VM dédiée*, optimisée pour démarrer vite et conçue pour cet usage. Un composant sur la machine hôte permet de faire le proxy et d’envoyer les instructions à l’agent Kata via l’hyperviseur. Les micro-VMs sont des VMs avec un minimum de fonctionnalités, seulement le strict nécessaire pour faire fonctionner des conteneurs.
            **** Ces "sandbox runtimes" permettent d’isoler les conteneurs, mais au prix de *performances dégradées*, et parfois plus : 
                ***** *gVisor* n’est pas compatible avec toutes les applications, notamment celles qui nécessitent un accès direct aux système de fichier, et il impactent aussi les performances.
                ***** *Nabla container* induit également une baisse de performance et plus important encore, il n’est pas tout à fait fini et *ne semble plus très maintenu*.
            **** *Kata containers* : lancement de la v1.0 le 2018/05/22 (https://techcrunch.com/2018/05/22/the-kata-containers-project-hits-1-0/)
            **** *gVisor* : release initiale en 2018/05/02 (https://en.wikipedia.org/wiki/GVisor)
                ***** blog de Google annonçant la sortie de gVisor le 2018/05/02 : https://cloud.google.com/blog/products/identity-security/open-sourcing-gvisor-a-sandboxed-container-runtime +
                "To that end, we’d like to introduce gVisor, a new kind of sandbox that helps provide secure isolation for containers, while being more lightweight than a virtual machine (VM). gVisor integrates with Docker and Kubernetes, making it simple and easy to run sandboxed containers in production environments."
                ***** https://www.zdnet.com/article/google-open-sources-gvisor-a-sandboxed-container-runtime/ (2018/05/03) : +
                "With gVisor, Google has introduced a new way to *sandbox containers*. These are containers that provide a *secure isolation boundary* between the host operating system and the application running within the container."
            **** *Nabla containers* : les Nabla containers ont été lancés en 2018/07 https://blog.hansenpartnership.com/a-new-method-of-containment-ibm-nabla-containers/ 
            **** Le choix de ces nouveaux runtimes est expliqué par Justin Cormarck, le CTO de Docker, à la KubeCon 2018 : https://static.sched.com/hosted_files/kccna18/c6/KubeCon_%20How%20to%20Choose%20a%20Kubernetes%20Runtime.pdf / https://www.youtube.com/watch?v=OZJkwvAnLb4 +
            Le choix de ces nouveaux containers runtimes est lié à l'usage de plus en plus massif de Kubernetes, et des containers qu'il fait tourner : de plus en plus de containers qui tournent impliquant une attention plus poussée à leur sécurité

        *** *2019* : les conséquences de l'essor de Kubernetes (le déclin de Docker)
            **** 2019/04 : la CNCF archive le projet rkt, suite à une adoption utilisateur en forte baisse
            **** 2019/11/13 : Docker se scinde en 2 : Mirantis rachète Docker Enterprise, et Docker Inc se recentre autour de Docker Desktop (et Docker Hub) et lève 35 millions auprès de ses précédents investisseurs Benchmark Capital et Insight Partners. +
            Voici l'explication officielle de Docker : +
            "Docker is ushering in a new era with a return to our roots by focusing on advancing developers’ workflows when building, sharing and running modern applications. As part of this refocus, Mirantis announced it has acquired the Docker Enterprise platform business,” Docker said in a statement when asked about this change. “Moving forward, we will expand Docker Desktop and Docker Hub’s roles in the developer workflow for modern apps. Specifically, we are investing in expanding our cloud services to enable developers to quickly discover technologies for use when building applications, to easily share these apps with teammates and the community, and to run apps frictionlessly on any Kubernetes endpoint, whether locally or in the cloud." +
            Pour plus d'explication, voir : 
                ***** https://techcrunch.com/2019/11/13/mirantis-acquires-docker-enterprise/
                ***** https://www.nextinpact.com/lebrief/40573/10329-docker-se-scinde-en-deux--mirantis-rachete-la-branche---entreprise--
        *** *2020/02* : project rkt is ended (https://github.com/rkt/rkt/issues/4024), so same thing for appc

    ** https://searchitoperations.techtarget.com/feature/Dive-into-the-decades-long-history-of-container-technology (2020/04) (TRES BONNES EXPLICATIONS et bon graphique, complet résumant l'histoire des containers avec ses grandes étapes)

        *** *1979* : développement de chroot, dans la version 7 d'Unix +
        "Chroot marked the beginning of container-style process isolation by restricting an application's file access to a specific directory -- the root -- and its children. A key benefit of chroot separation was improved system security, such that an isolated environment could not compromise external systems if an internal vulnerability was exploited."
        *** *2003* : Google introduced Borg, the organization's container cluster management system. +
        "It relied on the *isolation mechanisms that Linux already had in place*. In those early days in the evolution of containers, *security wasn't much of a concern*. Anyone could see what was going on inside the machine, which enabled a system of accounting for who was using the most memory and how to make the system perform better."
        *** *2006* (et pas 2004, erreur du site) : control groups / cgroups +
        "Nevertheless, this kind of container technology could only go so far. This led to the development of process containers, which became control groups (cgroups) as early as 2004. Cgroups noted the relationships between processes and reined in user access to specific activities and memory volumes. *The cgroups concept was absorbed into the Linux kernel in January 2008*, after which the Linux container technology LXC emerged. Namespaces developed shortly thereafter to provide the basis for container network security -- to hide a user's or group's activity from others."
        *** *2013* : l'émergence de Docker +
        Docker floated onto the scene in 2013 with an easy-to-use GUI, and the ability to package, provision and run container technology. Because Docker enabled multiple applications with different OS requirements to run on the same OS kernel in containers, IT admins and organizations saw opportunity for simplification and resource savings. +
        *Unlike VMs*, containers have a significantly smaller resource footprint, are faster to spin up and down, and require less overhead to manage. VMs must also each encapsulate a fully independent OS and other resources, while *containers share the same OS kernel* and use a proxy system to connect to the resources they need, depending upon where those resources are located. +
        *Concern and hesitation* arose in the IT community regarding the *security of a shared OS kernel*. A vulnerable container could result in a vulnerable ecosystem without the right precautions baked into the container technology. Additional complaints early in the modern evolution of containers bemoaned the lack of data persistence, which is important to the vast majority of enterprise applications. Efficient networking also posed problems, as well as the logistics of regulatory compliance and distributed application management.
        *** *2017* : Kubernetes a le vent en poupe
        *** *2017/04* : Microsoft enabled organizations to run Linux containers on Windows Server. This was a major development for Microsoft shops that wanted to containerize applications and stay compatible with their existing systems.
        *** *2020* : Gartner predicts that by 2022, more than 75% of global organizations will be running containerized applications in production, up from less than 30% today. +
        Worldwide container management revenue will grow strongly from a small base of $465.8 million in 2020, to reach $944 million in 2024, according to a new forecast from Gartner, Inc. +
        For more details, see https://www.gartner.com/en/newsroom/press-releases/2020-06-25-gartner-forecasts-strong-revenue-growth-for-global-co 
        *** *2021* : +
        Gartner predicts that by 2022, more than 75% of global organisations will be running containerised applications in production, up from less than 30% today. The analyst’s figures are reflected in the latest Red Hat Enterprise Open Source Report 2021, which shows container adoption is already widespread. Of the 1,250 IT leaders surveyed, just under 50% said they use containers in production to at least some degree. A further 37% use containers for development only, while just 16% are still evaluating or researching container adoption, according to Red Hat. +
        Voir https://www.computerweekly.com/feature/Containers-for-a-post-pandemic-IT-architecture
            **** Red Hat Enterprise Open Source Report 2021 : https://www.redhat.com/rhdc/managed-files/rh-enterprise-open-source-report-f27565-202101-en.pdf

    ** https://oziie.medium.com/something-missed-history-of-container-technology-e978f202464a (2020/03/31) : TRES BONNE RESSOURCE (que de très bonnes explications), et bon graphique résumant l'histoire des containers avec ses grandes étapes, et bonnes explications des techno impliquées

        *** le graphique vient en fait du site www.plesk.com : +
        https://www.plesk.com/blog/business-industry/infographic-brief-history-linux-containerization/

        *** *2013* : Docker +
        "Docker was introduced in 2013 by an San Francisco company that offers PaaS cloud services named dotCloud as an open-source project, and its founder is Solomon Hykes. When it first came out, *it aimed to convert monolitich applications into image and container structure by using LXC* (Linux containers). Later on, it started to develop his own container runtime, *libcontainer*, and after this stage, libcontainer was started to be used."

        *** *2014/12* : rkt +
        Rkt is a secure and lightweight Docker alternative container system developed by CoreOS. It is built on a container standard known as *App Container* or *appc*. For this reason, rkt images can be run on container systems that support the “appc” format. +
        "Unlike Docker, rkt runs containers with un-privileged users (unlike priority… Unlike Docker…). Thus, even if there is a kernel level deficit and the user can get out of the container, this does not affect other containers and users."
            **** rkt venait répondre à certaines des *problèmatiques de sécurité* existant avec Docker : +
            "As it is known, containers are process groups that can be created by granting some rights to users on the system or by processing with root. In addition, the operation of a user in one container is not seen by the other container. Users are safe in this way as long as there is no abuse on the Linux kernel. However, in some systems such as Docker, *malicious users who can get out of the container through an abuse on the kernel can ruin everything*. Such a risk exists despite measures."

        *** *l'avenir* (et la multiplication des runtimes) : *podman* (avec *buildah* et *Skopeo*), et le passage aux *daemonless* runtimes

            **** "*Podman* works with the “runC” we mentioned earlier so it works in accordance with the *daemonless* concept." It corrects some "daemon with" problems : 
                ***** At the point where no news is received from Daemon, there will be no access to the processes.
                ***** All Docker operations are performed by one or more users with the same root privileges. This could create a vulnerability.
            **** Pour une bonne présentation du pourquoi de podman (les problèmes de sécurité de Docker et l'hégémonie de Kubernetes) et une demo de son utilisation, voir https://www.redhat.com/en/blog/say-hello-buildah-podman-and-skopeo (2019/10) +
            "This excites some people who always saw the *monolith daemon that required root access for everything as a problem*. This brings us to the heart of this article – the *daemon-less* and largely *rootless* suite of container management tools."
            **** *Podman ne build pas d'image OCI*, il délègue cela à buildah

            **** *Buildah* : Buildah is a common containerize tool for container systems that comply with the OCI (Open Container Initiative) standards, one of the most important reasons for its development being its power in building container images.
                ***** 1st release v0.11 2018/01/17
                ***** Buildah is a tool that facilitates building OCI images
                ***** The build commands in Podman are actually a subset of Buildah commands and they use the same codes.
                ***** Buildah also works as rootless and daemonless.
            **** Voir également cet excellent article sur les daemonless container runtimes Podman et Buildah, ainsi que le lien qui les unit : https://developers.redhat.com/blog/2018/11/20/buildah-podman-containers-without-daemons : +
            "Kubernetes installations can be complex with multiple runtime dependencies and runtime engines. *CRI-O* was created to provide a lightweight runtime for Kubernetes which adds an *abstraction layer between the cluster and the runtime that allows for various OCI runtime technologies*. However you still have the *problem of depending on daemon*(s) in your cluster for builds - I.e. if you are using the cluster for builds you still need a Docker daemon. +
            Enter Buildah. Buildah allows you to have a Kubernetes cluster without any Docker daemon for both runtime and builds. Excellent. But what if things go wrong? What if you want to do troubleshooting or debugging of containers in your cluster? Buildah isn’t really built for that, what you need is a client tool for working with containers and the one that comes to mind is Docker CLI - but then you’re back to using the daemon. +
            This is where Podman steps in. Podman allows you to do all of the Docker commands without the daemon dependency. To see examples of Podman replacing the docker command, see Alessandro Arrichiello's Intro to Podman and Doug Tidwell's Podman—The next generation of Linux container tools. +
            With Podman you can run, build (it calls Buildah under the covers for this), modify and troubleshoot containers in your Kubernetes cluster. With the two projects together, you have a well rounded solution for your OCI container image and container needs."

            **** *Skopeo* : gestion d'image, au sens de téléchargement, push et signature (principalement)

            **** https://www.redhat.com/en/blog/say-hello-buildah-podman-and-skopeo : Buildah builds, Podman runs, and Skopeo transfers container images.

    ** vidéos sympas détaillant les débuts de l'histoire des  containers (jusqu'à Docker), et résumant bien l'usage des namespaces et cgroups : https://www.youtube.com/watch?v=9Egk9Tnc28E&list=PL5JFPVMx5WzXB-NlH13_G8R8dgfz564uo&index=2
        *** les vidéos 2 et 3 de la série présentent (rapidement) l'histoire de la containerisation, et l'écosystème Docker avec l'OCI et CRI (de plus, le speaker explique très rapidement comment installer correctement Docker sur Ubuntu en 2021)

    ** https://faun.pub/the-missing-introduction-to-containerization-de1fbb73efc5 (2019/03): là aussi, une bonne explication de l'histoire des containers
        *** avec une bonne explication de l'*architecture actuelle de Docker* (à partir de la 1.11) : +
--
Prior to version 1.11, Docker engine was used to manage volumes, networks, containers, images, etc.. +
Now, Docker architecture is broken into four components:

    * Docker engine,
    * containerd,
    * containerd-shim
    * and runC.

The binaries are respectively called docker, docker-containerd, docker-containerd-shim, and docker-runc.

Let’s enumerate the step to run a container using the new architecture of docker:

    1. Docker engine creates the container (from an image) and passes it to containerd.
    2. Containerd calls containerd-shim
    3. Containerd-shim uses runC to run the container
    4. Containerd-shim allows the runtime (runC in this case) to exit after it starts the container

Using this new architecture we can run “*daemon-less containers*” and we have two advantages:

    * runC can exit after starting the container and we don’t have to have the whole runtime processes running.
    * containerd-shim keeps the file descriptors like stdin, stdout, and stderr open even when Docker and/or containerd die.
--
        *** Pour un autre *très bon schéma de l'architecture actuelle de Docker* : https://iximiuz.com/en/posts/implementing-container-runtime-shim/ (2021/08/24)
            **** L'article également très bien le fonctionnement du shim containerd-shim

=== 2022 - WebAssembly (WASM), modules WASM, containers Javascript

Rappel : WebAssembly = *WASM*

* 2022/05 : https://javascript.developpez.com/actu/333398/Les-conteneurs-JavaScript-surpasseront-ils-les-conteneurs-Linux-Le-createur-de-Node-js-pense-que-les-conteneurs-JavaScript-pourraient-simplifier-l-ecriture-des-services-Web/
    ** "Selon *Dahl* (créateur de Node.js et de Deno), étant donné que les logiciels de serveur dépendent souvent de nombreuses ressources et configurations système, leur déploiement était difficile par le passé. *Les conteneurs Linux* ont alors résolu ce problème. Cependant, Dahl estime qu'un *environnement hermétique similaire peut être trouvé dans le JavaScript du navigateur*, bien qu'à un niveau d'abstraction plus élevé."
    ** "Il est donc logique de considérer JavaScript comme le langage de script universel" +
    "Selon le créateur de Node.js, le conteneur JavaScript n'est pas destiné à traiter la même ampleur de problèmes que les conteneurs Linux."
    ** "En gros, le créateur de Node.js de Deno pense que l*'universalité de JavaScript favorise l'émergence d'une nouvelle abstraction de type conteneur*. Les *conteneurs Linux* ne vont pas disparaître, mais penser en matière de conteneurs JavaScript pourrait simplifier de nombreux services Web."

    ** QUESTION : Parler de Deno, ou trop spécifique / particulier pour une chronologie ?
        *** Semble trop spécifique, de côté

* 2022/11 : LCC 288 : https://lescastcodeurs.com/2022/11/21/lcc-288-l-episode-marathon-mastodonien/
    ** *Docker annonce une technical preview des conteneurs WASM* https://www.docker.com/blog/docker-wasm-technical-preview/ (2022/10)
        *** Nouveau packaging qui wrap un exécutable WASM et le fait tourner avec le runtime *WasmEdge*.
        *** C'est un nouveau type de conteneur.
        *** Il y a beaucoup d'activité autour de WASM, et il y a eu de nombreuses annonces et démonstration lors de la conférence CloudNativeCon et le jour spécial sur WASM, lors de KubeCon.
        *** https://www.infoq.com/news/2022/11/cloud-native-wasm-day/.
        *** Docker utilise Docker Desktop et Docker engine pour démarrer des shims.
        *** Ces shims (processes) lancent soit runc (donc pour faire tourner un conteneur), soit wasmedge pour faire tourner des modules wasm.
        *** Donc docker s'éloigne des conteneurs et essaie de toucher l'orchestration.

    ** 2022/10 - Docker Wasm Technical Preview (le *support par Docker des conteneurs WASM*) : https://www.docker.com/blog/docker-wasm-technical-preview/

        *** As part of this release, we’re also happy to announce that *Docker will be joining the Bytecode Alliance* as a voting member.

        *** *What is Wasm ?* +
        WebAssembly is a relatively new technology that allows you to *compile application code* written in over 40+ languages (including Rust, C, C++, JavaScript, and Golang) and *run it inside sandboxed environments*.
        *** The *original use cases* were focused on *running native code in web browsers*, such as Figma, AutoCAD, and Photoshop. In fact, fastq.bio saw a 20x speed improvement when converting their web-based DNA sequence quality analyzer to Wasm. And Disney built their Disney+ Application Development Kit on top of Wasm! The benefits in the browser are easy to see.
        *** But *Wasm is quickly spreading beyond the browser thanks to the WebAssembly System Interface (WASI)*. Companies like Vercel, Fastly, Shopify, and Cloudflare support using Wasm for running code at the edge, and Fermyon is building a platform to run Wasm microservices in the cloud.


        *** Plutôt que de parler de "containers Javascript", il est peut-être préférable de parler de "Wasm modules" (comme dans l'article)
        *** Bon schéma, simple et clair, du lancement par containerD ET de runC et des containers "classiques" ET de WasmEdge et des modules Wasm via les bons shim.

    ** Docker fait carrément la promotion de son rapprochement avec Wasm, c'est sur la page d'accueil de leur site : 
+
.https://www.docker.com/
----    
WHAT’S NEW
Docker + Wasm = Awesome!
Wasm is a new, FAST, and LIGHT alternative to the Linux/Windows containers you’re using in Docker today — give it a try with the Docker+Wasm Beta.
----
        *** Dans cette dernière définition, c'est surtout le "LIGHT" qui est important : *WASM* rime avec *SECURE*, *PERFORMANT* et *LIGHT*
            **** Et ne pas oublier la *portabilité* (polyglotte)

    ** https://wasmlabs.dev/articles/docker-without-containers/ : Recently Docker announced support for WebAssembly in cooperation with WasmEdge (Wasm runtime)
    
    ** Cf MeetUp TechRocks "A la découverte de WebAssembly", https://www.youtube.com/watch?v=-W2ze6tiTyk, il est indiqué que les 3 principales caractéristiques de WASM sont la *sécurité*, les *performances* et la *portabilité*
        *** Faire le parallèle entre "light" et portabilité
        *** *l'idée de base de WebAssembly* : "faire tourner un autre langage que Javascript dans le browser"
            **** même si Wasm a été initié par la W3C, mais dans la réalité c'est surtout Mozilla qui a participé à l'évolution de Wasm côté browser (avec Firefox)
                ***** Il y a 2 ans, grosse vague de licenciement chez *Mozilla*, toute l'équipe WASM a pris la porte, MAIS a été reprise telle quelle chez *Fastly*
                ***** Mozilla poussait surtout WASM sur le navigateur, et Fastly sur le edge computing (donc côté backend)
                ***** Fastly - https://en.wikipedia.org/wiki/Fastly : Fastly is an American cloud computing services provider. It describes its network as an edge cloud platform, which is designed to help developers extend their core cloud infrastructure to the edge of the network, closer to users. +
                -> Donc fonction de CDN côté Fastly.
                ***** ChatGPT : Fastly is a content delivery network (CDN) that has developed an edge computing platform called Compute@Edge that uses WebAssembly as a runtime for executing custom code at the edge of the network.

        *** Le rôle de *WASI* est un peu d'ouvrir la sandbox WASM aux appels I/O, car à la base WASM ne peut PAS faire d'appels I/O
        *** https://youtu.be/-W2ze6tiTyk?t=2060 : *WASI va permettre de sandboxer WASM* : "quand toi WASM tu vas essayer dans ton langage de faire un file descriptor .open, ça va en fait appeler telle external function qui va appeler le runtime"
            **** le principe est que le module WASM va demander au runtime de faire quelque chose qu'il ne sait pas faire lui-même / ne connaît pas ("j'ai cet appel de fonction là, je ne sais pas ce que c'est, merci de t'en occuper toi runtime")

        *** Philippe Charrière : le langage le plus adapté pour faire du WASM doit actuellement être RUST
        *** super explication du fonctionnement interne de WASM via sa stack à ~52:10 (https://youtu.be/-W2ze6tiTyk?t=3130)

    ** https://www.infoq.com/news/2022/11/cloud-native-wasm-day/
        *** "Wasm was originally developed as a secure sandbox for the web browser. In recent years, it has found many applications on the server-side as a secure, lightweight, fast, and portable alternative to VMs and Linux containers (LXCs)"
        *** *WasmEdge* : "Major Wasm *runtimes* such as WasmEdge and Wasmtime are already committed to supporting and implementing the component model proposal."
        *** De nombreux liens vers les derniers articles sur les Wasm modules
        *** De nombreux langages sont maintenant supportés (ou en voie de l'être) par Wasm : PHP, Java (mais sans GC, donc pour short-lived Java programs), Python, .Net
        *** Wasm gagne même la Data : +
        "Guba Sandor and Dubas Adam from Cisco presented a *Wasm-based plugin system* for the *Envoy Proxy* that is specifically designed for *customizing logging data pipelines*."

* TODO : rappeler rapidement *la force et les avantages de Wasm*
    ** https://www.linkedin.com/pulse/webassembly-un-nouveau-must-pour-le-d%C3%A9veloppement-web-arnaud/?originalSubdomain=fr
        *** "on peut résumer trois grands objectifs pour Wasm : la *rapidité*, la *portabilité* et la *flexibilité*."
        *** One important WebAssembly advantage revolves around *edge computing*.
    
    ** "Why Containers and WebAssembly Work Well Together" : https://www.docker.com/blog/why-containers-and-webassembly-work-well-together/#:~:text=While%20Docker%20excels%20at%20building,creating%20their%20multi%2Darchitecture%20builds.
        *** "While Docker excels at building and deploying cross-platform cloud applications, Wasm is well-suited to portable, binary code compilation for browser-based applications."
        *** Bonne explication des différents types de compute infrastructure : “three different categories of compute infrastructure"
            **** *Virtual machines* (heavyweight class) - AKA the “workhorse” of the cloud, VMs package together an entire operating system — kernels and drivers included, plus code or data — to run an application virtually on compatible hardware. VMs are also great for OS testing and solving infrastructure challenges related to servers, but, they’re often multiple GB in size and consequently start up very slowly.
            **** *Containers* (middleweight class) - Containers make it remarkably easy to package all application code, dependencies, and other components together and run cross-platform. Container images measure just tens to hundreds of MB in size, and start up in seconds.
            **** *WebAssembly* (lightweight class) - A step smaller than containers, WebAssembly binaries are minuscule, can run in a secure sandbox, and start up nearly instantly since they were initially built for web browsers.

NOTE: Définition par Red Hat : *Edge computing* is computing that takes place at or near the physical location of either the user or the source of the data.

Cf l'abstract du meetup Tech Rocks du 23/02/2023 : 
+
.https://www.meetup.com/fr-FR/Meetup-CTO-Tech-Rocks/events/290691230/
--
Tech.Rocks est heureux de vous convier à un meetup virtuel dédié à la découverte de *Web Assembly*, une *technologie de virtualisation* pour implémenter des services *portables* plus *sécurisés* et plus performants.

Ils nous aideront également à mieux comprendre pourquoi Web Assembly s'annonce comme la nouvelle *révolution* pour la *portabilité*, la *sécurité* et la *performance* des applications et services en ligne.
--

* https://www.linkedin.com/pulse/rapport-tendances-2023-didier-girard tendandes 2023 par Didier Girard, avec un section consacrée à *WebAssembly* (WASM)
    ** solution permettant d'exécuter du code bas niveau directement dans le navigateur, offrant des *améliorations spectaculaires des performances*.
    ** solution pour l'exécution, dans le navigateur, d'applications écrites en C++, Rust ou Go.
    ** WebAssembly va aussi bien au-delà du navigateur. +
    Cette technologie peut aussi être utilisée dans les applications de cloud computing et d'Internet des objets (IoT) : *WebAssembly fournit un environnement de sandboxing sécurisé dans lequel le code peut s'exécuter sans avoir d'impact sur les autres programmes*.
        *** Donc une notion proche de celle des conteneurs.

* *Définition de WebAssembly*
    ** https://fr.wikipedia.org/wiki/WebAssembly : 
        *** WebAssembly, abrégé wasm, est un *standard* du World Wide Web (W3C) pour le développement d’applications +
        Le standard consiste en un *bytecode*, sa *représentation textuelle* et un *environnement d'exécution* dans un *bac à sable* compatible avec *JavaScript*. Il peut être *exécuté dans un navigateur Web et en dehors*. +
        Comme WebAssembly ne spécifie qu'un langage de bas niveau, le *bytecode est généralement produit en compilant un langage de plus haut niveau*. +
        De nombreux langages de programmation possèdent aujourd'hui un compilateur WebAssembly : Rust, C, C++, C#, Go, Java, Lua, Python, Ruby, Fortran ou Pascal2.+
        Les navigateurs Web compilent le bytecode wasm dans le langage machine de l'hôte sur lequel ils sont utilisés avant de l'exécuter.

    ** https://medium.com/@gear_techs/what-is-the-webassembly-virtual-machine-why-should-you-use-it-5bfa521e7880
        *** WebAssembly is a way to run programming languages — other than JavaScript — in your web pages. Essentially, *Wasm is just a virtual machine* that runs on all modern browsers.
            *** https://wasmlabs.dev/articles/docker-without-containers/ : "Browser engines integrate a *Wasm virtual machine*, usually called *a Wasm runtime*, which can run the Wasm binary instructions."

* *Bytecode Alliance* (BCA) : un partenariat industriel poussant le développement de Wasm, tout particulièrement en dehors du browser
    ** ChatGPT : Bytecode Alliance : a community-driven organization focused on advancing the use of WebAssembly beyond the web. The alliance includes members like Mozilla, Fastly, Intel, and Red Hat, and is committed to creating a more secure, efficient, and open web.

    ** https://bytecodealliance.org/#what-is-the-bytecode-alliance
        *** "The Bytecode Alliance is a nonprofit organization working to provide state-of-the-art foundations for the *development of runtime environments* and language toolchains where security, efficiency, and modularity can all coexist across a wide range of devices and architectures. We enable innovation in compilers, runtimes, and tooling, *focusing on fine-grained sandboxing*, capabilities-based security, modularity, and standards such as WebAssembly and WASI."

    ** *2019/11/12* - *création de la Bytecode Alliance* +
    https://hacks.mozilla.org/2019/11/announcing-the-bytecode-alliance/ : "*Announcing* the Bytecode Alliance: Building a secure by default, composable future for WebAssembly"
        *** Le MEME article sur le site de BCA : https://bytecodealliance.org/articles/announcing-the-bytecode-alliance
        *** *EXCELLENTE RESSOURCE*, détaillant très bien les objectifs de WebAssembly, tout particulièrement en matière de sécurité, et expliquant son fonctionnement lui permettant d'atteindre ses objectifs.
        *** The founding members of the Bytecode Alliance are Mozilla, Fastly, Intel, and Red Hat.
        *** "Today we announce the formation of the Bytecode Alliance, a new industry partnership coming together to forge WebAssembly’s outside-the-browser future by collaborating on implementing standards and proposing new ones."
            **** Dès 2019, il était déjà question de *sortir Wasm du "seul navigateur"*
        *** Objectif de *SECURITE avant tout* (le 2nd objectif de WASM étant les *PERFORMANCES*) : 
            **** we’re putting in solid, secure foundations that can *make it safe to use untrusted code*, no matter where you’re running it—whether on the cloud, natively on someone’s desktop, or even on a tiny IoT device.
            **** This is a unique moment in time at the dawn of a new technology, where we have the opportunity to fix what’s broken and build new, *secure-by-default foundations for native development* that are *portable and scalable*

        *** Constat actuel : Now *80% of your average code base is built with modules downloaded from registries* like JavaScript’s npm, Python’s PyPI, Rust’s crates.io, and others.

    ** 2021/04/28 : *Transformation de la BCA en 1 fondation* : https://deislabs.io/posts/bytecode-alliance/
        *** "Today, the Bytecode Alliance (BCA) has officially launched as a foundation (with Microsoft as a founding member)"

* *Fonctionnement de WebAssembly* : https://bytecodealliance.org/articles/announcing-the-bytecode-alliance

    ** *Constat côté sécurité* AVANT WASM :
        *** "This memory isolation does make it much safer to run two programs at the same time. But this isn’t perfect security. A malicious program can still mess with certain other resources, like files in the file system."
        *** *VMs* and *containers* were *developed to fix this*. They ensure that something running in one VM or container can’t access the file system of another. And with *sandboxing*, it’s possible to take away access to APIs and syscalls.
        *** BUT, DRAWBACKS of VMs and containers : "All of these techniques are relatively heavyweight. If we wrap hundreds of packages into their own sandboxed process, we’d quickly run *out of memory*. We’d also make the function *calls between the different packages much slower and more complicated*."

    ** Ce qu'apporte WASM à ce niveau : As we’re building out the WebAssembly ecosystem, we can design how the pieces fit together in a way that gives you the kind of isolation that you get with processes or containers, but without the downsides.
        *** *WebAssembly can provide the kind of isolation* that makes it safe to run untrusted code. We can have an architecture that’s like Unix’s many small processes, or *like containers and microservices*. +
        -> BUT this isolation is *much lighter weight*, and the *communication between them isn’t much slower* than a regular function call.
    
    ** Donc WASM est LIGHT, FAST and SECURE, comment est-ce réalisé ? 

        *** *each WebAssembly module is sandboxed by default* : By default, the module doesn’t have access to APIs and system calls. +
        If you want the module to be able to interact with anything outside of the module, you have to explicitly provide the module with the function or syscall.

        *** *memory model* : Unlike a normal binary compiled directly to something like x86, a WebAssembly module doesn’t have access to all of the memory in its process. It *only has access to the chunk of memory that has been assigned to it*.
            **** In theory, *scripting languages would also provide this kind of isolation*. Code in scripting languages can’t directly access the memory in the process. It can only access memory through the variables it has in scope. +
            * But in most scripting language ecosystems, code makes a lot of use of a shared global object*. That’s effectively the same as shared memory. So the conventions in the ecosystem make memory isolation a problem for scripting languages as well. +
            WebAssembly could have had this problem. In the early days, some wanted to establish a convention of passing a shared memory in to every module. But *the community group opted for the more secure convention of keeping memory encapsulated by default*.
            -> This gives us *memory isolation between the two modules*. That means that a malicious module can’t mess with the memory of its parent module.

        *** By default, WebAssembly only has a handful of numeric types, which means you *can only pass single digits across*.
            **** BUT, with *interface types*, modules can communicate using more complex values—things like like strings, sequences, records, variants, and nested combinations of these. +
            That makes it easy for two modules to exchange data, but in a way that’s secure and fast. The WebAssembly engine can do *direct copies between the caller and the callee’s memories*, *without having to serialize and deserialize the data*. And this works even if the two modules aren’t compiled from the same language.

        *** "And those APIs or system calls might have access to shared resources, like the file system. And as we talked about in a previous post, the way that most operating systems handle access to the file system really falls down in providing the security we need here." +
        This is where comes in *WASI*, *the WebAssembly system interface* : +
        That gives us *a way to isolate these different modules from each other* and give them *fine-grained permissions to particular parts of the file system and other resources*, and also *fine grained permissions for different system calls*.

        *** Etude en cours pour "In technical terms, we’re *planning to use a fine grained form of per-module virtualization*  [...] and we’re working on bringing this to WebAssembly"

        *** Tous les points précédents pris en compte, these features make it possible for us to have similar isolation to that of a process, but with much lower overhead. This pattern of usage is what we’re calling a *WebAssembly nanoprocess*.
            **** These *nanoprocesses* —these *little container-like things*— can fit in all sorts of places that regular processes and containers and VMs can’t go.
            **** Ces nanoprocess semblent tout à fait indiqués pour *faire tourner des MICROSERVICES*
            **** "But these services can’t go all the places that libraries can go because they’re too big. They are often running inside a process, which is running inside of a container, which is running on a server. This means that you often have to use a coarse-grained approach when breaking your app apart into these services." +
            -> With wasm, we can *replace microservices with nanoprocesses* and get the same security and language independence benefits. It *gives us the composability of microservices without the weight*. This means we can use a microservices-style architecture and the language interoperability that provides, but with a finer-grained approach to defining the component services.

        *** Plusieurs exemples de sociétés et de use cases utilisant les nanoprocess sont données : 
            **** *Fastly* : They’ve come up with an innovative architecture using WebAssembly nanoprocesses which makes it possible to securely host tens of thousands of simultaneously running programs in the same process. Their approach completely isolates the request from previous requests, ensuring full VM isolation.

    ** De bonnes explications du fonctionnement de WASM dans sa *page Wikipedia anglaise* : https://en.wikipedia.org/wiki/WebAssembly

        *** "WebAssembly implementations usually use either ahead-of-time (AOT) or just-in-time (JIT) *compilation*, but *may also use an interpreter*. While the first implementations have landed in web browsers, there are also non-browser implementations for general-purpose use, including Wasmer,[10] Wasmtime[40] or WAMR,[16] wasm3, WAVM, and many others."
        *** "Because *WebAssembly executables are precompiled*, it is possible to use a variety of programming languages to make them.[42] This is achieved *either through direct compilation to Wasm*, or *through implementation of the corresponding virtual machines in Wasm*. There have been around 40 programming languages reported to support Wasm as a compilation target"

        *** About *WASI* (WebAssembly System Interface) : +
        "WASI is a simple interface (ABI and API) designed by Mozilla intended to be portable to any platform.[79] It provides POSIX-like features like file I/O constrained by capability-based security."

            **** ChatGPT : WASI provides a standard interface between WebAssembly modules and the host environment, allowing WebAssembly to be used outside of the web, for example in serverless computing or edge computing.


            **** *2019/03/27* - Lin Clark (Fastly) Announcing WASI : https://twitter.com/linclark/status/1110920999061594113 +
            A system interface for running WebAssembly outside the web (and inside it too)
            **** En réaction à ce Tweet, Solomon Hykes répondit (https://twitter.com/solomonstre/status/1111004913222324225) : +
            "If WASM+WASI existed in 2008, *we wouldn't have needed to created Docker*. That's how important it is. *Webassembly on the server is the future of computing*. A standardized system interface was the missing link. Let's hope WASI is up to the task!"

            **** *ABI* : *Application Binary Interface* : https://en.wikipedia.org/wiki/Application_binary_interface +
            "An application binary interface (ABI) is an *interface between two binary program modules*. Often, one of these modules is a library or operating system facility, and the other is a program that is being run by a user." +
            "An ABI *defines how data structures or computational routines are accessed in machine code*, which is a low-level, hardware-dependent format. In contrast, an API defines this access in source code, which is a relatively high-level, hardware-independent, often human-readable format."

            **** https://blog.jdriven.com/2022/08/wasi-capability-based-networking/ : très bon article sur le fonctionnement de WASI, avec de bons schémas pouvant être réutilisés
        
        *** 2019 : *Solomon Hykes*, a co-founder of Docker, wrote in 2019, "*If WASM+WASI existed in 2008, we wouldn't have needed to create Docker*. That's how important it is. WebAssembly on the server is the future of computing."[84] Wasmer, out in version 1.0, provides "software containerization, we create universal binaries that work anywhere without modification, including operating systems like Linux, macOS, Windows, and web browsers. Wasm automatically sandboxes applications by default for secure execution".[84]

        *** *Virtual Machine* : +
        Wasm code (binary code, i.e. bytecode) is intended to be run on a portable virtual stack machine (VM).[85] The VM is designed to be faster to parse and execute than JavaScript and to have a compact code representation.[50] An external functionality (like syscalls) that may be expected by Wasm binary code is not stipulated by the standard. It rather provides a way to deliver interfacing via modules by the host environment that the VM implementation runs in.
            **** https://wasmlabs.dev/articles/docker-without-containers/ : Browser engines integrate a Wasm virtual machine, usually called a Wasm runtime, which can run the Wasm binary instructions.

        *** *Wasm program* : +
        A *Wasm program* is designed to be a separate *module* containing collections of various Wasm-defined values and program type definitions. These are expressed in either binary or textual format that both have a common structure.

        *** *Code representation* : +
        In March 2017, the WebAssembly Community Group reached consensus on the initial (MVP) binary format, JavaScript API, and reference interpreter.[96] It defines a *WebAssembly binary format* (*.wasm*), which is not designed to be used by humans, as well as a human-readable *WebAssembly text format* (*.wat*) that resembles a cross between S-expressions and traditional assembly languages.
            **** Philippe Charrière : il faut voir le .wasm comme un .jar avec Java

    ** TRES BON ARTICLE récent (2022/12) présentant WebAssembly, son fonctionnement, ses liens avec les containers et Docker +
    https://wasmlabs.dev/articles/docker-without-containers/
        *** Quelques infos sur Wasm Labs @ VMWare OCTO (https://wasmlabs.dev/) : +
        "Wasm Labs is a team inside VMware's Office of the CTO. We create and contribute to projects that showcase the possibilities of WebAssembly, and help developers adopt this new and exciting technology."
            **** Attention ! OCTO veut ici dire "Office of the CTO" chez VMWare

        *** avec en plus des schémas complets et réutilisables
        *** reprend la fameuse déclaration de Solomon "If WASM+WASI existed in 2008..."
        *** L'article explique aussi l'usage par Docker de WASM à la place des containers Linux
            **** Pour les SLIDES, reprendre le très bon schéma de https://www.docker.com/blog/docker-wasm-technical-preview/ faisant apparaître le containerd-wasm-shim
        *** L'article retrace, très rapidement, l'évolution de la conteneurisation avec son "future" que serait WASM (le successeur des containers)
        *** Pour l'usage d'un langage interprété avec WASM, inclure le très schéma à la section "What about interpreted languages?"
        *** et des comparaisons de taille entre modules WASM et containers Docker
        *** Donne l'exemple de WordPress tournant dans le navigateur avec Wasm : https://wordpress.wasmlabs.dev/

    ** Autre article récent (2023/01/21) donnant avec des demo de WASM (Hello World, appel de WASM depuis Javascript, etc.)

* Très bon article, récent (2022/08) sur le fonctionnement des *WASM nanoprocess* et sur WASI : https://blog.jdriven.com/2022/08/wasi-capability-based-networking/

    ** On commence par une explication du "*secure capability based networking*" de WASI ET une *comparaison* du *network namespace* utilisé par les containers avec le *network isolation model de WASI* : 

        *** "Namespaces are a feature of the linux kernel providing isolation of global resources. There are different kind of *namespaces*, like *cgroup*, mount, process and network namespaces. The *network namespace* is interesting to compare with *WASI’s network isolation model* because it’s the *standard for containerization technologies*."

        *** IMPORTANT : Network namespaces are great for isolating resources for different processes. But *WASI’s nano process model takes isolation a step further*: with WASI you also define the capabilities of guest Wasm modules loaded in as third party libraries. This way you can restrict a module to make a network call to only a certain host, while an other module can only call another host.
            **** Voici l'explication de la *différence entre isolation par les containers et par WASM / WASI*

    ** https://github.com/deislabs/bindle/blob/main/docs/webassembly.md +
    Redonne des définitions issues de BCA (Bytecode Alliance)
        *** Nanoprocesses: We used to refer to this as "tianyan" as well, but have since adopted the language used by BCA. We believe our usage of the term is the same as BCAs: *A nanoprocess is a Wasm module that can execute on its own, but communicate to other Wasm modules via the component architecture* (Module Linking, Interface Types) and *WASI* (IO Streams, IO Arrays). +
        -> Le plus important ici: "*A nanoprocess is a Wasm module* that can execute on its own"            

* *forces / faiblesses de WASM* : 

    ** https://medium.com/@gear_techs/what-is-the-webassembly-virtual-machine-why-should-you-use-it-5bfa521e7880 +
    What are the main *benefits of WebAssembly Virtual Machine* ?
        *** Wasm is extremely fast, efficient and portable. Code can be executed at near-native speed across different platforms.
        *** It’s also very secure as it’s run in a safe, sandboxed environment and like other web code, it will enforce the browsers same-origin and permissionless security policies.

    ** Google Bard : 
        *** advantages of WASM : 
            **** *Speed* : WebAssembly is compiled to machine code, which makes it *faster than containers*, which are typically run in a virtual machine.
                **** Attention ! Je ne suis pas fan de la formulation, le code WASM étant également exécuté sur une machine virtuelle
            **** *Portability*: WebAssembly is a cross-platform technology, which means that it can run on different operating systems and hardware architectures. Containers, on the other hand, are typically tied to a specific operating system.
            **** *Security*: WebAssembly is sandboxed, which means that code running in a WebAssembly container cannot access the host system or other containers. Containers, on the other hand, can potentially access the host system if they are not properly configured.
            **** *Size* (LIGHT) : WebAssembly is a *binary format*, which means that it can be compressed more efficiently than containers, which are typically based on text formats.
        *** drawbacks of WASM : 
            **** *Complexity*: WebAssembly is a complex technology, which can make it difficult to develop and debug applications. Containers, on the other hand, are relatively simple to use.
            **** *Lack of maturity*: WebAssembly is a relatively new technology, which means that it is not as mature as containers. This can make it more difficult to find support and resources for WebAssembly development.
            **** *Limited ecosystem*: The WebAssembly ecosystem is still in its early stages, which means that there are fewer tools and libraries available for WebAssembly development than for containers. 

    ** ChatGPT : 
        *** advantages of WASM
            **** *Portability* : WebAssembly modules are platform-agnostic and can run in any environment that supports WebAssembly, making it easy to run the same code on multiple platforms.
            **** *Performance* : WebAssembly code is compiled to machine code, which can result in faster execution compared to interpreted languages like JavaScript.
            **** *Security* : WebAssembly runs in a sandboxed environment, which makes it more secure compared to running code directly on a host system.
            **** *Smaller size* : WebAssembly modules are typically smaller in size compared to container images, making them faster to download and requiring less storage space.
        *** drawbacks of WASM
            **** *Limited ecosystem* : While the WebAssembly ecosystem is growing, it is still not as mature as the container ecosystem, which has a wide range of tools and services for managing and deploying applications.
            **** *Limited language support* : While WebAssembly supports multiple programming languages, it is still limited compared to the wide range of languages supported by containers.
            **** *Limited networking support* : WebAssembly modules currently have limited support for networking and communication with other modules, which can limit their usefulness for complex distributed applications.
            **** *Limited flexibility* : WebAssembly modules are designed to run within a sandboxed environment, which can limit their flexibility compared to containers, which provide a more complete runtime environment.

        *** advantages of containers
            **** *Flexibility* : Containers provide a complete runtime environment for applications, including the necessary libraries, dependencies, and configurations, which provides a high degree of flexibility for deploying and managing complex applications.
            **** *Mature ecosystem* : The container ecosystem is mature and has a wide range of tools and services for managing and deploying applications.
            **** *Large language support* : Containers support a wide range of programming languages and frameworks, which provides flexibility for building and deploying applications in multiple languages.
            **** *Distributed applications* : Containers are designed for distributed applications, which can run across multiple hosts and environments.
        *** drawbacks of containers
            **** *Larger size* : Container images can be large in size, which can result in slower deployment and require more storage space.
            **** *Security risks* : Containers can introduce security risks, particularly if they are not properly configured or managed.
            **** *Performance overhead* : Containers can introduce a performance overhead compared to running code directly on a host system.
            **** *Limited portability* : While containers are designed to be portable, there can be differences in the underlying host environment that can affect performance and behavior.
        
    ** Forces : 
        *** Philippe Charrière - léger / *LIGHT* : +
        Dans le cadre de FaaS (Function as a Service), WASM est vraiment tout petit : tu peux créer une fonction hôte / host, comme un petit exécutable en Go, qui va le loader, l'appeler et le servir comme un microservice. + 
        Tu vas pouvoir le mettre dans une image Docker "from scratch" (https://codeburst.io/docker-from-scratch-2a84552470c8) qui va peser 20 Mo à tout casser (contre plusieurs centaines de Mo ou Go avec les images habituelles). +
        Du coup, quand on va déployer l'application dans du Kube, déjà ça va se faire vite (l'image étant petite), et en plus quand tu vas la scaller et passer à plusieurs dizaines de pod d'un coup, ça va se faire très rapidement. Et LA on gagne en efficacité.

            **** *Host function* : In the context of WebAssembly, a host function is a function that is implemented in the environment where the WebAssembly module is running, typically outside of the WebAssembly module itself. Host functions can be used to provide access to system resources and services that are not available within the WebAssembly sandbox, such as file I/O, network access, or user input.

        *** Philippe Charrière - *performance* : plus que les perf, c'est surtout l'*efficacité* de WASM qui est intéressante : ce qu'on arrive à faire avec peu de ressources

        *** https://wasmlabs.dev/articles/docker-without-containers/ *portability* : +
        One of the best things about Wasm is its portability. Docker has made traditional containers the way to go when one wants a portable application. However, on top of the big image size, traditional containers are also bound to the architecture of the platform on which they run. Many of us have been through the ups and downs of having to build versions of our software that support different architectures and packaging those in different images for each architecture. +
        -> WebAssembly brings true portability to the picture. You can *build a binary once and run it everywhere*.
            **** Ce qui n'est pas sans rappeler l'une des promesses de Java

    ** Faiblesse importante : jeunesse du projet
        *** nombre de types limité : on ne peut globalement passer que des chiffres à une fonction, ele ne peut te retourner qu'une seule valeur qui sera un chiffre. Si tu veux faire un Hello World en passant ton nom (donc une String) dedans, ce n'est pas évident.
        *** pas de debugging possible actuellement
        *** tout est à faire côté gestion des exceptions

* *Comparaison avec les containers*

    ** Bard : 
        *** WebAssembly is a binary instruction format for a stack-based virtual machine. It is designed to be portable across different platforms and browsers. It is also designed to be fast and efficient.
        *** Containers, on the other hand, are a way of isolating applications from each other on a shared operating system. They are typically used to deploy and manage microservices. Containers are often used in conjunction with orchestration tools like Kubernetes.

    ** ChatGPT : 
        *** WebAssembly is not a replacement for containerization, but it is a technology that has the potential to change the way we think about software development and deployment, especially in the context of the web and serverless computing.
        *** WebAssembly is not a replacement for containerization, which provides a more complete solution for managing and deploying applications across different environments.

        *** WebAssembly is a binary instruction format for a stack-based virtual machine that can run code in a wide variety of environments, including web browsers, serverless computing, edge computing, and even desktop and mobile applications. The goal of WebAssembly is to provide a portable, efficient, and safe way to run code in a sandboxed environment, regardless of the platform or programming language used to write the code.
        *** On the other hand, containers are a form of operating system virtualization that allow multiple applications or services to run in isolation on a single host machine. Containers provide a complete runtime environment for applications, including the necessary libraries, dependencies, and configurations. Containers are designed to be lightweight, portable, and easy to deploy and manage, and they are commonly used in cloud computing, DevOps, and microservices architectures.
        *** While WebAssembly and containers can both be used to deploy and run software, they serve different purposes and have different strengths and weaknesses. WebAssembly is best suited for running code in a portable and efficient way, particularly in the context of the web and edge computing. Containers, on the other hand, provide a more complete and flexible solution for managing and deploying complex applications across different environments, particularly in cloud computing and DevOps.

    ** https://blog.jdriven.com/2022/08/wasi-capability-based-networking/
    "The *network namespace* is interesting to compare with *WASI’s network isolation model* because it’s the *standard for containerization technologies*."

* *Usages de WASM* : 

    ** *Figma* est un des plus gros utilisateurs de WebAssembly
        *** https://fr.wikipedia.org/wiki/Figma : Figma est un éditeur de graphiques vectoriels, un *outil de prototypage collaboratif* parmi les plus utilisés pour tout ce qui est conception d'interface utilisateur et UX (expérience utilisateur)
        *** ChatGPT : Figma is a web-based collaborative design tool that allows teams to create and share designs in real-time. The company uses WebAssembly to improve the performance of its design editor, particularly for computationally-intensive tasks such as rendering complex vector graphics.
            **** Figma announced its use of WebAssembly in 2018/06
        *** 2022/09/15 - *Figma a été racheté par Adobe pour 20 milliards de dollars*
            **** https://news.adobe.com/news/news-details/2022/Adobe-to-Acquire-Figma/default.asp
            **** 2023/02/24 : rachat contesté par les autorités de concurrence américaine et britannique (Adobe étant déjà ultra-dominant dans le secteur du desgin) +
            https://www.usine-digitale.fr/article/antitrust-le-gouvernement-americain-lance-une-action-en-justice-contre-le-rachat-de-figma-par-adobe.N2105011
                ***** Figma est utilisé par 4 millions d’utilisateurs, notamment chez Microsoft, Google, Twitter ou encore Uber.

    ** *Google Earth* maintenant présent sur tous les navigateurs grâce à WebAssembly
        *** 2020/02/26 : https://medium.com/google-earth/google-earth-comes-to-more-browsers-thanks-to-webassembly-1877d95810d6+
        After six months of a public beta, we are now making Google Earth accessible on Firefox, Edge and Opera browsers. This was made possible by *moving Google Earth for Chrome onto WebAssembly* (Wasm), the W3C web standard for bringing native code to the web.

    ** *Fastly* : Fastly is a content delivery network (CDN) that has developed an edge computing platform called Compute@Edge that uses WebAssembly as a runtime for executing custom code at the edge of the network.

    ** *Dropbox* : Dropbox has been exploring the use of WebAssembly as a way to improve the performance of its web-based file viewer, particularly for large files that can take a long time to load and render.
        *** Dropbox announced its use of WebAssembly in 2019/09
    
    ** https://youtu.be/-W2ze6tiTyk?t=1881 +
    *Shopify* : on peut embarquer un interprèteur javascript dans un module WASM, afin de pouvoir dire à tes clients "fournissez moi des fonctions javascript, moi j'ai la garantie qu'elles sont sandboxées, qu'elles s'exécutent dans mon module WASM, et je vous les lance comme ça" -> tu perds en rapidité, mais c'est plus secure
        *** ChatGPT :  the company uses WebAssembly to improve the performance of its online store editor, particularly for computationally-intensive tasks such as rendering product images. +
        In addition to its use of WebAssembly, Shopify has also been actively contributing to the development of the WebAssembly ecosystem, including through its involvement in the WebAssembly Interface Types (WIT) working group and its development of the Wasm-bindgen tool for creating WebAssembly bindings in Rust.

    ** *Adobe*: Adobe has been experimenting with using WebAssembly to improve the performance of its Creative Cloud suite of products.
        *** Adobe announced its use of WebAssembly in 2019/11

    ** *Slack*: Slack has used WebAssembly to improve the performance of its desktop application, particularly for computationally-intensive tasks such as encryption and compression.
        *** Slack announced its use of WebAssembly in 2020/09

    ** *GitHub*: GitHub has been exploring the use of WebAssembly as a way to enable high-performance code execution in its Actions platform for building and testing software.
        *** GitHub announced its support for WebAssembly in 2021/04
    
    ** https://youtu.be/-W2ze6tiTyk?t=1908 +
    Projet de *R&D Azure* : *Kruslet* +
    Dans Kubernetes, au lieu de lancer un container Docker, on va lancer un module WASM
        *** https://cloudblogs.microsoft.com/opensource/2020/04/07/announcing-krustlet-kubernetes-rust-kubelet-webassembly-wasm/
        *** 2020/04/07 : v0.1.0 de Kruslet
        *** C'est en devenir, mais cela montre dans quelle direction sont actuellement en train de creuser les plus grands.
    ** De manière générale, Microsoft travaille énormément sur WebAssembly

    ** Business : gagner des sous en dépensant moins de ressources (WASM est très fort à ce niveau)
    ** Philippe Charrière : la spec n'est pas encore prête, mais, surtout pour tout ce qui est FaaS (Function as a Service) on sent bien qu'il y a un intérêt.

* *Dates importantes de WebAssembly* : 

    ** *2015/06/17* : *création de WebAssembly*, https://fr.wikipedia.org/wiki/WebAssembly
        *** présentation officielle : *2015/06/17*
        *** 1ere démonstration : 2016/03/16
    ** *2017/03/06* : *1st release*, WebAssembly 1.0 is released, https://en.wikipedia.org/wiki/WebAssembly
        *** In March 2017, the design of the minimum viable product (MVP) was declared to be finished and the preview phase ended.
            **** https://www.infoworld.com/article/3176681/webassembly-is-now-ready-for-browsers-to-use.html +
            "WebAssembly, a portable code format that could make for a faster web, has moved to minimum viable product (MVP) status, with browser vendors now able to switch WebAssembly on by default."
            **** WebAssembly became a *World Wide Web Consortium recommendation* on 2019/12/05
    ** *2018/03/06* : The Rust programming language adds support for WebAssembly as a compilation target
        *** Rust added support for compiling to WebAssembly (WASM) as a compilation target with the release of wasm-bindgen. The *wasm-bindgen* project is designed to generate WASM binaries and target the wasm32-unknown-unknown target in Rust so as to build WebAssembly applications.
        *** https://rustwasm.github.io/wasm-bindgen/reference/rust-targets.html +
        The *wasm-bindgen* project is designed to target the wasm32-unknown-unknown target in Rust.
        *** Wasm-bindgen is a tool for creating WebAssembly bindings in Rust. 
            **** *WebAssembly bindings* are a mechanism for allowing programming languages to interoperate with WebAssembly modules. The idea is to provide a way for code written in one language to call functions defined in a WebAssembly module, and vice versa.

    ** *2019/03/27* : 1st version de WASI (WebAssembly System Interface), https://twitter.com/linclark/status/1110920999061594113
    ** *2019/11/12* : *création de la Bytecode Alliance*, partenariat industriel poussant le développement de Wasm, tout particulièrement en dehors du browser. Inclut parmis ses membres des sociétés comme Mozilla, Fastly, Intel, Red Hat, etc.
    ** *2019/12/05* : WebAssembly became a *World Wide Web Consortium recommendation*
    ** *2022/10/24* : Docker announces support for WebAssembly, in cooperation with WasmEdge, and joins the Bytecode Alliance as a voting member
        *** 2022 : l'année ou WASM pousse très fort

    -> *A partir de 2019*, cf les usages à venir, Wasm pousse très fort et est de plus en plus adopté, jusqu'à "l'apothéose" avec le support par Docker en coopération avec WasmEdge. +
    La déclaration de Solomon Hykes sur Wasm / WASI comme étant le "future of computing" date de 2019

* *Wasmer*

    ** standalone runtime environment
    ** Bard : Wasmer is a fast and secure WebAssembly runtime that enables super lightweight containers to run anywhere: from Desktop to the Cloud, Edge and IoT devices. It is a complete WebAssembly stack, providing a runtime, compiler, and toolchain.
    ** ChatGPT : Wasmer is an open-source runtime for executing WebAssembly code on various platforms, including desktop and mobile devices, servers, and even in web browsers. It provides a lightweight and secure sandboxed environment for running WebAssembly code, and supports multiple programming languages, including Rust, C, C++, Python, and JavaScript.

* *Comparison between the size of a WebAssembly module and the one of a container image*
    ** ChatGPT : 
        *** The size of a WebAssembly (Wasm) module and a container image can vary widely depending on the specific application, libraries, and dependencies involved. However, in general, WebAssembly modules are typically much smaller than container images. +
        A typical *container image* might be *several hundred megabytes in size*, although they can range *from a few megabytes* to several gigabytes or more.

        *** Même avec des distroless container image, on restera à hauteur de quelques Mo au minimum, et on montera facilement à 100 ou 200 Mo
            **** gcr.io/distroless/static: This is a distroless container image that contains only the application binary and its runtime dependencies. It is suitable for running static binaries, such as Go binaries : *size of around 3-5 Mo*
            **** gcr.io/distroless/java: This is a distroless container image that contains only the Java Virtual Machine (JVM) and the application code. It is suitable for running Java applications : *size of around 100-200 Mo*

        ** Could you please give me the *size of some famous WebAssembly modules* ? +
        Les exemples donnés ont des tailles variant de *quelques dizaines de Ko à moins d'une dizaine de Mo*
            *** *ffmpeg.wasm*: This is a WebAssembly port of the popular FFmpeg multimedia framework, which can be used to encode, decode, and process audio and video files. The size of the ffmpeg.wasm module is around *3.3 Mo*
            *** *protobuf.wasm*: This is a WebAssembly port of the Google Protocol Buffers serialization format, which can be used to encode and decode structured data. The size of the protobuf.wasm module is around *700 Ko*.

        ** Could you now give me the size of some famous container images ? +
        Les exemples donnés ont des tailles variant de *quelques dizaines de Mo à plusieurs centaines de Mo*
            *** *nginx:latest*: This is the latest version of the popular Nginx web server, which is commonly used as a reverse proxy, load balancer, or static content server. The size of the nginx:latest image is around *133 Mo*.
            *** *openjdk:11-jre-slim*: This is a slimmed-down version of the OpenJDK 11 Java runtime environment, which can be used to run Java applications. The size of the openjdk:11-jre-slim image is around *282 Mo*.

        ** https://wasmlabs.dev/articles/docker-without-containers/
            ** image Docker "classique" pour PHP : 166 Mo
            ** image Docker pour PHP avec une Alpine : 30 Mo
            ** container "from scratch" contenant uniquement le module Wasm avec l'interprêteur PHP : 5.35 Mo

    ** Bard : 
        *** A typical *WebAssembly module* is around *100KB in size*, while a typical container image is around 1GB in size. This is because WebAssembly modules are compiled to machine code, while container images are typically based on text formats.

    ** 2016 : https://stackoverflow.com/questions/38597955/what-docker-image-size-is-considered-too-large +
    In my previous company, we adopted a micro-service architecture and used Docker to implement it. The average size of our Docker images were ~300MB - ~600MB. However my new company is using Docker mostly for development workflow, and the average image size is ~1.5GB - ~3GB. Some of the larger images (10GB+) are being actively refactored to reduce the image size.

* *Ressources sur WASM* : 
    ** Le livre écrit par Philippe Charrière (GitLab) "WASM cooking with Golang" https://wasm.cooking/
        *** Intérêt : package tout l'environnement avec la chaîne de production pour pouvoir être hands-on extrêment rapidement avec Wasm
        *** MAIS de l'aveu de Philippe est déjà obsolète ("ne l'achetez plus", Philippe pense à une suite)

==== slides

* Commencer par une définition de WebAssembly
* Puis présenter les raisons de sa création et ses principaux objectifs : la sécurité, les performances et la légèreté
    ** cf les ressources ci-dessus, c'est vraiment la légèreté qui est la raison de la naissance de WASM par rapport aux VMs et aux containers
    ** "WebAssembly is a way to run programming languages — other than JavaScript — in your web pages"
* Expliquer le principe de WebAssembly nanoprocess
* Citer Solomon Hykes et son talk de 2019 sur "If WASM+WASI existed in 2008, we wouldn't have needed to create Docker"
    ** Voici son tweet de l'époque : https://twitter.com/solomonstre/status/1111004913222324225
* Parler ensuite du lien créé en Docker et WASM, de la création de la Bytecode Alliance, puis de l'adhésion de Docker à cette dernière

TO BE COMPLETED

=== Distroless container images et "scratch" images

* 2023/02/02 : https://bell-sw.com/blog/distroless-containers-for-security-and-size/
    ** So distroless images (which should be called *"almost distroless"* but it won’t make a good selling point, will it?) do contain a Linux distribution, albeit an incredibly stripped-down one, without a package manager, shell, or other typical Linux components. This approach is in line with the modern practices of running an application in the immutable container, meaning that we have no need for introducing system changes or utilizing libraries not involved in app’s work
        *** Bonne image pour comparer les tailles des images distroless qui peuvent quand même vite monter...

    ** Mais les images distroless ont leurs inconvénients : 
        *** Distroless containers are hard to debug due to the absence of shell access.
        *** Configuration of distroless images is anything but simple. To adjust Google's images, you need to know bazel, and adding new packages is complicated without a package manager. +
        A limited range of out-of-the-box distroless images increases the risk of the image being incompatible with your app.

* ChatGPT : 
    ** Distroless container images are container images that contain only the application and its runtime dependencies, but do not include any additional operating system components or package managers. This means that they are stripped down to the bare essentials needed to run the application, and do not include any unnecessary or potentially insecure software.
    ** The idea behind distroless containers is to create a more secure and lightweight environment for running applications in containers. By removing the operating system components and package managers, the attack surface of the container is reduced, making it less vulnerable to security threats. Additionally, the smaller size of the container can improve performance and reduce the amount of resources required to run the application.

    ** Strictly speaking, distroless container images do not exclude a Linux operating system entirely. Instead, they exclude the distribution-specific userland and package manager components that are traditionally included in container images. +
    In a distroless container image, only the essential libraries and dependencies required to run the application are included, rather than a full operating system with its own set of tools and utilities. This approach reduces the size of the image and limits the attack surface of the application. +
    So while distroless container images do still rely on a Linux operating system, they do not include the distribution-specific components that are often unnecessary and can introduce security risks or compatibility issues.

    ** Intérêts des images distroless : 
        *** *Microservices*: Distroless container images are a good fit for microservices architecture, where each microservice is packaged in its own container. By using distroless containers, the size and complexity of each microservice can be reduced, making it easier to deploy and manage.
        *** *Cloud-native applications*: Distroless containers are well-suited for cloud-native applications that are designed to run on distributed, dynamic infrastructure. These types of applications require containers that are lightweight, portable, and secure, and that can be quickly and easily deployed and scaled up or down as needed.
        *** *Security-conscious applications*: Applications that require a high level of security can benefit from using distroless containers because they reduce the attack surface of the application by eliminating unnecessary components and packages.
        *** *Kubernetes deployments*: Distroless containers are a good choice for running with Kubernetes because they are designed to be minimal and self-contained. This makes it easier to manage and scale containerized applications on a Kubernetes cluster.
        *** *Cross-platform compatibility*: Distroless containers are also a good choice for applications that need to run across different platforms or environments. Since they don't rely on distribution-specific packages or tools, they are more compatible with a wider range of systems and environments.

    ** Explication claire et concise : +
    Distroless container images are designed to provide a minimalistic and secure environment for running containerized applications, by excluding distribution-specific components that are often unnecessary and can introduce security risks or compatibility issues. They contain only the essential libraries and dependencies required to run the application, resulting in smaller images and improved performance. This approach also simplifies deployment and ensures compatibility across different systems and environments.

* Bard : 
    ** distroless container images do include a Linux operating system, albeit a very stripped-down one. The Linux kernel is included in all distroless container images, as it is necessary for the container to run. However, other components of the Linux operating system, such as the GNU C Library (glibc), are not included. This makes distroless container images smaller and more secure than traditional container images that include a full Linux operating system.

* https://twitter.com/tracymiranda/status/1636778664871313414?s=20 : Met en avant les Distroless container images créées en 2017 par Google
    ** https://www.trendmicro.com/en_us/research/22/i/enhancing-cloud-security-by-reducing-container-images-through-di.html +
    "Google created Distroless container images, which are images that contain only the application and its runtime dependencies. Unlike images for standard Linux distributions, Distroless container images do not have package managers, shells, or other programs."

* *2017* - *Repo de Google sur les "Distroless" Container images* : https://github.com/GoogleContainerTools/distroless
    ** (almost) "Distroless" images contain only your application and its runtime dependencies. They do not contain package managers, shells or any other programs you would expect to find in a standard Linux distribution.
    ** To be precise, distroless container images do include a Linux operating system, albeit a very stripped-down one.
    ** Le 1er commit de ce repo date du *2017/04/12* : https://github.com/GoogleContainerTools/distroless/commit/02de2e7f738c27d8811003167355d357a146997a

* *"Scratch" images* : https://blog.baeke.info/2021/03/28/distroless-or-scratch-for-go-apps/
    ** Official "scratch" image from Docker : https://hub.docker.com/_/scratch
        *** "This image is most useful in the context of building base images (such as debian and busybox) or super minimal images (that contain only a single binary and whatever it requires, such as hello-world). +
        As of Docker 1.5.0 (specifically, docker/docker#8827), FROM scratch is a no-op in the Dockerfile, and will not create an extra layer in your image (so a previously 2-layer image will be a 1-layer image instead)."
    ** https://docs.docker.com/engine/userguide/eng-image/baseimages/
        *** "You can use Docker’s reserved, minimal image, scratch, as a starting point for building containers. Using the scratch “image” signals to the build process that you want the next command in the Dockerfile to be the first filesystem layer in your image. +
        While scratch appears in Docker’s repository on the hub, you can’t pull it, run it, or tag any image with the name scratch. Instead, you can refer to it in your Dockerfile. For example, to create a minimal container using scratch:"
+
[source, docker]
----
FROM scratch
COPY hello /
CMD ["/hello"]
----

=== 2022 et + : Autres éléments à regarder

* 2023/03/17 : anniversaire, les 10 ans de Docker

* A brief history of minimal, secure container images : https://twitter.com/tracymiranda/status/1636778664871313414?s=20
    ** Regarder le Wolfi Community call du 2023/03/08 : https://www.youtube.com/watch?v=T6rOdF3ZeRs
        *** Wolfi is a lightweight GNU software distribution designed around minimalism

== Plan du talk

* Petite phrase introductive : "Pour comprendre le présent, et pouvoir envisager sereinement le futur, il est bon de connaître / pas avoir oublié le passé / d'où l'on vient"
* ou "Pour comprendre où l'on est, pouvoir envisager sereinement où l'on va, il est bon de savoir d'où l'on vient"

Commencez par un petit disclaimer pour le public ? "ce que ce talk n'est pas ?" (à savoir une prez sur comment utiliser Docker, CRI-O ou Podman)

1. un rappel : qu'est-ce qu'un container ?

Julia EVANS en dit la chose suivante (https://jvns.ca/blog/2016/10/10/what-even-is-a-container/) : +
"The word “container” doesn’t mean anything super precise. Basically there are a few *new Linux kernel features* (“namespaces” and “cgroups”) that let you isolate processes from each other. When you use those features, you call it “containers”." +

On trouve également le rappel, la définition "toute bête" suivante : *A container is a group of processes*

-> D'où : a container is a "just" a group of processes that are isolated from the system (the host) by some means.

[start=2]
2. L'histoire de la containerisation d'hier à aujourd'hui : La chronologie

    * cgroups et namespaces : les premisses des containers
    * Puis Docker
        ** prévoir schéma de ce "Docker l'ancien"
    * Puis Docker 1.11 et le "split" avec containerd et runc
        ** nouveau schéma, Docker Engine, server avec containerd et runc
        ** checker où en était Kubernetes à date du split (2016/04). CRI avait-il déjà fait son apparition ?
    * Puis l'arrivée de Kubernetes qui a entraîné l'apparition de CRI
        ** nouveau schéma avec Docker ET Kubernetes, et leurs container runtimes
    * Et maintenant toutes les alternatives possibles à "Docker" (en fait, de nouveaux high et low container runtime)
        ** pour les alternatives, voir https://linoxide.com/docker-alternative-container-tools/
        ** Podman (grâce à Kubernetes) est devenu un incontournable

    On commence par une *Chronologie complète* du début des containers à nos jours, puis on en propose *une 2nd* avec (opinionated point of view) uniquement les plus grandes étapes que je compte détailler.

    A chaque début de nouvelle section, reprendre où nous en sommes dans la chronologie temporelle

    -> NON, la chronologie temporelle est trop grande, ce sera impossible à afficher

Les étapes majeures à présenter plus en détails : 

* *Les cgroups et les namespaces*

    ** pourquoi a-t-on fait ça ? Principalement pour des besoins d'isolation
        *** retrouver les 1ers usages

* *L'arrivée de Docker, le début des containers*

    ** et au début, les containers, pour l'immense majorité des devs, c'était Docker et rien que Docker.
        *** Maintenant, il y a Docker la compagnie, et Docker la technologie
    ** Docker la compagnie ? Les images, les containers, la ligne de commande ?
    ** A la base la compagnie Docker a créé un outil simple et ergonomique pour travailler avec les containers, outil appelé "docker" (la CLI docker pour être plus précis)
        *** cette CLI permet très facilement to build images, pull them from registries, create, start and manager containers
    ** et la grosse différence se fait avec le passage à la version 1.11, et l'apparition de containerd et runc

    ** *Docker "à l'ancienne" avant la 1.11 (2016/04)*
        *** https://jvns.ca/blog/2016/10/02/i-just-want-to-run-a-container/

* *Le split de Docker (v1.11.0) : l'apparition de containerd et runc (high level et low level container runtime)*

    On a maintenant le Docker engine
    ** Docker client (CLI, GUI, etc.) 
    ** parle à un Docker Daemon 
    ** qui parle à containerd : un autre daemon qui va aller surveiller vos containers, les redémarrer
        *** containerd supervise les containers (start, stop, pause)
    ** qui parle à runc : une librairie, un espèce de wrapper qui va vous permettre de lancer plus facilement des processus isolés
        *** et c'est runc qui va lancer votre processus de façon isolé via les features de votre kernel (namespaces & co, etc.)
        *** runC can help you avoid being strongly tied to specific technologies, hardware, or cloud service providers.

    ** *containerd* et *runc* ont commencé à apparaître à partir de Docker *1.11.0* (2016/04) ?
        *** à confirmer via https://jvns.ca/blog/2016/10/02/i-just-want-to-run-a-container/ (site de 2016)
            **** OUI, confirmé via https://faun.pub/docker-containerd-standalone-runtimes-heres-what-you-should-know-b834ef155426 : +
            "Docker Engine 1.11 was the first release built on runC (a runtime based on Open Container Intiative technology) and containerd."
        *** Regarder avant tout le blog de Docker : https://www.docker.com/blog/docker-engine-1-11-runc/ (2016/04/13, sortie de Docker 1.11)
            **** "Over the last year (2015), Docker has helped advance the work of the OCI to make it more readily available to more users. It started in *December 2015*, when we *introduced containerd*, a daemon to control runC. This was part of our effort to *break out Docker into small reusable components*."
            **** *2017/03/15* Docker's donation of containerd to the CNCF
        *** voir https://containerd.io/ pour un bon schéma de *containerd*, montrant les low-level runtimes qui gravitent aujourd'hui autour (2021) : https://containerd.io/img/architecture.png

    ** détailler ici les low-level container runtimes, et les high-level container runtimes

* *l'arrivée de Kubernetes, et la démultiplication des runtimes*

    Kubernetes : fait naturellement tourner des containers dans des pods.

    * Donc l'ecosystem des containers est loin de se limiter au seul "Docker", c'est vraiment un *assemblage de diverses technos*, parmi lesquelles on peut citer : 
        ** pour builder des images OCI compliant : Kaniko (Google), buildah (RedHat), Makisu (Uber)
        ** pour lancer des containers depuis des images : CRI-o, rkt, containerd, Kata containers, gVisor, singularity, nabla, podman

    * Parler des confusions possible entre les différents "shim" : le deprecated docker-shim, et containerd-shim

== Chronologie complète

* *1970s* : Le concept d'isolation émerge du côté des systèmes Unix. +

    "The *original idea* of a container has been around since the 1970s, when the concept was first employed on *Unix systems* to *better isolate application code*. While useful in certain application development and deployment scenarios, the *biggest drawback* to containers in those early days was the simple fact that they were *anything but portable*." +
    "Back in the 1970s, *early containers created an isolated environment where services and applications could run without interfering with other processes* – producing something akin to a *sandbox to test applications*, services, and other processes. The original idea was to *isolate the container's workload from production systems* in way that *enabled developers to test their applications and processes on production hardware without risking disruption to other services*."

    "During the development of Unix version 7 in 1979, the *chroot* system call was introduced, changing the root directory of a process and its children to a new location in the filesystem." +
        "This advance was *the beginning process isolation*: segregating file access for each process. Chroot was added to BSD in 1982."

    ** Voir également cet EXCELLENT article sur les débuts d'Unix (Unics à l'époque, pour "Uniplexed Information and Computing Service") : +
    https://www.spiria.com/fr/blogue/breves-technos/unix-a-50-ans/
        ** L'article inclut la fameuse photo de Ken Thompson et Dennis Ritchie à côté d'un PDP-11 chez Bell Labs (vers 1972).
        Pour rappel, ce sont les créateurs respectifs d'Unix et du langage C, et Ken Thompson est également le créateur du premier shell Unix en 1971, sur la 1ere version d'Unix.
        ** "Dans les années 60, les Laboratoires Bell participaient à un projet avec le MIT et General Electric ayant pour objectif de mettre au point un système de temps partagé. Les ordinateurs de l’époque étant *très coûteux*, il s’agissait *de partager les ressources entre différents utilisateurs*. 
        ** C'est dans ce contexte que Ken Thompson, assisté de Dennis Ritchie ont créé le système d'exploitation Unix (à l'époque appelé Unics "Uniplexed Information and Computing Service")

* *1979* : GRANDE ETAPE - *chroot* +

    ** "During the development of Unix version 7 in 1979, the *chroot* system call was introduced, changing the root directory of a process and its children to a new location in the filesystem."
    ** "This advance was *the beginning process isolation*: segregating file access for each process. Chroot was added to BSD in 1982."
    ** développement de chroot, dans la version 7 d'Unix
    ** "Chroot marked the beginning of container-style process isolation by restricting an application's file access to a specific directory -- the root -- and its children. A key benefit of chroot separation was improved system security, such that an isolated environment could not compromise external systems if an internal vulnerability was exploited."

        *** system call - http://www.di.uevora.pt/~lmr/syscalls.html : +
        A system call is just what its name implies -- a request for the operating system to do something on behalf of the user's program. The system calls are functions used in the kernel itself.

    ** Pour des exemples de chroot "breakouts", voir https://securityqueens.co.uk/im-in-chroot-jail-get-me-out-of-here/ +
    L'article contient également un bon schéma illustrant le résultat d'un chroot

    ** Pour des bien expliqués de l'usage de chroot, voir https://www.ibm.com/docs/en/zos/2.1.0?topic=descriptions-chroot-change-root-directory-execution-command
        *** To run a child shell with another file system as the root file system (assuming that /tmp is the mount point of a file system), enter: +
            mkdir /tmp/bin
            cp /bin/sh /tmp/bin
            chroot /tmp /bin/sh
        *** This makes the directory name / (slash) refer to the /tmp for the duration of the /bin/sh command. *It also makes the original root file system inaccessible*. The file system on the /tmp file must contain the standard directories of a root file system. +
        Running the sh command creates a child shell that runs as a separate process from your original shell.

    ** Rappel côté sécurité : chroot is easy to escape from if you're root and pivot_root isn't +
    -> so containers use pivot_root instead of chroot

* *2000/03* : *FreeBSD Jails* +

    ** "jails", an early implementation of container technology, was added to FreeBSD
    ** At that time, "a small shared-environment hosting provider came up with FreeBSD jails to achieve *clear-cut separation between its services and those of its customers* for *security* and *ease of administration*. FreeBSD Jails allows administrators to partition a FreeBSD computer system into several independent, smaller systems – called “jails” – with the ability to assign an IP address for each system and configuration."
    ** https://en.wikipedia.org/wiki/FreeBSD_jail : "Jails were first introduced in FreeBSD version 4.0, that was released on *March 14, 2000*"

    ** Pour un bon schéma illustrant l'usage de Jails, voir https://www.admin-magazine.com/Archive/2013/13/How-to-configure-and-use-jailed-processes-in-FreeBSD/(offset)/6

* *2001* : *Linux VServer* +

    ** Container technology made it to the Linux side of the house +
    "Jacques Gélinas created the VServer project, which according to the 0.0 version’s change log allowed “running several general purpose Linux server on a single box with a high degree of Independence and security.”"
    ** "Like FreeBSD Jails, Linux VServer is a jail mechanism that can partition resources (file systems, network addresses, memory) on a computer system. Introduced in 2001, this operating system virtualization that is implemented by *patching the Linux kernel*. Experimental patches are still available, but the last stable patch was released in 2006."

    ** Linux-VServer is a virtual private server implementation that was created by adding operating system-level virtualization capabilities to the Linux kernel. +
    https://en.wikipedia.org/wiki/Linux-VServer

    ** http://linux-vserver.org/Overview : The Linux-VServer technology is a *soft partitioning concept* based on Security Contexts which permits the creation of many independent Virtual Private Servers (VPS) that run simultaneously on a single physical server at full speed, efficiently sharing hardware resources.

    ** Attention, à ne pas confondre avec Linux Virtual Server (LVS) qui est un outil de load balancing.
        *** https://en.wikipedia.org/wiki/Linux_Virtual_Server

    ** Contrairement à d'autres solutions de conteneurisation de l'époque, Linux VServer a introduit une approche de "virtualisation légère" qui permettait de partager le noyau de l'OS entre les différents conteneurs (les "VPS" de VServer), plutôt que de créer un noyau séparé pour chaque conteneur. Cela permettait une utilisation plus efficace des ressources système, tout en garantissant une bonne isolation des processus.

    ** The Linux-VServer solution was the *first effort on Linux* to "separate the user-space environment into distinct units (Virtual Private Servers) in such a way that each VPS looks and feels like a real server to the processes contained within." +
    Linux-VServer is a virtual private server implementation that was created by *adding operating system-level virtualization capabilities to the Linux kernel*. +
    Ce projet permet d'exécuter un ou plusieurs environnements d'exploitation (systèmes d'exploitation "*sans le noyau*") ; autrement dit, il *permet d'exécuter une ou plusieurs distributions sur une distribution*.

    ** https://www.malekal.com/quest-ce-que-le-noyau-linux-kernel-role-versions-et-comment-ca-marche/ +
    "Les distributions Linux sont un ensemble de logiciels libres autour du noyau Linux"

    ** Ce projet permet d'*exécuter une ou plusieurs distributions sur une distribution* (distribution = système d'exploitation sans le noyau)

Patcher le kernel Linux : une *charge en plus* pour les distributeurs et les sysadmin.

Last stable patch was released in 2006

* *2002/08* : Les 1er *Linux namespaces* (mount namespaces) sont ajoutés au kernel Linux 2.4.19 (2002/08/03)

    ** *namespaces* : allow processes to have their own network / PIDs / users / hostname / mounts / and more !

    ** la vidéo https://www.youtube.com/watch?v=sK5i-N34im8[cgroups, namespaces, and beyond: what are containers made from?] de Jérôme PETAZZONI (Docker) explique en détails les différentes fonctionnalités des *cgroups*, *différents types de namespaces*. +
        ATTENTION ! Elle date de 2015 !
            **** Il est également question des *container runtimes* qui sont basés sur les cgroups et les namespaces. +
            Exemples de container runtimes basés sur des namespaces et des cgroups : 
                ***** *LXC* (Linux Containers) : easy for sysadmins / OPS, hard for devs (requires significant elbow grease)
                ***** *systemd-nspawn*
                ***** *Docker*
                ***** *rkt*
                ***** *runC*
                ***** All those container runtimes use the same kernel features (at that time, 2015 ?)
            **** et maintenant des container runtimes qui ne sont PAS basés sur les namespaces et les cgroups : 
                **** *OpenVZ* : by example Travis CI gives you root in OpenVZ
                **** *Jails* / *Zones*

    ** Bon un bon schéma des différents Linux namespaces, voir https://8gwifi.org/docs/linux-namespace.jsp

    ** *namespaces* are a Linux kernel feature allowing your processes to be separated from the other processes on the computer. +
    You can have PID namespace, networking namespace, mount namespace. +
    Namespaces can be created using the `unshare` program.

    ** *namespaces* limit what you can see : https://youtu.be/sK5i-N34im8?t=1519[Jérôme Petazzoni à la DockerCon 2015]

    ** Pour les *dates* de création des *cgroups* et *namespaces*, voir cet article : https://www.silicon.co.uk/software/open-source/linux-kernel-cgroups-namespaces-containers-186240

    ** *namespaces* were originally developed by *Eric Biederman*, and the final major namespace was merged into *Linux 3.8*. +
        Cf Wikipedia (https://en.wikipedia.org/wiki/Linux_namespaces) : 
        "The Linux Namespaces originated in *2002 in the 2.4.19 kernel* (2002/08/03) with work on the *mount namespace* kind. Additional namespaces were added beginning in 2006[2] and continuing into the future. +
        Adequate containers support functionality was finished in kernel *version 3.8* with the *introduction of User namespaces*."
            **** Et l'info très intéressante est ici : ce sont les user namespaces, introduit avec le kernel 3.8 de Linux qui ont changé la donne, et dont Solomon Hykes dit en 2013 (voir la conf ci-dessous, à 16:19) que, ça y est, "les namespaces marchent maintenant".
            **** https://kernelnewbies.org/Linux_3.8 : "*Linux 3.8* was released on Mon, *18 Feb 2013*."

    ** Description des *mount namespaces* par Jérôme Petazzoni de Docker durant la DockerCon 2015 : https://youtu.be/sK5i-N34im8?t=1666

    ** *Namespaces* let you virtualize system resources, like the file system or networking for each container.
        *** Namespaces are *"what you can see"*

    ** At their core, low-level container runtimes are responsible for setting up these namespaces and cgroups for containers, and then running commands inside those namespaces and cgroups.

    ** fin 2007 : ajout des 1eres briques de l'implémentation des user namespaces dans le kernel Linux 2.6.23 par Eric Biederman (Red Hat) +
        "Red Hatter Eric W. Biederman’s 2008 user namespaces patches being arguably the most complex and one of the most important namespaces in the context of containers. The implementation of user namespaces allows a process to have it’s own set of users and in particular to *allows a process root privileges inside a container, but not outside*."

    ** "Nevertheless, this kind of container technology (speaking of Borg) could only go so far. This led to the development of process containers, which became control groups (cgroups) as early as 2004. Cgroups noted the relationships between processes and reined in user access to specific activities and memory volumes. The cgroups concept was absorbed into the Linux kernel in January 2008, after which the Linux container technology LXC emerged. *Namespaces developed shortly thereafter to provide the basis for container network security* -- to hide a user's or group's activity from others."

    ** vidéos sympas détaillant les débuts de l'histoire des  containers (jusqu'à Docker), et résumant bien l'usage des namespaces et cgroups : https://www.youtube.com/watch?v=9Egk9Tnc28E&list=PL5JFPVMx5WzXB-NlH13_G8R8dgfz564uo&index=2

* *2003* : Google introduced *Borg*, the organization's container cluster management system. +

    ** "It relied on the *isolation mechanisms that Linux already had in place*. In those early days in the evolution of containers, *security wasn't much of a concern*. Anyone could see what was going on inside the machine, which enabled a system of accounting for who was using the most memory and how to make the system perform better."
    ** *Borg* is Google's cluster manager that runs hundreds of thousands of jobs, from many thousands of different applications, across a number of clusters each with up to tens of thousands of machines. +
    See https://research.google/pubs/pub43438/ for more details

    ** Borg est bien toujours utilisé aujourd'hui (2022) dans les datacenters de Google, et PAS Kubernetes, qui est utilisé pour des services Cloud et est surtout destiné à des utilisateurs externes.
        *** https://www.theregister.com/2020/08/21/microsoft_is_not_the_enemy_kubernetes_founder/ : +
        "although Kubernetes was in some ways an evolution of Borg, a key difference was that *Kubernetes* was always intended for *external developers*."
        *** Will Google migrate from Borg to Kubernetes? "We run some cloud services on k8s already, but Borg has 14+ years of features that are custom built for search, ads, Gmail, and K8s DOES NOT want those custom features," Brendan Burns said (Brendan Burns : Google en 2013, mais aujourd'hui chez Microsoft et responsable de Kubernetes on Azure)
        *** https://wiki.sfeir.com/kubernetes/borg/ : rappel SFEIR est très proche de Google, c'est une source sûre
            **** "Borg est l’OS des datacenters de Google. Il est utilisé pour gérer les workload et déterminer quelle machine recevra quel traitement. C’est donc Borg qui au quotidien détermine quelles sont les machines qui vont exécuter les traitements permettant à Gmail, GSuite, YouTube de fonctionner."

* *2004* : *Solaris Containers* +

    * "In 2004, the first public beta of Solaris Containers was released that combines system resource controls and boundary separation provided by zones, which were able to leverage features like snapshots and cloning from ZFS."
    * Solaris Containers (including Solaris Zones) is an implementation of operating system-level virtualization technology for x86 and SPARC systems, first released publicly in February 2004 in build 51 beta of Solaris 10, and subsequently in the first full release of Solaris 10, 2005.

    ** https://en.wikipedia.org/wiki/Solaris_Containers : +
    A Solaris Container is the combination of system resource controls and the boundary separation provided by zones. Zones act as completely isolated virtual servers within a single operating system instance.

    ** *ZFS* (https://fr.wikipedia.org/wiki/ZFS) : +
    Les caractéristiques de ce système de fichiers sont sa *très haute capacité de stockage*, l'intégration de beaucoup de concepts que l'on trouve sur d'autres systèmes de fichiers, et la *gestion de volume*. Il utilise pour cela des structures de données comme les B-tree "On-Disk", et un adressage des secteurs disque logique au lieu d'un adressage physique. +
    Produit par Sun Microsystems (société rachetée par Oracle en 2009) pour *Solaris 10* et au-delà, il a été conçu par l'équipe de Jeff Bonwick (en). Annoncé pour septembre 2004, il a été intégré à Solaris le 31 octobre 2005 et le 16 novembre 2005 en tant que caractéristique du build 27 d'OpenSolaris. Sun a annoncé que ZFS était intégré dans la mise à jour de Solaris datée de juin 2006, soit un an après l'ouverture de la communauté OpenSolaris.

    ** Rappel : en 2017, Oracle, propriétaire de Solaris suite au rachat de Sun, a annoncé qu'*il n'y aurait pas de Solaris 2012*. +
    Le support étendu de Solaris 11 est toujours prévu pour 2034, mais les licenciements côté Solaris sont nombreux, soyons clairs, c'est la fin pour Solaris...
        *** https://www.zdnet.fr/actualites/suppressions-d-emploi-chez-oracle-solaris-va-s-eteindre-39856916.htm

* *2005* : *Open VZ* +

    "This is an operating system-level virtualization technology for Linux which uses a patched Linux kernel for virtualization, isolation, resource management and checkpointing. The code was not released as part of the official Linux kernel."

    ** voir https://fr.wikipedia.org/wiki/OpenVZ +
    OpenVZ permet à un serveur physique d'exécuter de multiples instances de systèmes d'exploitation isolés, qualifiées de serveurs privés virtuels (VPS) ou environnements virtuels (VE).

    ** pour un bon schéma de l'architecture d'OpenVZ, voir http://www.virtualizationsoftwares.com/openvz-open-virtualization/
        *** Lien mort ! Lui préférer https://www.markus-gattol.name/ws/openvz.html, section "Hardware Node"

    ** OpenVZ et Linux-VServer sont souvent comparés (regarder le schéma d'architecture, on retrouve les Virtual Private Servers de Linux-VServer)
        ** tous les 2 utilisent une technologie de virtualisation de niveau système d’exploitation pour Linux, basé sur une version modifiée de son noyau.
            *** OSLV : Operating System Level Virtualization

    ** Bonne définition d'OpenVZ : https://sitevalley.com/blog/xen-and-openvz-technology-insight-and-comparison/, section "OpenVZ"
        *** OpenVZ, itself, is built on top of Linux.
        *** OpenVZ uses *Operating System Level Virtualization* (OSLV). +
        With OSLV virtualization *the operating environment (OS) is virtualized instead of the hardware*. Thus, while there is only one operating system kernel, multiple programs run in isolation from each other within the single OS instance.

    ** Pour une bonne explication du principe de base d'OpenVZ, voir https://cesar.resinfo.org/IMG/pdf/jtsiars-openvz_1_.pdf
        *** surtout slide 3 "principe de base" : +
            **** OpenVZ (comme Vserver) = Virtualisation au niveau Kernel : 
            **** Il s'agit d'un partitionnement logique au niveau des ressourcessystèmes : processus, réseau et « file system »
            **** C'est le noyau du système d'exploitation qui fait une isolation entre des machines logiques, tout comme il isole déjà les processus entre eux.
                ***** isolation des « processus » et
                ***** mise en cage (chroot) des différents « file system » des machines virtuelles.
            **** Dans ce cas, il n'y a pas d'« émulation » à proprement parler comment dans d'autres système de virtualisation. 

    ** "This is an operating system-level virtualization technology for Linux which uses a patched Linux kernel for virtualization, isolation, resource management and checkpointing. The code was not released as part of the official Linux kernel."
        

* *2006* : début des travaux sur les *Process Containers* chez Google (later renamed *cgroups* / Control Groups)

        ** Début des travaux sur les cgroups par Paul Menage and Rohit Seth chez Google

        ** Paul Menage (Google) travaille sur les "process containers", plus tard renommé en cgroups (control groups) +
        "Cgroups allow processes to be grouped together, and ensure that each group gets a share of memory, CPU and disk I/O; preventing any one container from monopolizing any of these resources"

        ** "Process Containers (launched by Google in 2006) was designed for limiting, accounting and isolating resource usage (CPU, memory, disk I/O, network) of a collection of processes. It was renamed “Control Groups (cgroups)” a year later and eventually merged to Linux kernel 2.6.24."

        ** "Nevertheless, this kind of container technology (speaking of Borg) could only go so far. This led to the development of process containers, which became control groups (cgroups) as early as 2004. Cgroups noted the relationships between processes and reined in user access to specific activities and memory volumes. *The cgroups concept was absorbed into the Linux kernel in January 2008*, after which the Linux container technology LXC emerged. Namespaces developed shortly thereafter to provide the basis for container network security -- to hide a user's or group's activity from others."

        ** Cf wikipedia (https://en.wikipedia.org/wiki/Cgroups), *cgroups* : +
        "cgroups (abbreviated from control groups) is a *Linux kernel feature* that limits, accounts for, and isolates the resource usage (CPU, memory, disk I/O, network, etc.) of a collection of processes."
        ** la vidéo https://www.youtube.com/watch?v=sK5i-N34im8[cgroups, namespaces, and beyond: what are containers made from?] de Jérôme PETAZZONI (Docker) explique en détails les différentes fonctionnalités des *cgroups*, *différents types de namespaces*. +
        ATTENTION ! Elle date de 2015 !
            *** Il est également question des *container runtimes* qui sont basés sur les cgroups et les namespaces. +
            Exemples de container runtimes basés sur des namespaces et des cgroups : 
                **** *LXC* (Linux Containers) : easy for sysadmins / OPS, hard for devs (requires significant elbow grease)
                **** *systemd-nspawn*
                **** *Docker*
                **** *rkt*
                **** *runC*
                **** All those container runtimes use the same kernel features (at that time, 2015 ?)
            *** et maintenant des container runtimes qui ne sont PAS basés sur les namespaces et les cgroups : 
                *** *OpenVZ* : by example Travis CI gives you root in OpenVZ
                *** *Jails* / *Zones*

        ** le travail sur les *cgroups* a commencé en 2006 chez Google sous le nom "process containers", avant d'être renommé en "control groups" pour éviter toute confusion avec le terme "container" dans un contexte Linux Kernel.
            **** cf Wikipedia (https://en.wikipedia.org/wiki/Cgroups) : +
            "A control group (abbreviated as cgroup) is a *collection of processes that are bound by the same criteria* and associated with a set of parameters or limits. These groups can be *hierarchical*, meaning that *each group inherits limits from its parent group*. The kernel provides access to multiple controllers (also called subsystems) through the cgroup interface;[2] for example, the "memory" controller limits memory use, "cpuacct" accounts CPU usage, etc."

        ** Pour les *dates* de création des *cgroups* et *namespaces*, voir cet article : https://www.silicon.co.uk/software/open-source/linux-kernel-cgroups-namespaces-containers-186240

            *** *cgroups* were originally developed by Paul Menage and Rohit Seth of Google, and their first features were merged into *Linux 2.6.24* (*2008/01*) +
            Cf Wikipedia (https://en.wikipedia.org/wiki/Cgroups) : 
            "Engineers at Google (primarily *Paul Menage* and *Rohit Seth*) *started the work on this feature in 2006* under the name "*process containers*".[1] In late 2007, the nomenclature changed to "control groups" to avoid confusion caused by multiple meanings of the term "container" in the Linux kernel context, and the control groups functionality was merged into the Linux kernel mainline in *kernel version 2.6.24*, which was *released in January 2008*."

        ** *cgroups* : limit memory / CPU usage for a group of processes

        ** *cgroups* provide a way to limit the amount of resources, such as CPU and memory, that each container can use.
            *** control groups are "what you can use"
        ** At their core, low-level container runtimes are responsible for setting up these namespaces and cgroups for containers, and then running commands inside those namespaces and cgroups.

        ** vidéos sympas détaillant les débuts de l'histoire des  containers (jusqu'à Docker), et résumant bien l'usage des namespaces et cgroups : https://www.youtube.com/watch?v=9Egk9Tnc28E&list=PL5JFPVMx5WzXB-NlH13_G8R8dgfz564uo&index=2

        ** *cgroups* limit how much you can use : https://youtu.be/sK5i-N34im8?t=1519[Jérôme Petazzoni à la DockerCon 2015]

        ** schéma sur les cgroups : https://www.lightnetics.com/topic/17326/what-are-control-groups-in-linux

* *2007/10* : ajout des 1eres briques de l'implémentation des *user namespaces* dans le kernel Linux 2.6.23 par Eric Biederman (Red Hat) +

    "Red Hatter Eric W. Biederman’s 2008 user namespaces patches being arguably the most complex and one of the most important namespaces in the context of containers. The implementation of user namespaces allows a process to have it’s own set of users and in particular to *allows a process root privileges inside a container, but not outside*."
        ** le kernel Linux 2.6.23 est releasé le 2007/10/10 (https://lwn.net/Articles/253813/)

    ** Les meilleurs explications et schémas que j'ai trouvés sur les user namespaces : 
        *** https://blog.quarkslab.com/digging-into-linux-namespaces-part-2.html
        *** julia-evans_how-containers-work_14_user-namespaces.jpg

* *2008/01* : GRANDE ETAPE - ajout de la fonctionnalité des *cgroups* dans le kernel Linux 2.6.24

* *2008/08* : GRANDE ETAPE - création du projet *Linux Containers (LXC)* par des ingénieurs d'IBM. +

    "It layered some userspace tooling on top of cgroups and namespaces"
        ** https://fr.wikipedia.org/wiki/LXC : initial release 2008/08/06
        ** "LXC (LinuX Containers) was the first, most complete implementation of Linux container manager. It was implemented in 2008 using cgroups and Linux namespaces, and it works on a single Linux kernel *without requiring any patches*."

* *2011* : Warden +

    "CloudFoundry started Warden in 2011, *using LXC in the early stages* and later replacing it with its own implementation. Warden can isolate environments on any operating system, *running as a daemon* and *providing an API for container management*. It developed a *client-server model* to manage a collection of containers across multiple hosts, and Warden includes a service to manage *cgroups*, *namespaces* and the *process life cycle*."

    ** Voir également https://github.com/cloudfoundry-attic/warden
    The project's primary goal is to provide a simple API for managing isolated environments. These isolated environments -- or containers -- can be limited in terms of CPU usage, memory usage, disk usage, and network access. As of writing, the only supported OS is Linux.

    ** Autre très bonne explication sur Warden : http://underlap.blogspot.com/2014/06/warden-meets-libcontainer.html
    ** Voir également https://tanzu.vmware.com/content/blog/cloud-foundrys-container-technology-a-garden-overview (très bonnes explications)
        *** Warden creates a root process, called "wshd" for Warden shell *daemon*, in each container.
        *** warden shell daemon (wshd) which is responsible for managing applications and performing other functions inside the container.
        *** (The Warden protocol is defined using Google protocol buffer definitions)

    ** CloundFoundry a plus tard remplacé les Containers Linux par leur propre implémentation, utilisant toujours les cgroups et les namespaces
    
    ** Warden avait avait pour but de fournir moyen simple de gérer des environnements isolés (des conteneurs 😉)

* *2013/02* : GRANDE ETAPE - ajout des *user namespaces* au kernel Linux 3.8

    ** A ce moment, on a les cgroups et les namespaces (que Solomon HYKES présente comme si importants), mais il y a également d'autres Linux Kernel features utilisés par les containers que l'on va présenter rapidement.

    ** Solomon Hykes l’a affirmé lors de son tout premier talk de présentation Docker, c’est la sortie de ces user namespaces "qui marchent maintenant", qui a rendu Docker possible.
        *** "Why we built Docker ?" https://www.youtube.com/watch?v=3N3n9FzebAA dotScale 2013, le 2013/06/07, juste après la 1ere publication de Docker

* *2013/03/20* : ETAPE MAJEURE - *1ere release de Docker*

    ** "When Docker emerged in 2013, containers exploded in popularity. It’s no coincidence the growth of Docker and container use goes hand-in-hand." +
    "Just *as Warden did*, Docker also used LXC in its initial stages and later replaced that container manager with its own library, libcontainer. But there’s no doubt that Docker separated itself from the pack by offering an entire ecosystem for container management."

    ** Docker floated onto the scene in 2013 with an easy-to-use GUI, and the ability to package, provision and run container technology. Because Docker enabled multiple applications with different OS requirements to run on the same OS kernel in containers, IT admins and organizations saw opportunity for simplification and resource savings. +
    *Unlike VMs*, containers have a significantly smaller resource footprint, are faster to spin up and down, and require less overhead to manage. VMs must also each encapsulate a fully independent OS and other resources, while *containers share the same OS kernel* and use a proxy system to connect to the resources they need, depending upon where those resources are located.

    ** "Docker was introduced in 2013 by an San Francisco company that offers PaaS cloud services named dotCloud as an open-source project, and its founder is Solomon Hykes. When it first came out, *it aimed to convert monolitich applications into image and container structure by using LXC* (Linux containers). Later on, it started to develop his own container runtime, *libcontainer*, and after this stage, libcontainer was started to be used."

    ** *Hésitations et craintes côté sécurité :* 

        *** (https://searchitoperations.techtarget.com/feature/Dive-into-the-decades-long-history-of-container-technology) +
        *Concern and hesitation* arose in the IT community regarding the *security of a shared OS kernel*. A vulnerable container could result in a vulnerable ecosystem without the right precautions baked into the container technology. Additional complaints early in the modern evolution of containers bemoaned the lack of data persistence, which is important to the vast majority of enterprise applications. Efficient networking also posed problems, as well as the logistics of regulatory compliance and distributed application management.

        *** Documentation de Docker qui mentionne que le *daemon docker tourne toujours en root* : https://docs.docker.com/engine/install/linux-postinstall/
            **** D'où des problématiques de *Privilege Escalation* (élévation de privilèges)
            **** un mode pour faire run Docker without root privileges existe, mais ce n'est pas parfait, et c'est arrivé plus tard : +
            https://docs.docker.com/engine/install/linux-postinstall/
            **** "The Docker daemon always runs as the root user. +
            If you don’t want to preface the docker command with sudo, *create a Unix group called docker and add users to it*. When the Docker daemon starts, it creates a Unix socket accessible by members of the docker group."
        **** tout le problème vient de groupe "docker", voir https://github.com/chrisfosterelli/dockerrootplease et https://fosterelli.co/privilege-escalation-via-docker

        *** Pour une bonne présentation du pourquoi de podman, à savoir les problèmes de sécurité de Docker et l'hégémonie de Kubernetes, et une demo de son utilisation, voir https://www.redhat.com/en/blog/say-hello-buildah-podman-and-skopeo (2019/10) +
        "This excites some people who always saw the *monolith daemon that required root access for everything as a problem*.
            **** -> Ce qui *a conduit à l'émergence des daemonless et autre rootless comme Podman* et Buildah

            **** Originally built as a monolithic daemon, *dockerd*, and the *docker client (Docker CLI)* application. +
            The daemon provided most of the logic of building containers, managing the images, and running containers, along with an API.
                ***** et ce daemon avait besoin d'être root pour tout

        *** https://www.techtarget.com/searchitoperations/feature/Dive-into-the-decades-long-history-of-container-technology +
        "Sysdig's "2022 Cloud-Native Security and Usage Report" found that 75% of respondents were running containers with high or critical vulnerabilities, and *76% had containers running as root*, playing a dangerous game with IT ecosystem safety."
            **** une partie des résultats de l'étude est disponible ici : https://sysdig.com/blog/2022-cloud-native-security-usage-report/
            **** chercher Container Security le chiffre de 76% précédent qui nous intéresse

* *2013/10* : LMCTFY +

    "Let Me Contain That For You (LMCTFY) kicked off in 2013 as an open-source version of Google's container stack (based on Borg internals), providing Linux application containers. Applications can be made “container aware,” creating and managing their own subcontainers. Active deployment in LMCTFY stopped in 015 after Google started contributing core LMCTFY concepts to libcontainer, which is now part of the Open Container Foundation."
    ** initial release 2013/10/13, et final release (0.4.5) 2014/03/28

* *2014/02/20* : release de la 1ere version 1.0 de LXC

* *2014/06/07* : GRANDE ETAPE - toute première release de *Kubernetes* par Google (1er commit GitHub), qui le présente comme une version open source de Borg (Google’s *internal* container cluster-management system)

    ** Kubernetes en peu de mots : un gestionnaire de cluster de conteneurs open source
    ** pour cette date du 06/06, voir https://techcrunch.com/2018/06/06/four-years-after-release-of-kubernetes-1-0-it-has-come-long-way/
        *** Pour plus de détails sur l'histoire de Kubernetes, voir https://blog.risingstack.com/the-history-of-kubernetes/

* *2014/11* : 1ere release de rkt (prononcer "rocket"), le container runtime créé par les équipes de CoreOS.

    ** https://blog.wescale.fr/2017/01/23/introduction-a-rkt/
    ** Rkt is a secure and lightweight Docker alternative container system developed by CoreOS. It is built on a container standard known as *App Container* or *appc*. For this reason, rkt images can be run on container systems that support the “appc” format. +
    "Unlike Docker, rkt runs containers with un-privileged users (unlike priority… Unlike Docker…). Thus, even if there is a kernel level deficit and the user can get out of the container, this does not affect other containers and users."
    ** rkt venait répondre à certaines des *problèmatiques de sécurité* existant avec Docker : +
    "As it is known, containers are process groups that can be created by granting some rights to users on the system or by processing with root. In addition, the operation of a user in one container is not seen by the other container. Users are safe in this way as long as there is no abuse on the Linux kernel. However, in some systems such as Docker, *malicious users who can get out of the container through an abuse on the kernel can ruin everything*. Such a risk exists despite measures."

* *2015/06* : GRANDE ETAPE - *Docker Inc donne la codebase du projet Docker à l'OCI*, projet de la Linux Fondation, *créé pour cette occasion*. +

    "In June 2015, Docker the company, the largest contributor to Docker the project (Red Hat is the second), donated the project’s existing codebase to the *Open Container Initiative*, a lightweight governance structure under the auspices of the Linux Foundation created to *prevent fragmentation* and promote open standards by “cloud giants” including Red Hat."
        ** ce "*prevent fragmentation*" est très probablement la principal raison du "split" de Docker opéré par Docker Inc

* *2015/07* : GRANDE ETAPE - *runc* est publié pour la 1ere fois, et son code a *tout de suite donné par Docker à l'OCI* (runc est l'implémentation de référence de la runtime-spec)
    ** runc est libcontainer qui a été "sorti" de Docker pour le mettre à disposition de la communauté au travers de l'OCI

* *2015/07/21* : ETAPE MAJEURE - *release de la 1ere version de Kubernetes* par Google, et *création de la CNCF*, comme umbrella projet de la Linux Foundation, à laquelle Kubernetes sera donné comme élément fondateur. +

    Google versera / contribuera cette v1.0 de Kubernetes à la CNCF en tant que tout 1er projet et élément fondateur. +
    Pour rappel, la CNCF se définit comme "a Linux Foundation project that was founded in 2015 to help advance container technology and align the tech industry around its evolution" (voir https://en.wikipedia.org/wiki/Cloud_Native_Computing_Foundation et https://fr.wikipedia.org/wiki/Cloud_Native_Computing_Foundation)

-> DONC EN 2015/06 et 2015/07, on a la création à la fois de la CNCF et de l'OCI

* *2015/12* : GRANDE ETAPE - Docker introduce *containerd*

* *2016/03/14* : 1ere apparition des cgroups v2 dans le Linux Kernel 4.5

    ** plus tard utilisé par runc et crun
        *** *runc* : runc fully supports cgroup v2 (unified mode) since v1.0.0-rc93, 2021/02/04 (https://github.com/opencontainers/runc)
        *** *crun* : dans la spécification d'origine en 2019/03, et "add support for OCI unified cgroups v2 in v0.15, 2020/09/23" (https://github.com/containers/crun/releases?q=cgroup+v2&expanded=true)

* *2016/04/13* : ETAPE MAJEURE - sortie de la *version 1.11 de Docker* : 1ere release de Docker basée sur containerd et runC (OCI)
    ** cette release marque le passage à l'architecture "moderne" de Docker

-> A partir de 2017, Kubernetes a de plus en plus le vent en poupe

* *2017/03/15* : GRANDE ETAPE - *Docker's donation of containerd project to the CNCF* AND *CoreOS's donation of rkt to the CNCF* 

    ** Cette donation a eu le *2017/03/15*, voir l'annonce de Solomon Hykes https://www.docker.com/blog/docker-donates-containerd-to-cncf/ +
    Cet article explique également que containerd a été créé en 2016/12 : +
    "Back in December 2016, Docker spun out its core container runtime functionality into a standalone component, incorporating it into a separate project called *containerd*, [...]"
    ** Les 2 annonces ont eu lieu le même jour, durant le même meeting du CNCF TOC (Technical Oversight Committee, où Docker et CoreOS étaient déjà représentés) +
    Voir https://www.cncf.io/blog/2017/03/29/rkt-pod-native-container-engine-launches-cncf/ : +
    "On March 15, 2017, at the CNCF TOC meeting, CoreOS and Docker made proposals to add rkt and containerd as new projects for inclusion in the CNCF. During the meeting, we as rkt co-founders, proposed rkt, and Michael Crosby, a containerd project lead and co-founder, proposed containerd."

* *2017/04* : Microsoft enabled organizations to run Linux containers on Windows Server. This was a major development for Microsoft shops that wanted to containerize applications and stay compatible with their existing systems.

* *2017/04/12* - apparition des "Distroless" container images chez Google
    ** repo Google : https://github.com/GoogleContainerTools/distroless
    ** 1er commit : https://github.com/GoogleContainerTools/distroless/commit/02de2e7f738c27d8811003167355d357a146997a
    ** (almost) Distroless container images are designed to provide a minimalistic and secure environment for running containerized applications, by excluding distribution-specific components that are often unnecessary and can introduce security risks or compatibility issues. They contain only the essential libraries and dependencies required to run the application, resulting in smaller images and improved performance. This approach also simplifies deployment and ensures compatibility across different systems and environments.
    ** To be precise, distroless container images do include a Linux operating system, albeit a very stripped-down one.

* *2017/07/19* : Release of the OCI v1.0 runtime and image specifications

* *2017/10* : DockerCon 2017, Docker announced they will support the Kubernetes container orchestrator, and Azure and AWS fell in line, with AKS (Azure Kubernetes Service) and Amazon EKS (Amazon Elastic Kubernetes Service)

(* 2018 beginnning : CoreOS was acquired by Red Hat at the beginning of 2018)

-> 2018, l'avènement de Kubernetes, tous les plus grands Cloud providers en proposant une offre packagé, ET la démultiplication des container runtimes de types différents (sandbox runtimes, daemonless runtimes)

-> *2018* : ETAPE MAJEURE - *L'avènement de Kubernetes*, où tous les Cloud providers propose leur offre de Kubernetes managé +
"The massive adoption of Kubernetes pushed cloud vendors such as AWS, Google with GKE (Google Kubernetes Engine), Azure, and Oracle with Container Engine for Kubernetes, to offer managed Kubernetes services. Furthermore, leading software vendors such as VMWare, RedHat, and Rancher started offering Kubernetes-based management platforms." +
L'usage croissant de Kubernetes a démultiplié l'usage des containers (1 seul cluster Kubernetes pouvant en faire tourner jusqu'à un maximum de 300 000), rendant la *sécurisation* de ces derniers d'autant plus importante. +
Ce besoin accrue de sécurisation a amené à l'apparition de nouveaux types de runtimes : les sandbox runtimes, ainsi que les daemonless / rootless runtimes 

* émergences des "*sandbox runtimes*" : *Kata containers*, *gVisor*, *Nabla* : +

    "We also witnessed emerging hybrid technologies that combine *VM-like isolation with container speed*. Open source projects such as Kata containers, gVisor, and Nabla attempt to provide *secured container runtimes* with lightweight virtual machines that perform the same way container do, but provide *stronger workload isolation*." +
    Voir cet article https://www.agaetis.fr/blogpost/les-runtimes-oci qui expliquent bien ce que sont les "*sandbox runtimes*" comme gVisor, Nabla containers et Kata containers : +
    "Les sandbox runtimes, des runtimes qui *isolent un peu plus les conteneurs de la machine hôte* en limitant les interactions entre le kernel et les conteneurs." +
    L'accent est donc mis sur la *SECURITE* : il faut combler les failles de sécurité des containers popularisés par Docker, c'est la raison d'être des sandbox runtimes. +
    "Les sandbox runtimes *limitent les interactions entre le conteneur et le kernel* pour *réduire au maximum la surface d’attaque*, permettant ainsi une plus grande isolation. Dans cette catégorie nous allons voir gVisor,  Nabla containers et Kata containers. Chacun utilisent une méthode différente pour y arriver". +
    Rappelons cette crainte que l'on avait du temps des débuts de Docker en 2013 : +
    "*Concern and hesitation* arose in the IT community regarding the *security of a shared OS kernel*" (https://searchitoperations.techtarget.com/feature/Dive-into-the-decades-long-history-of-container-technology)
        *** *gVisor* implémente son propre kernel, *Sentry*, et son composant pour les interactions avec le système de fichiers, *Gofer*
        *** *Nabla containers* utilise la technique de *l’unikernel* qui consiste à packager l’application avec une bibliothèque d’OS qui remplace un OS normal pour aboutir à une image de machine virtuelle minimale et dédiée à l’application.
        *** *Kata containers* lance les conteneurs dans une *micro-VM dédiée*, optimisée pour démarrer vite et conçue pour cet usage. Un composant sur la machine hôte permet de faire le proxy et d’envoyer les instructions à l’agent Kata via l’hyperviseur. Les micro-VMs sont des VMs avec un minimum de fonctionnalités, seulement le strict nécessaire pour faire fonctionner des conteneurs.
    ** Ces "sandbox runtimes" permettent d’isoler les conteneurs, mais au prix de *performances dégradées*, et parfois plus : 
        *** *gVisor* n’est pas compatible avec toutes les applications, notamment celles qui nécessitent un accès direct aux système de fichier, et il impactent aussi les performances.
        *** *Nabla container* induit également une baisse de performance et plus important encore, il n’est pas tout à fait fini et *ne semble plus très maintenu*.

    ** *2018/05/22* : *Kata containers* : lancement de la v1.0 le 2018/05/22 (https://techcrunch.com/2018/05/22/the-kata-containers-project-hits-1-0/)

    ** *2018/05/02* : *gVisor* : release initiale en 2018/05/02 (https://en.wikipedia.org/wiki/GVisor)
        *** blog de Google annonçant la sortie de gVisor le 2018/05/02 : https://cloud.google.com/blog/products/identity-security/open-sourcing-gvisor-a-sandboxed-container-runtime +
        "To that end, we’d like to introduce gVisor, a new kind of sandbox that helps provide secure isolation for containers, while being more lightweight than a virtual machine (VM). gVisor integrates with Docker and Kubernetes, making it simple and easy to run sandboxed containers in production environments."
        *** https://www.zdnet.com/article/google-open-sources-gvisor-a-sandboxed-container-runtime/ (2018/05/03) : +
        "With gVisor, Google has introduced a new way to *sandbox containers*. These are containers that provide a *secure isolation boundary* between the host operating system and the application running within the container."

    ** *2018/07* : *Nabla containers* : les Nabla containers ont été lancés en 2018/07 https://blog.hansenpartnership.com/a-new-method-of-containment-ibm-nabla-containers/ 
    ** Le choix de ces nouveaux runtimes est expliqué par Justin Cormarck, le CTO de Docker, à la KubeCon 2018 : https://static.sched.com/hosted_files/kccna18/c6/KubeCon_%20How%20to%20Choose%20a%20Kubernetes%20Runtime.pdf / https://www.youtube.com/watch?v=OZJkwvAnLb4 +
    Le choix de ces nouveaux containers runtimes est lié à l'usage de plus en plus massif de Kubernetes, et des containers qu'il fait tourner : de plus en plus de containers qui tournent impliquant une attention plus poussée à leur sécurité.

* émergence des *daemonless runtimes* et du *rootless* : *podman* (avec *buildah* et *Skopeo*)

    ** *2018/04* : 1ere release de Podman sur le repo https://github.com/containers/podman/releases +
        "*Podman* works with the “runC” we mentioned earlier so it works in accordance with the *daemonless* concept." It corrects some "daemon with" problems : 
            *** At the point where no news is received from Daemon, there will be no access to the processes.
            *** All Docker operations are performed by one or more users with the same root privileges. This could create a vulnerability.
            *** Pour une bonne présentation du pourquoi de podman (les problèmes de sécurité de Docker et l'hégémonie de Kubernetes) et une demo de son utilisation, voir https://www.redhat.com/en/blog/say-hello-buildah-podman-and-skopeo (2019/10) +
            "This excites some people who always saw the *monolith daemon that required root access for everything as a problem*. This brings us to the heart of this article – the *daemon-less* and largely *rootless* suite of container management tools."
            *** *Podman ne build pas d'image OCI*, il délègue cela à buildah

            *** Documentation de Docker qui mentionne que le *daemon docker tourne toujours en root* : https://docs.docker.com/engine/install/linux-postinstall/
                **** un mode pour faire run Docker without root privileges existe, mais ce n'est pas parfait, et c'est arrivé plus tard : +
                https://docs.docker.com/engine/install/linux-postinstall/
                **** "The Docker daemon always runs as the root user. +
                If you don’t want to preface the docker command with sudo, *create a Unix group called docker and add users to it*. When the Docker daemon starts, it creates a Unix socket accessible by members of the docker group."
                    ***** tout le problème vient de groupe "docker", voir https://github.com/chrisfosterelli/dockerrootplease et https://fosterelli.co/privilege-escalation-via-docker (2015/04/22)

        *** *Buildah* : Buildah is a common containerize tool for container systems that comply with the OCI (Open Container Initiative) standards, one of the most important reasons for its development being its power in building container images.
            **** 1st release v0.11 2018/01/17
            **** The build commands in Podman are actually a subset of Buildah commands and they use the same codes.
            **** Buildah also works as rootless and daemonless.

        *** Voir également cet excellent article sur les *daemonless container runtimes* Podman et Buildah, ainsi que le lien qui les unit : https://developers.redhat.com/blog/2018/11/20/buildah-podman-containers-without-daemons : +
        "Kubernetes installations can be complex with multiple runtime dependencies and runtime engines. *CRI-O* was created to provide a lightweight runtime for Kubernetes which adds an *abstraction layer between the cluster and the runtime that allows for various OCI runtime technologies*. However you still have the *problem of depending on daemon*(s) in your cluster for builds - I.e. if you are using the cluster for builds you still need a Docker daemon. +
        Enter Buildah. Buildah allows you to have a Kubernetes cluster without any Docker daemon for both runtime and builds. Excellent. But what if things go wrong? What if you want to do troubleshooting or debugging of containers in your cluster? Buildah isn’t really built for that, what you need is a client tool for working with containers and the one that comes to mind is Docker CLI - but then you’re back to using the daemon. +
        This is where Podman steps in. Podman allows you to do all of the Docker commands without the daemon dependency. To see examples of Podman replacing the docker command, see Alessandro Arrichiello's Intro to Podman and Doug Tidwell's Podman—The next generation of Linux container tools. +
        With Podman you can run, build (it calls Buildah under the covers for this), modify and troubleshoot containers in your Kubernetes cluster. With the two projects together, you have a well rounded solution for your OCI container image and container needs."
        
        *** *Skopeo* : gestion d'image, au sens de téléchargement, push et signature (principalement)

-> *2019* : GRANDE ETAPE - les conséquences de l'essor de Kubernetes (le déclin de Docker et d'autres container runtimes)

* *2019/04* : la CNCF archive le projet rkt, suite à une adoption utilisateur en forte baisse

* *2019/11/13* : Docker se scinde en 2 : Mirantis rachète Docker Enterprise, et Docker Inc se recentre autour de Docker Desktop (et Docker Hub) et lève 35 millions auprès de ses précédents investisseurs Benchmark Capital et Insight Partners. +

    Voici l'explication officielle de Docker : +
    "Docker is ushering in a new era with a return to our roots by focusing on advancing developers’ workflows when building, sharing and running modern applications. As part of this refocus, Mirantis announced it has acquired the Docker Enterprise platform business,” Docker said in a statement when asked about this change. “Moving forward, we will expand Docker Desktop and Docker Hub’s roles in the developer workflow for modern apps. Specifically, we are investing in expanding our cloud services to enable developers to quickly discover technologies for use when building applications, to easily share these apps with teammates and the community, and to run apps frictionlessly on any Kubernetes endpoint, whether locally or in the cloud." +
    Pour plus d'explication, voir : 
        ** https://techcrunch.com/2019/11/13/mirantis-acquires-docker-enterprise/
        ** https://www.nextinpact.com/lebrief/40573/10329-docker-se-scinde-en-deux--mirantis-rachete-la-branche---entreprise--

* *2020/02* : project rkt is ended (https://github.com/rkt/rkt/issues/4024), so same thing for appc

* *2020/12* : dockerd qui est déprécié pour Kubernetes 1.20 (2020/12), et remplacé par containerd +

    Kubernetes community announced it is deprecating Docker as a container runtime after v1.20. +
    Pour être plus précis, c'est l'usage du docker daemon (dockerd), au travers de Dockershim, qui est déprécié par Kubernetes. +
    Pour se "connecter à Docker", Kubernetes passera à partir de sa v1.20 par containerd : kubelet appelera directement containerd via son CRI-plugin

* *2020/06* : Gartner predicts that by 2022, more than 75% of global organizations will be running containerized applications in production, up from less than 30% today. +

    Worldwide container management revenue will grow strongly from a small base of $465.8 million in 2020, to reach $944 million in 2024, according to a new forecast from Gartner, Inc. +
    For more details, see https://www.gartner.com/en/newsroom/press-releases/2020-06-25-gartner-forecasts-strong-revenue-growth-for-global-co 

* *2021/01* : Red Hat Enterprise Open Source Report 2021 shows container adoption is already widespread +

    Gartner predicted in 2020 that, by 2022, more than 75% of global organisations will be running containerised applications in production, against 30% in 2020. +
    The analyst’s figures are reflected in the latest Red Hat Enterprise Open Source Report 2021, which shows container adoption is already widespread. +
    Of the 1,250 IT leaders surveyed, just under 50% said they use containers in production to at least some degree. A further 37% use containers for development only, while just 16% are still evaluating or researching container adoption, according to Red Hat. +
    Voir https://www.computerweekly.com/feature/Containers-for-a-post-pandemic-IT-architecture

* *2022/10* : Poussée des modules Wasm ("containers Javascript") et technical preview Docker PLUS Wasm

*Informations globales à donner* : 

    * insister sur les Linux Kernel features, pas que les cgroups et namespaces
        ** montrer un exemple de code expliquant que l'on peut "coder un container" uniquement avec ces features (Gist en GO de *Julien Friedman*)
    * parler des "user namespaces qui marchent vraiment" de la version 3.8 du kernel Linux qui ont, cf Solomon Hykes lui-même, permis la sortie de Docker

    * bien définir ce qu'est un container runtime
        ** et définir ce que sont les low-level et high-level container runtimes
    * schéma de l'architecture de Docker A PARTIR DE LA 1.11
    * schéma des relations entre Docker ET Kubernetes avec les container runtimes qu'ils utilisent
    
    * parler des problèmes de sécurité qui faisaient peur à la sortie de Docker avec les containers : "shared OS kernel"

    * pour parler du daemonless, bien rappeler que tous les noms d'outils se terminant par "d" indiquent qu'il s'agit de daemon
        ** https://en.wikipedia.org/wiki/Daemon_(computing) : a daemon is a computer program that runs as a background process, rather than being under the direct control of an interactive user. +
        For example, syslogd is a daemon that implements system logging facility, and sshd is a daemon that serves incoming SSH connections.

    * redéfinir rapidement ce qu'est un "shim" +
    "In tech terms, a shim is a component in a software system, which acts as a *bridge between different APIs*, or as a compatibility layer. A shim is sometimes added when you want to use a third-party component, but you need a little bit of glue code to make it work."

*Ressources à donner dans le talk* : 

    * la série d'articles de Ian Lewis
    * Le talk durant lequel Solomon Hykes a introduit Docker, le 2013/06/07 : https://www.youtube.com/watch?v=3N3n9FzebAA[Why Docker ?]
    * Le talk a été donné à la conférence dotScale 2013, juste après la 1ere publication de Docker.

* TODO : parler du grand problème de Docker, le "monolith daemon that required root access for everything" +
Ce qui a conduit à l'émergence des daemonless et autre rootless comme Podman et Buildah
* TODO : parler des confusions possibles en dockershim et containerd-shim

* TODO : pour Docker, bien dire que quoi est constitué Docker aujourd'hui, et faire apparaître un schéma des éléments du Docker Engine : +
Docker server (avec dockerd, containerd, containerd-shim, runc), l'API (Docker Engine API) et la CLI (ligne de commande "docker") +
S'inspirer par exemple de https://iximiuz.com/en/posts/implementing-container-runtime-shim/
    ** et en ajouter un de plus pour faire apparaître dockershim et le lien avec Kubernetes avant la v1.20
        *** et en ayant sur le même schéma dockershim et dockerd ce serait fantastique
    ** faire un mix avec le schéma du docker engine disponible ici : https://kubernetes.io/blog/2018/05/24/kubernetes-containerd-integration-goes-ga/ (qui fait apparaître dockerd)
    ** Voici un schéma faisant apparaître tous les éléments de Docker, avec dockershim et dockerd (faire juste attention au schéma indiquant le docker engine, on a l'impression que ce dernier utilise PLUTOT QUE contient dockerd, la CLI et l'API)

=== Outils à tester pour créer un chronologie en ligne ?

Voir le https://elearningindustry.com/top-10-free-timeline-creation-tools-for-teachers. +
Les outils suivants ont l'air bien adaptés : 

    * https://timeline.knightlab.com/
    * https://www.timetoast.com/ : je trouve cet outil très bien, à tester

-> Pas forcément le rendu le plus ergonomique qui soit : 

    * trop d'infos et de détails à donner en chronologie complète
    * et pour la chronologie condensée, une format texte Asciidoctor est finalement très lisible

== Chronologie temporelle condensée

* *1979* : développement de *chroot*, dans la version 7 d'Unix, Le concept d'isolation émerge. +
-> "le point de départ, les prémices de la conteneurisation"
* *2000/03* : *FreeBSD Jails* 
* *2001* : *Linux VServer*
* *2002/08* : Les 1er *Linux namespaces* (mount namespaces) sont ajoutés au kernel Linux 2.4.19 (2002/08/03)
* *2003* : Google introduced *Borg*, the organization's container cluster management system.
* *2004* : *Solaris Containers*
* *2005* : *Open VZ* (Open Virtuzzo)
* *2006* : début des travaux sur les *Process Containers* chez Google (later renamed *cgroups* / Control Groups)
* *2007/10* : ajout des 1eres briques de l'implémentation des *user namespaces* dans le kernel Linux 2.6.23 par Eric Biederman (Red Hat)
* *2008/01* : GRANDE ETAPE - ajout de la fonctionnalité des *cgroups* dans le kernel Linux 2.6.24
* *2008/08* : GRANDE ETAPE - création du projet *Linux Containers (LXC)* par des ingénieurs d'IBM. +
* *2011* : Warden
* *2013/02* : GRANDE ETAPE - ajout des *user namespaces* au kernel Linux 3.8
* *2013/03/20* : ETAPE MAJEURE - *1ere release de Docker* +
-> "Le monde découvre les containers"
* *2013/10* : LMCTFY
* *2014/02/20* : release de la 1ere version 1.0 de LXC
* *2014/06/07* : GRANDE ETAPE - toute première release de *Kubernetes* par Google (1er commit GitHub)
* *2014/11* : 1ere release de rkt (prononcer "rocket"), le container runtime créé par les équipes de CoreOS.
* *2015/06/17* : création de *WebAssembly*
* *2015/06* : GRANDE ETAPE - *Docker Inc donne la codebase du projet Docker à l'OCI*, projet de la Linux Fondation, *créé pour cette occasion*.
* *2015/07* : GRANDE ETAPE - *runc* est publié pour la 1ere fois, et son code a *tout de suite donné par Docker à l'OCI* (runc est l'implémentation de référence de la runtime-spec)
* *2015/07/21* : ETAPE MAJEURE - *release de la 1ere version de Kubernetes* par Google, et *création de la CNCF*, comme umbrella projet de la Linux Foundation, à laquelle Kubernetes sera donné comme élément fondateur.
* *2015/12* : GRANDE ETAPE - Docker introduce *containerd*
* *2016/03/14* : 1ere apparition des cgroups v2 dans le Linux Kernel 4.5
* *2016/04/13* : ETAPE MAJEURE - sortie de la *version 1.11 de Docker* : 1ere release de Docker basée sur containerd et runC (OCI) +
-> cette étape marque l'apparition des low-level et des high-level containers runtimes
* *2017/03/06* : GRANDE ETAPE - release de la v1.0 de WebAssembly 1.0 (MVP)
* *2017/03/15* : GRANDE ETAPE - *Docker's donation of containerd project to the CNCF*, and *CoreOS's donation of rkt to the CNCF*
* *2017/04* : Microsoft enabled organizations to run Linux containers on Windows Server.
* *2017/04/12* : apparition des "Distroless" container images chez Google
* *2017/07/19* : Release of the OCI v1.0 runtime and image specifications
* *2017/10* : DockerCon 2017, Docker announced they will support the Kubernetes container orchestrator, same thing for Azure and Amazon.
* *2018* : ETAPE MAJEURE - *L'avènement de Kubernetes*, où tous les Cloud providers propose leur offre de Kubernetes managé +
-> A partir de cette date, les containers deviennent présents partout, et dès lors, la *sécurité des containers* devient un sujet primordial.
    ** 2018 marque également l'apparition des "*sandbox runtimes*" : *Kata containers*, *gVisor*, *Nabla*
        *** *2018/05/02* : release initiale de *gVisor* par Google
        *** *2018/05/22* : lancement de la v1.0 de *Kata containers*
        *** *2018/07* : lancemnet des *Nabla containers*
    ** ainsi que celles des *daemonless runtimes* et du *rootless* : *podman* (avec *buildah* et *Skopeo*)
        *** *2018/04* : 1ere release de Podman
* *2019* : GRANDE ETAPE - les conséquences de l'essor de Kubernetes (le déclin de Docker et d'autres container runtimes)
* *2019/03/27* : ETAPE MAJEURE - 1st version de WASI (WebAssembly System Interface) qui marge l'essor de WebAssembly
* *2019/04* : la CNCF archive le projet rkt, suite à une adoption utilisateur en forte baisse
* *2019/11/12* : GRANDE ETAPE - *création de la Bytecode Alliance*, partenariat industriel poussant le développement de Wasm, tout particulièrement en dehors du browser (va marquer le début de l'essort rapide de WebAssembly)
* *2019/11/13* : Docker se scinde en 2 : Mirantis rachète Docker Enterprise, et Docker Inc se recentre autour de Docker Desktop (et Docker Hub) 
* *2020/02* : le project rkt est arrêté (https://github.com/rkt/rkt/issues/4024)
* *2020/12* : le daemon docker (dockerd) est déprécié pour Kubernetes 1.20 (2020/12), et est remplacé par containerd
* *2021* : les containers sont maintenant partout, et le constat suivent tiré des études de Gartner : +
"Gartner predicted in 2020 that, by 2022, more than 75% of global organisations will be running containerised applications in production, against 30% in 2020."
* *2021/05/05* : release de la v1.0.0 (version stable) de la distribution-spec de l'OCI (3e spécification de l'OCI)
* *2022/10/24* : ETAPE MAJEURE - Technical Preview de Docker, announces support for WebAssembly, in cooperation with WasmEdge, and joins the Bytecode Alliance as a voting member

~46 dates à présenter dans les 45 min de ce format conférence.

    * si l'on veut garder 5 min pour les questions, cela laisse 40 min pour le talk, soit ~52 par date
    * 52 sec ne seront pas suffisante pour certaines grandes dates (docker, l'apparition des high et low level container runtimes, WebAssembly, etc.) +
    Il faudra passer rapidement certaines dates pour se concentrer sur les plus importantes

== Points à aborder durant le talk

* Intro : se présenter LinkedIn et GitHub
    ** et redonner l'article dans le blog de Devoxx France depuis 2022 pour la préparation du salon ET la base de connaissance.
    ** et reparler rapidement du talk de veille donné à Devoxx (à la base DevFest Nantes)
    ** dire que je donnerai l'URL vers mes notes au final, car on ne pourra pas tout aborder

* placer le "moto" du talk : "Pour comprendre le présent, et pouvoir envisager sereinement le futur, il est bon de connaître / pas avoir oublié le passé / d'où l'on vient"
    ** ou "Pour comprendre où l'on est, pouvoir envisager sereinement où l'on va, il est bon de savoir d'où l'on vient"

* A chaque date (certaines dates ?), redonner des éléments de l'époque
* Présenter chaque changement de date en gros plan
    
* Commencer par un disclaimer de ce que sera et ne sera pas le talk
    ** on ne va PAS vous apprendre ici à utiliser tous les outils et techno citées
    ** et je ne suis pas un expert de toutes les techno citées...

* expliquer que certaines technos deviennent des superstars et d'autres meurent...(cf Rocket)
* insérer la courbe de hype du Gartner pour certaines dates importantes de la conteneurisation (release de Docker par exemple)

* Bien définir ce qu'est un runtime
* Et bien définir également ce qu'est un container
* Donner la phrase introductive "Pour comprendre le présent..."

* Les PC en 1979 (rares, chers, peu puissants) et bien redéfinir ce qu'est un runtime
    ** "Bon, on va commencer par un temps que les moins de 20 ans, ne peuvent pas connaîtreeeuuuuuuuu"

* bien réexpliqer cgroups et namespaces
    ** et redonner les docs de Julia Evans

* Docker par Solomon et parallèle avec les conteneurs maritimes et le "shipping"
    ** si les conteneurs avaient tous des tailles différentes, ce serait un fiche lego / tetris que de les charger sur un porte-conteneur (trouver une photo d'un porte-conteneur)
    ** mettre une image de Solomon à cette conf
* Sortie de Docker, insister sur les 1ers problèmes de sécurité
    ** cycle de hype du Gartner et "présence oppressante" des OPS dans votre dos quand vous avez pensé "il faut qu'on pousse ça en PROD !"
    ** redonner le bout de code Docker qui permet en 1 ligne une élévation de privilèges, le fameux "Docker root please"
        *** https://github.com/VonC/blog/blob/201711_docker/PITCHME.md
        *** https://github.com/chrisfosterelli/dockerrootplease 
            **** https://fosterelli.co/privilege-escalation-via-docker (2015/04/22)
            **** "A tous ceux qui sont OPS dans la salle, attention, souvenir douloureux..."
            **** "s'il y a des OPS dans la salle, pardonnez-moi, je vais rappeler un souvenir douloureux..." +
            "Ca rappelle des souvenirs à certains ?" et slide avec "Docker Root Please"

* création "à la main" d'un conteneur en quelques lignes de codes

* donner des détails sur les différents low and high level container runtimes
* bien réexpliquer ce qu'est un shim

* reprendre le très bon schéma de https://www.docker.com/blog/docker-wasm-technical-preview/ faisant apparaître le containerd-wasm-shim
* WASM : la phrase de Solomon sur WASM, "si WASM / WASI avait existé en 2008, il n'y aurait pas eu Docker..."
* Donner la ressource https://wasmlabs.dev/articles/docker-without-containers/ et les infos sur Wasm Labs
    ** et donner l'exemple de Wordpress qui tourne dans le browser : https://wordpress.wasmlabs.dev/

* Remerciements : bien indiquer que j'ai utilisé un très grand nombre de doc pour ma curation de contenus, et que j'en remercie tous les auteurs, même si je ne les ai pas tous cités.

-> La tendance : l'industrialisation (normalisation, cf la 3e spec de l'OCI, la bytecode alliance) et donc la sécurité

.La réflexion globable à avoir sur toute l'évolution proposée
[IMPORTANT]
====
-> Cette réflexion peut évidemment se généraliser à d'autres secteurs (mais sûrement pas tous non plus)

* Au début, les outils n'étaient pas là
* Puis ils ont été créés (namespaces, cgroups, Jails, Linux VServer, LXC, etc.) MAIS ils étaient compliqués d'usage / d'utilisation
* Donc on passe à la simplification de ces derniers : Warden et surtout Docker
    ** Mais également LXC qui était historiquement utilisé "tel quel" par Docker (avait le mérite d'exister, mais était compliqué d'usage) avant d'être réécrit en interne pour être simplifié (libcontainer)
* Et une fois qu'on a quelque chose qui marche ET qui est simple d'utilisation, on peut l'ouvrir, et le mettre à la disposition de la communauté (libcontainer qui devient runc et qui est donné à l'OCI)
====

TODO : ajouter un slide pour expliquer cette réflexion ? Sous forme de schéma fléché ?

    * donner également un exemple récent dans un autre secteur. +
        ** Exemple avec la sécurité, et la "jungle actuelle" des différentes offres de protection.
        Cf BFM Tech & Co du 2023/04/13, avec le rapprochement Free Pro et ITrust pour pouvoir proposer un service de sécurité "plus facile d'accès" et "simple d'utilisation"
        guichet unique
        FIC : Forum International de la Cybersécurité
        Phase de simplication : plutôt que de devoir couvrir les différentes fonctionnalités requises du secteur avec 15 offres différentes (ce qu'aucun DSI n'est capable de faire, sauf s'il vient lui-même de ce secteur), des offres / solutions de regroupement apparaissent (exemple des "guichets uniques") afin de proposer une "offre globale", plus simple à appréhender


== Création des slides

1. clone du repo avec récupération du contenu du submodule reveal.js :

    git clone --recursive https://github.com/Ardemius/history-of-containerization.git

2. lancement du container :

    docker run -it -v <path-to-the-cloned-repo>/docs:/documents/ asciidoctor/docker-asciidoctor

    docker run -it -v D:\resources\my-talks-and-trainings\history-of-containerization:/documents/ asciidoctor/docker-asciidoctor

3. génération des slides :

    asciidoctor-revealjs history-of-containerization-slides.adoc -o docs/slides.html

[NOTE]
====
Le template de slides pour ce devoxx est disponible sur le repo : https://github.com/quantixx/template-presentation
====

== Lexique

[glossary]
ghcr:: GitHub Container registry



