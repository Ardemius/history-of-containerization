= L'évolution de la conteneurisation, de 1979 à aujourd'hui
Thomas SCHWENDER <https://github.com/ardemius[@ardemius]>
// Handling GitHub admonition blocks icons
ifndef::env-github[:icons: font]
ifdef::env-github[]
:status:
:outfilesuffix: .adoc
:caution-caption: :fire:
:important-caption: :exclamation:
:note-caption: :paperclip:
:tip-caption: :bulb:
:warning-caption: :warning:
endif::[]
:imagesdir: ./images
:source-highlighter: highlightjs
// Next 2 ones are to handle line breaks in some particular elements (list, footnotes, etc.)
:lb: pass:[<br> +]
:sb: pass:[<br>]
// check https://github.com/Ardemius/personal-wiki/wiki/AsciiDoctor-tips for tips on table of content in GitHub
:toc: macro
:toclevels: 4
// To number the sections of the table of contents
:sectnums:
// To turn off figure caption labels and numbers
:figure-caption!:
// Same for examples
//:example-caption!:
// To turn off ALL captions
// :caption:

toc::[]

== Overview

Proposition de sujet (conference) pour Devoxx France 2022

Talk ayant pour but de retracer l'histoire de la conteneurisation, de donner ses grandes dates depuis ses débuts jusqu'à nos jours, et d'expliquer les raisons derrière chaque grande évolution.

L'autre idée derrière ce talk : "mais POURQUOI on a fait tout ça ?"

Ce talk ne va pas rentrer dans le détail de toutes les étapes

*Thème principal* : Cloud, Containers et Infrastructure, DevOps

*Niveau de la présentation* : Ouvert à tout niveau d'audience, débutant comme expert

*Tags* : 

    * Container
    * history
    * container characteristics

== Recherche du nom de la conférence

* A Brief History of Containers
* A brief history of containerization, from yesterday to today

* Petite histoire de la conteneurisation, d'hier à nos jours
* *L'évolution de la conteneurisation, de 1979 à aujourd'hui* : on va garder ce titre

== Abstract

* Expliquer le "pourquoi ces changements, ces évolutions ?" afin de comprendre comment nous en sommes arrivés à la situation actuelle

.idées de formulation
----
La plupart d'entre nous ont découvert le concept de conteneur en 2013 avec l'arrivée de Docker.
2013 avait marqué l'arrivée de Docker, et fait connaître le concept de conteneur au plus grand nombre.

Aujourd'hui, avec l'avènement du Cloud et de Kubernetes, les conteneurs sont partout autour de nous.


Mais en fait, l'histoire de ces derniers / des conteneurs est bien plus étendue que cela, et il faut remonter à la fin des années 70s pour en trouver le véritable début / commencement avec l'apparition de chroot.
Mais l'histoire de ces derniers / des conteneurs est en fait bien plus étendue que cela, et il faut remonter à la fin des années 70s pour en trouver le véritable début / commencement avec l'apparition de chroot.
Mais l'histoire de la conteneurisation a en fait débuté il y a bien longtemps, dès la fin des années 70s avec l'apparition de chroot, et depuis

Toutefois, l'histoire de la conteneurisation reste globalement méconnue.
Qui pour se souvenir que tout a finalement commencé en 1979 avec l'apparition de chroot, qui marqua le début de l'isolation des process ?
Qu'il a fallu attendre d'avoir les cgroups (2008), puis plus tard les user namespaces (2013) pour qu'enfin Docker puisse voir le jour ?
Que la généralisation des containers, conséquence de l'adoption de Kubernetes par tous les Cloud providers (2018), entraîna l'apparition des sandbox runtimes et des daemonless runtimes ?

Au cours de ce talk, nous allons détailler les grandes étapes qui ont marqué l'histoire de la conteneurisation, et expliquer ce qui les a déclenchées / POURQUOI elles ont eu lieu.
L'objectif est qu'en fin de séance, vous ayez compris quelle orientation a suivi la conteneurisation depuis toutes ces années, et ce vers quoi nous allons.

Le but de ce talk est de vous expliquer POURQUOI les choses ont changés, afin de vous permettre de comprendre l'orientation qu'a suivi la conteneurisaiton depuis toutes ses années.
Pourquoi a-t-il 


Après ce sont suivis les Linux namespaces, les cgroups, Docker, l'arrivée des low-level et high-level runtimes avec runc et containerd, jusqu'aux derniers sandbox runtimes et daemonless runtimes.

Au cours de ce talk, nous allons détailler les étapes majeures de la conteneurisation, et vous permettre de comprendre POURQUOI <nous> sommes passés de l'une à l'autre, et quelles orientations ont suivies / suivent les containers <aujourd'hui>

et il faut remonter à la fin des années 70 pour voir apparaître chroot, le point de départ / l'ancêtre de la containerisation

et a été marquée par de
et de nombreux évènements et shift technologiques ont été vécus pour arriver aux conteneurs que nous conaissons aujourd'hui

Au cours de ce talk, nous allons détailler

cgroups et namespaces, arrivée de docker, split et arrivée des low-level et high-level runtimes avec runc et containerd, etc.
Nous allons détailler dans ce talk les grandes étapes de la conteneurisation, et vous permettre de comprendre POURQUOI nous 

Avec l'avènement du Cloud et de Kubernetes, les containers sont aujourd'hui partout autour de nous.
2013 avait 

Chaque 
Plus Docker, il y a maintenant runc et containerd
----

.version finale
----
La plupart d'entre nous ont découvert le concept de conteneur en 2013 avec l'arrivée de Docker.
Aujourd'hui, avec l'avènement du Cloud et de Kubernetes, les conteneurs sont partout autour de nous.

Toutefois, l'histoire de la conteneurisation reste globalement méconnue.
Qui pour se souvenir que tout a finalement commencé en 1979 avec l'apparition de chroot, qui marqua le début de l'isolation des process ?
Qu'il a fallu attendre d'avoir les cgroups (2008), puis plus tard les user namespaces (2013) pour qu'enfin Docker puisse voir le jour ?
Que la généralisation des containers, conséquence de l'adoption de Kubernetes par tous les Cloud providers (2018), entraîna l'apparition des sandbox runtimes et des daemonless runtimes ?

Au cours de ce talk, nous allons détailler les grandes étapes qui ont marqué l'histoire de la conteneurisation, et expliquer POURQUOI elles ont eu lieu.
L'objectif est qu'en sortant, vous ayez compris quelle orientation a suivi la conteneurisation depuis toutes ces années, et ce vers quoi nous allons.
----

.version finale au format markdown (pour le CFP de Devoxx France)
----
La plupart d'entre nous ont découvert le concept de conteneur en **2013** avec l'arrivée de **Docker**.  
Aujourd'hui, avec l'avènement du Cloud et de Kubernetes, les conteneurs sont partout autour de nous.

Toutefois, **l'histoire de la conteneurisation** reste globalement méconnue.  
Qui pour se souvenir que tout a finalement commencé en **1979** avec l'apparition de **chroot**, qui marqua le début de l'isolation des process ?  
Qu'il a fallu attendre d'avoir les **cgroups** (2008), puis plus tard les **user namespaces** (2013) pour qu'enfin Docker puisse voir le jour ?  
Que la généralisation des containers, conséquence de **l'adoption de Kubernetes** par tous les Cloud providers (2018), entraîna l'apparition des **sandbox runtimes** et des **daemonless runtimes** ?

Au cours de ce talk, nous allons détailler les **grandes étapes** qui ont marqué l'histoire de la conteneurisation, et expliquer **POURQUOI** elles ont eu lieu.  
L'objectif est qu'en sortant, vous ayez compris **quelle orientation a suivi la conteneurisation** depuis toutes ces années, et ce vers quoi nous allons.
----

== Message pour le comité

* Donner le lien vers le repo GitHub, pour montrer la recherche documentaire
* Expliquer que l'idée du talk est venue de la prez à la StarTECH sur les low-level et high-level containers : quelques personnes connaissaient ces concepts, mais personne ne savaient d'où ils venaient.

----
Bonjour,

Mi-2021 j'ai fait une présentation à la communauté technique de ma société sur les low-level et les high-level container runtimes (ex : runc et containerd). Il est apparu que, si certains en avaient déjà entendu parler, **aucun ne savait réellement comment ni pourquoi les conteneurs en étaient arrivés là**.

C'est de là d'où m'est venue l'idée de ce talk : **détailler la chronologie des grandes étapes de la conteneurisation**, de ses débuts à nos jours, et surtout **expliquer POURQUOI ces étapes ont eu lieu**.  
J'ai demandé aux membres de la communauté s'ils trouvaient le sujet intéressant, et les retours ont été très enthousiastes.

J'ai déjà terminé ma recherche documentaire, ainsi que l'étude associée en très grande partie.  
Ceux qui le souhaitent peuvent déjà jeter un oeil à la chronologie que je compte présenter dans mes notes sur GitHub :  
[https://github.com/Ardemius/history-of-containerization/blob/main/history-of-containerization-notes.adoc#8-frise-temporelle-r%C3%A9duite](https://github.com/Ardemius/history-of-containerization/blob/main/history-of-containerization-notes.adoc#8-frise-temporelle-r%C3%A9duite)  
J'ai dans l'idée de la passer dans un outil comme [https://www.timetoast.com/](https://www.timetoast.com/) pour la rendre plus dynamique.

**Je suis preneur de toutes vos questions et retours** pour approfondir le sujet.  
Il est déjà prévu avec ma communauté technique que je fasse plusieurs présentations blanches début d'année pour aider ma préparation.
----

.version markdown pour le CFP de Devoxx France 2022
----
Bonjour,

Mi-2021 j'ai fait une présentation à la communauté technique de ma société sur les low-level et les high-level container runtimes (ex : runc et containerd). Il est apparu que, si certains en avaient déjà entendu parler, aucun ne savait réellement comment ni pourquoi les conteneurs en étaient arrivés là.

C'est de là d'où m'est venue l'idée de ce talk : détailler la chronologie des grandes étapes de la conteneurisation, de ses débuts à nos jours, et surtout expliquer POURQUOI ces étapes ont eu lieu.
J'ai demandé aux membres de la communauté s'ils trouvaient le sujet intéressant, et les retours ont été très enthousiastes.

J'ai déjà terminé ma recherche documentaire, ainsi que l'étude associée en très grande partie. Ceux qui le souhaitent peuvent déjà jeter un oeil à la chronologie que je compte présenter dans mes notes sur GitHub :  
https://github.com/Ardemius/history-of-containerization/blob/main/history-of-containerization-notes.adoc#8-frise-temporelle-r%C3%A9duite  
J'ai dans l'idée de la passer dans un outil comme https://www.timetoast.com/ pour la rendre plus dynamique.

Je suis preneur de toutes vos questions et retours pour approfondir le sujet.  
Il est déjà prévu avec ma communauté technique que je fasse plusieurs présentations blanches début d'année pour aider ma préparation.
----



















== RESSOURCES

* A : https://www.tutorialworks.com/difference-docker-containerd-runc-crio-oci/ : excellente ressource
    ** Jeter un oeil au site parent, qui est vraiment très bien : https://www.tutorialworks.com/
    ** Attention, les schémas ne font pas suffisamment apparaître le *Docker Daemon* (*dockerd*) selon moi

* B : https://blog.engineering.publicissapient.fr/2019/12/23/docker-est-mort-vive-docker/
    ** Dans les 1eres minutes, le Docker Daemon est mieux présenté
        *** reprendre le schéma en 2:01, il est complet AVEC le docker daemon
            **** pour un schéma complet, voir également : http://sysblog.informatique.univ-paris-diderot.fr/wp-content/uploads/2020/03/Docker-2.3.png
        *** on pourrait également faire apparaître le *docker registry* sur le schéma

    ** Très bonne présentation des différents éléments de "Docker", qui est un fork de *Moby*
        *** https://mobyproject.org/ : Moby is an open framework created by Docker to assemble specialized container systems without reinventing the wheel.
            **** Moby permet de pratiquer avec la plomberie de Docker "Docker internals", il n'est pas conseillé si l'on souhaite simplement un moyen simple et rapide de lancer des containers

* Schéma de Docker en 2019 (récent) : https://www.codetd.com/en/article/6502770
    ** montre les 3 parties de *Docker engine*, à savoir : Docker Daemon (dockerd), ContainerD, RunC
        *** NON ! Préférer l'explication fournie plus bas : +
        Docker Engine = Docker Server (implémenté à l'aide de dockerd, qui lui même utilise containerd, qui lui même utilise runc) + API + CLI
        *** Docker Engine est qualifié de *container runtime* par Docker même (https://www.docker.com/products/container-runtime) +
        Je donne cette précision car en parlant de container runtimes, on parle plutôt de containerd et runc
    ** pour des définitions de *ContainerD* et *RunC*, voir https://jfrog.com/knowledge-base/the-basics-7-alternatives-to-docker-all-in-one-solutions-and-standalone-container-tools/
        *** voir également https://docs.docker.com/engine/api/, où il est écrit : +
            "Docker provides an API for interacting with the Docker daemon (called the Docker Engine API), as well as SDKs for Go and Python"
        *** NON ! Plus clair, site même de Docker : https://docs.docker.com/engine/ : 
+
----
Docker Engine acts as a client-server application with:

- A server with a long-running daemon process dockerd.
- APIs which specify interfaces that programs can use to talk to and instruct the Docker daemon.
- A command line interface (CLI) client docker.
----
            **** A l'aide de cette dernière explication, on se rend compte que *Docker Engine* regroupe en fait la *CLI*, la Docker Engine *API* et le *Docker daemon*. +
            Ce dernier est peut-être considéré ici comme *"englobant" containerd et runc*, étant donné que le *schéma d'architecture* https://docs.docker.com/get-started/overview/#docker-architecture montre le docker daemon en lien avec la gestion des images, elle-même liée aux containers
            **** concernant la *Docker Engine API* permettant l'interaction avec le Docker Daemon, voir https://docs.docker.com/engine/api/

    ** autre bon schéma : https://www.aquasec.com/cloud-native-academy/docker-container/docker-architecture/ +
    Ce dernier indique également que le Docker Engine englobe la CLI, l'API de comm avec le docker daemon, et le docker daemon lui-même +
    PAR CONTRE, est-ce toujours totalement d'actualité ? Aucune mention à runc et containerd, ce qui me pose un petit problème...
        *** OUI, c'est bien toujours d'actualité. Vu plus bas, le docker server (implémenté à l'aide de docker daemon) contient bien / utilise bien containerd et runc.
    ** réponse finale ici : https://www.studytrails.com/2018/12/04/docker-architecture-engine-containerd-runc/ +
    *Docker Engine* est bien composé de : 
        *** *Docker Server*, qui est implémenté à l'aide de *docker daemon (dockerd)*, et qui est responsable de la création des images, containers, networks et volumes
            **** Et on considère que le *Docker Server contient containerd et runc*
        *** a *RESTFul API* to talk to the docker server -> donc une API pour parler à dockerd, c'est à dire *Docker Engine API*
        *** une *CLI* (the docker command)
    
    ** *dockerd* is the thing that helps you *work with volumes*, *networking* or even *orchestration*. +
    And of course it *can launch containers* or *manage images* as well, *but containerd is listening on linux socket* and this is *just translated to calls to its GRPC API*. +
    see https://alenkacz.medium.com/whats-the-difference-between-runc-containerd-docker-3fc8f79d4d6e

    ** Une bonne comparaison, rapide et efficace entre Docker et Kubernetes : (https://www.threatstack.com/blog/diving-deeper-into-runtimes-kubernetes-cri-and-shims) +
    "*Docker* is a technology for automating the process of deploying containers. *Kubernetes* is orchestration software that gives us an API to manage how the containers will run." +
    "In a broad sense, Docker runs on nodes, and Kubernetes runs clusters of nodes. To run containers in pods, Kubernetes uses runtimes. Considering what we know about runtimes and how they are defined, Docker can be considered a runtime for Kubernetes, and is a high-level runtime as defined in our last post."

    ** On pourrait également définir Docker très simplement ainsi : *Docker allows to run containerized apps*
        *** Au final, les composants de Docker ont pour but de : *build des images*, et *run des containers*

* dockerd vs containerd vs runc : https://stackoverflow.com/questions/46649592/dockerd-vs-docker-containerd-vs-docker-runc-vs-docker-containerd-ctr-vs-docker-c
    ** on y trouve aussi une bonne explication sur *shim* : +
    "(docker-)containerd-shim - After runC actually runs the container, it exits (allowing us to not have any long-running processes responsible for our containers). The shim is the component which sits between containerd and runc to facilitate this."

    ** toujours concernant shim (*docker-containerd-shim*), voir pour une bonne explication : https://www.threatstack.com/blog/diving-deeper-into-runtimes-kubernetes-cri-and-shims +
    Le point essentiel de shim est de permettre "It allows for *daemon-less containers*." +
    "It basically sits as the parent of the container’s processes to facilitate communications, and eliminates the long running runtime processes for containers." +
    "The processes of the *shim and the container* are bound tightly; however, they are *totally separated from the process of the container manager*" +
    "Shim allows a runtime (runC) to exit after the container is started. Without this we would still be subject to long runtime processes."
        *** cet article décrit également très bien Kubernetes et Docker, et les liens entre Kubelet, implémentation de CRI (CRI-O) et un low-level container runtime (très souvent runc)
    ** autre bon article sur le sujet : https://alenkacz.medium.com/whats-the-difference-between-runc-containerd-docker-3fc8f79d4d6e
        *** *containerd-shim* is the *parent process of every container started* and it *also allows daemon-less containers* (meaning you can upgrade docker daemon without restarting all your containers, which was a big pain)
    ** voir également https://oziie.medium.com/something-missed-history-of-container-technology-e978f202464a :
        *** It provides container operation by using runC. It also provides a “*Daemonless container*” environment. This means that there is no need for a long-running runtime process for containers. There are 2 benefits of running a Daemonless container :
            **** *runC* stops after container starts and it doesn’t have to work during the working container process.
            **** *containerd-shim* :  It keeps file information such as stdin (standard input), stdout (standard output), stderr (standard error), even if Docker or containerd becomes inoperable for any reason.

    ** *dockershim* est également très bien expliqué dans https://www.tutorialworks.com/difference-docker-containerd-runc-crio-oci/ : +
    "In tech terms, a shim is a component in a software system, which acts as a *bridge between different APIs*, or as a compatibility layer. A shim is sometimes added when you want to use a third-party component, but you need a little bit of glue code to make it work."

* autre *FANTASTIQUE ressource*, la série d'articles de *Ian Lewis* (2017/12) : https://www.ianlewis.org/en/container-runtimes-part-1-introduction-container-r
    ** en fait, toutes les différentes facettes de l'écosystème des containers y sont présentées (docker, dockerd, containerd, runc)
    ** et une fois lu, voir également https://alenkacz.medium.com/whats-the-difference-between-runc-containerd-docker-3fc8f79d4d6e

* pour une explication de ce qui a amené aux containers, avec les *namespaces*, les *cgroups* (control groups), l'isolation des appels (*seccomp-bpf*), et finalement les "containers Docker", voir l'excellent article https://jvns.ca/blog/2016/10/10/what-even-is-a-container/
    ** Docker a fourni un wrapping simple et facile d'utilisation de ces fonctionnalités du kernel Linux (et en a également apporté d'autres également)
    ** Regarder absolument le super Zine "How Containers work" de *Julia Evans* (2020) : https://wizardzines.gumroad.com/l/containers-zine/buyonegiveone / https://jvns.ca/blog/2020/04/27/new-zine-how-containers-work/
        *** Ce Zine contient une description sympa des *container Kernel features* : 
            **** *pivot_root* : set a process's root directory to a directory with the contents of the container image
                ***** difference between pivot_root and *chroot* : chroot is easy to escape from if you're root and pivot root isn't +
                -> so containers use pivot_root instead of chroot
            **** *cgroups* : limit memory / CPU usage for a group of processes
            **** *namespaces* : allow processes to have their own network / PIDs / users / hostname / mounts / and more !
            **** *seccomp-bpf* : security: prevent dangerous system calls
                ***** seccomp means "secure computing"
                ***** bpf, pour Berkeley Packet Filter, est une extension de seccomp
            **** *capabilities* : security: avoid giving root access +
            Capabilities allow to reduce the privileges of an active process
            **** *overlay filesystems* : optimization to reduce disk space used by containers which are using the same image
            **** quand on utilise *toutes les fonctionnalités précédentes*, on a un *container*
            **** Et un GROS reminder : *A container is a group of processes*

    ** LCC (Les Cast Codeurs) 270 : interview de *Nicolas De Loof* sur Docker et Docker Compose 
        *** Définition de Docker : "Docker est un moyen de lancer des applications, des process, mais on va prendre le process Linux, celui que tu veux faire tourner sur ta machine de PROD, et on va te donner un moyen simple de le faire tourner chez toi tout pareil"
            **** L'idée c'est vraiment, cf Nicolas, "moyen de lancer des applications"

    ** Cf wikipedia (https://en.wikipedia.org/wiki/Cgroups), *cgroups* : +
    "cgroups (abbreviated from control groups) is a Linux kernel feature that limits, accounts for, and isolates the resource usage (CPU, memory, disk I/O, network, etc.) of a collection of processes."
        *** la vidéo https://www.youtube.com/watch?v=sK5i-N34im8[cgroups, namespaces, and beyond: what are containers made from?] de Jérôme PETAZZONI (Docker) explique en détails les différentes fonctionnalités des *cgroups*, *différents types de namespaces*. +
        ATTENTION ! Elle date de 2015 !
            **** Il est également question des *container runtimes* qui sont basés sur les cgroups et les namespaces. +
            Exemples de container runtimes basés sur des namespaces et des cgroups : 
                ***** *LXC* (Linux Containers) : easy for sysadmins / OPS, hard for devs (requires significant elbow grease)
                ***** *systemd-nspawn*
                ***** *Docker*
                ***** *rkt*
                ***** *runC*
                ***** All those container runtimes use the same kernel features (at that time, 2015 ?)
            **** et maintenant des container runtimes qui ne sont PAS basés sur les namespaces et les cgroups : 
                **** *OpenVZ* : by example Travis CI gives you root in OpenVZ
                **** *Jails* / *Zones*
            **** la vidéo de Jérôme se termine par un live demo d'une création de container *à la main* (un début de container)
            **** autre très bonne vidéo de container complètement créé à la main en Go, https://www.youtube.com/watch?v=Utf-A4rODH8, de *Liz RICE* (2016/10)
                **** Voir également le Gist en GO de *Julien Friedman* dont Liz s'est inspirée : https://gist.github.com/julz/c0017fa7a40de0543001 (au final on build un container en ~55 lignes de Go)

        *** le travail sur les *cgroups* a commencé en 2006 chez Google sous le nom "process containers", avant d'être renommé en "control groups" pour éviter toute confusion avec le terme "container" dans un contexte Linux Kernel.
            **** cf Wikipedia (https://en.wikipedia.org/wiki/Cgroups) : +
            "A control group (abbreviated as cgroup) is a *collection of processes that are bound by the same criteria* and associated with a set of parameters or limits. These groups can be *hierarchical*, meaning that *each group inherits limits from its parent group*. The kernel provides access to multiple controllers (also called subsystems) through the cgroup interface;[2] for example, the "memory" controller limits memory use, "cpuacct" accounts CPU usage, etc."

        *** Development and maintenance of cgroups was then taken over by Tejun Heo. Tejun Heo redesigned and rewrote cgroups. This rewrite is now called version 2, the documentation of *cgroups v2* first appeared in Linux kernel 4.5 released on 14 March 2016. +
        Unlike v1, cgroups v2 has only a *single process hierarchy* and discriminates between processes, not threads.

    ** *namespaces* are a Linux feature allowing your processes to be separated from the other processes on the computer. +
    You can have PID namespace, networking namespace, mount namespace. +
    Namespaces can be creates using the `unshare` program.

    ** Pour les *dates* de création des *cgroups* et *namespaces*, voir cet article : https://www.silicon.co.uk/software/open-source/linux-kernel-cgroups-namespaces-containers-186240

        *** *cgroups* were originally developed by Paul Menage and Rohit Seth of Google, and their first features were merged into *Linux 2.6.24* (*2008/01*) +
        Cf Wikipedia (https://en.wikipedia.org/wiki/Cgroups) : 
        "Engineers at Google (primarily *Paul Menage* and *Rohit Seth*) *started the work on this feature in 2006* under the name "*process containers*".[1] In late 2007, the nomenclature changed to "control groups" to avoid confusion caused by multiple meanings of the term "container" in the Linux kernel context, and the control groups functionality was merged into the Linux kernel mainline in *kernel version 2.6.24*, which was *released in January 2008*."

        *** *user namespaces* were originally developed by *Eric Biederman*, and the final major namespace was merged into *Linux 3.8*. +
        Cf Wikipedia (https://en.wikipedia.org/wiki/Linux_namespaces) : 
        "The Linux Namespaces originated in *2002 in the 2.4.19 kernel* (2002/08/03) with work on the *mount namespace* kind. Additional namespaces were added beginning in 2006[2] and continuing into the future. +
        Adequate containers support functionality was finished in kernel *version 3.8* with the *introduction of User namespaces*."
            **** Et l'info très intéressante est ici : ce sont les user namespaces, introduit avec le kernel 3.8 de Linux qui ont changé la donne, et dont Solomon Hykes dit en 2013 (voir la conf ci-dessous, à 16:19) que, ça y est, "les namespaces marchent maintenant".
            **** https://kernelnewbies.org/Linux_3.8 : "*Linux 3.8* was released on Mon, *18 Feb 2013*."

Une bonne définition d'un *container runtime* : +
.https://www.quora.com/What-is-container-runtime-in-Kubernetes/answer/John-Sundarraj
----
A container runtime is a library or software which has the ability to create, deploy and manage containers on its own. Basically, container runtimes are responsible for container lifecycle. It provides simple API layer to create, deploy and manage containers.
----

* *Docker was released for the 1st time the 2013/03/20*

* *Why Docker ?* by Solomon Hykes : https://www.youtube.com/watch?v=3N3n9FzebAA (2013/08/01, EXCELLENTE conf, toujours d'actualité) +
La grande raison de l'époque : *shipping software from A to B, reliably and automatically*
    ** It has to behave the same way on both machine, and this with technological stack behind applications being more and more complex
    ** and your shipping place can be different depending on developer environment, servers, etc etc. (a lot of possible combinations that result finally in different environments)
    ** 08:39 (https://youtu.be/3N3n9FzebAA?t=519), to avoid all those shipping problems in the (shipping) industry, one day in the 1950s, people agreed on using a standard box, with standard dimensions, weight, way to open the doors, etc etc. AND it resulted with the creation on the container we know today. +
    This "ugly box" allows *separation of concerns* : je crée un outil / soft, je veux le shipper, je le mets dans le container, et ma responsabilité pour le shipping s'arrête là. Je ne m'intéresse QU'A mon produit, et PAS au container. +
    De la même façon, pour les personnes en charge du shipping, elles n'ont pas besoin de s'intéresser à ce qu'il y a dans le container : elles savent que le container a une taille, un poids, des dimensions données, et que TOUS ces containers peuvent être utilisés via les mêmes moyens standards.
        *** ces "boîtes" ont réellement changé le monde à cette époque : AVANT, c'était une galère de livrer du fait de toutes les combinaisons possibles de packaging des produits à livrer.
            **** pour info, article sur l'histoire des shipping containers : https://mccontainers.com/blog/the-history-of-containers/ +
            "A couple of ISO standards were set to determine terminology, dimensions, classifications, identifiers and so on. Thanks to these standards we nowadays have the 20’ and 40’ containers, the 20’ container (Twenty-foot Equivalent Unit, or TEU) being the standard volume."
            **** la standardisation des containers dans il est fait mention ci-dessus arriva en 1967 (https://fr.wikipedia.org/wiki/Conteneur)
        *** We finally wanted to do the same in our IT world for our own shipping needs.
        
    ** Avant, on avait bien déjà des archives comme des jars, rvms, etc. MAIS ce *sandboxing n'était pas complet*

    ** Il y avait bien *les VMs* : cette fois-ci, on a l'appli et on livre finalement toute la machine avec. On est maintenant sûr qu'on a bien le même "contexte" à chaque livraison.
        *** C'est la seule façon de s'assurer de share software in a truly reliable and repeatable way : to *ship the WHOLE system with the application* (because, truly, the system is PART OF the application)
        *** *le souci* avec les VMs est que l'*on ship trop de choses* : hard drives, network interfaces, le total de RAM, le type de processeur, etc. 
            **** Et il ne faut pas que ce soit le développeur qui décide comment l'on va faire fonctionner son application sur toutes les infrastructures possibles, ce n'est pas son rôle (on brise la "separation of concerns" précédente)
                ***** Pour reprendre l'analogie avec les "vrais" containers, cela reviendrait à imposer le modèle de grue avec lequel les décharger, et le modèle de bateau avec lequel les transporter.
                ***** In our IT world, the infrastructure provider is NOT free to make those choices just because you give them to him with your application.
        *** autre souci, *les VMs sont volumineuses* : est-ce facile d'en faire tourner 10 en parallèle ? Non.
            **** En fait, les VMs ont certains des "défauts" des machines classiques : elles mettent du temps à booter, consomment beaucoup de RAM, etc etc. Pas le plus pratique pour un dev dans son travail quotidien.
        
    ** Pour avoir le *meilleur des 2 mondes*, archives et VMs, il faudrait : 
        *** Sandbox the entire system
        *** without machine details
        *** and without the performance hit
        *** Et tout ceci est rendu *possible grâce aux fonctionnalités du kernel Linux*, tout particulièrement le *namespacing* qui a été rendu "réellement" fonctionnel dernièrement
            **** avec ce nouveau namespacing (2013), on peut maintenant isoler n'importe quel process des autres, et faire "croire" à ce process qu'il a sa propre VM (alors qu'il ne l'a pas)
                ***** mais utiliser ces fonctionnalités d'isolation du kernel Linux n'est pas évident, ce qu'il manque est une façon standard de les utiliser (un container standard pour cela) : c'est ce qu'est Docker +
                Docker est avant tout : 
                ***** un standard container format
                ***** simple tools that enable people running the infrastructure to take that container (without knowing what is inside), and then run it

    ** Donc, pour résumer, on a fait Docker dans le but de *shipper*. +
    Il fallait donc que Docker ne soit pas "trop infâme" à utiliser.
        *** on avait déjà les Linux Containers (LXC) avant, mais ce type de Operating System (OS) Containers n'est pas des plus simples à utiliser. Ces derniers sont plutôt à destination des sysadmin, pas des équipes qui "ship"


* https://www.ianlewis.org/en/container-runtimes-part-1-introduction-container-r

    ** developers who want to run apps in containers will need more than just the features that low-level runtimes provide, they need APIs and features around image formats, image management, and sharing images, which are provided by high-level runtimes.
    ** Developers who implement low-level runtimes will say that higher level runtimes like *containerd* and *cri-o* are not actually container runtimes, as from their perspective they outsource the implementation of running a container to *runc*.

* https://www.ianlewis.org/en/container-runtimes-part-2-anatomy-low-level-contai : *LOW LEVEL CONTAINER RUNTIME*

    ** le concept de *low-level container runtime* est mis en avant
    ** Low-level runtimes have a limited feature set and typically perform the low-level tasks for *running a container* (ex : runC)
        ** low-level runtimes are responsible for the mechanics of actually running a container
        ** raison pour laquelle de nombreux low-level container runtime s'appellent "run<quelque chose>"
    ** *Namespaces* let you virtualize system resources, like the file system or networking for each container.
        *** Namespaces are "what you can see"
    ** *cgroups* provide a way to limit the amount of resources, such as CPU and memory, that each container can use.
        *** control groups are "what you can use"
    ** At their core, low-level container runtimes are responsible for setting up these namespaces and cgroups for containers, and then running commands inside those namespaces and cgroups.

    ** Examples of low-level container runtimes : 

        *** *lmctfy* (Let Me Contain That For You) : projet by Google, based on the internal container runtime that *Borg* uses. +
        It supports container hierarchies that use cgroups hierarchies via the container names (a root container called "busybox" could create sub-containers under the name "busybox/sub1" or "busybox/sub2") +
        While lmctfy provides some interesting features and ideas, other runtimes were more usable so Google decided it would be better for the community to focus worked on Docker's "libcontainer" instead of lmctfy.

            *** *libcontainer* : voir http://igm.univ-mlv.fr/~dr/XPOSE2014/Docker/fonctionnement.html +
            "Libcontainer est une bibliothèque écrite en Go pour la création de conteneurs avec des espaces de noms, les groupes de contrôle, les capacités et les contrôles d'accès du système de fichiers. Cette librairie a été développée pour faire le travail de lxc tout en simplifiant l'installation de docker. Elle vous permet de gérer le cycle de vie du conteneur, effectuer des opérations supplémentaires après que le container soit créé."
            *** *Borg* is Google's cluster manager that runs hundreds of thousands of jobs, from many thousands of different applications, across a number of clusters each with up to tens of thousands of machines. +
            See https://research.google/pubs/pub43438/ for more details
            *** https://faun.pub/the-missing-introduction-to-containerization-de1fbb73efc5 : The libcontainer repository has been archived now. +
            Voir le repo https://github.com/docker-archive/libcontainer, et l'article de blog http://blog.docker.com/2015/06/open-container-project-foundation/. +
            Ce dernier, datant du 2015/06/15 annonce la création de l'Open Container Projet (OCP, plus tard rebaptisé OCI) et la donation de *runc* par Docker à ce projet. +
            Il y est expliqué que *libcontainer* a été la base de *runc* : +
            "Docker has taken the entire contents of the libcontainer project, including [nsinit], and all modifications needed to make it run independently of Docker,  and donated it to this effort. This codebase, called runC, can be found at github/opencontainers/runc. libcontainer will cease to operate as a separate project."

        *** *runC* : most widely used container runtime
            **** originally developed as part of Docker, then extracted as a separate tool and library.
                ***** So runC is the low-level runtime that was broken off from Docker.
            **** runC implements the *OCI runtime spec* (Open Container Initiative)
                ***** Pour plus détails, lire l'OCI runtime spec : https://github.com/opencontainers/runtime-spec
            **** https://www.tutorialworks.com/difference-docker-containerd-runc-crio-oci/ : runc is responsible for creating and running the container process.
            **** pour une très bonne ressource sur runc, voir https://www.agaetis.fr/blogpost/les-runtimes-oci
                ***** il est question de *runc* et de *crun* comme des "native runtimes", auxquels on va comparer les "*sandbox runtimes*" que *gVisor*, *Nabla containers* et *Kata containers* +
                Ces derniers sont présentés comme "limitant les interactions entre le conteneur et le kernel pour réduire au maximum la surface d’attaque, permettant ainsi une plus grande isolation. Dans cette catégorie nous allons voir gVisor,  Nabla containers et Kata containers." Donc un accent mis sur la *sécurité*.
                ***** concernant plus précisément runc et crun, il est expliqué que : +
                "Ensuite viens crun, un runtime en C développé par Red Hat. Il est supposé plus performant que runc et est le runtime par défaut de Podman. Même si crun a supporté *cgroups v2* avant runc, ce dernier a rattrapé son retard depuis."

        *** *rkt* (CoreOS *Rocket*):
            **** developed by CoreOS, which was later acquired by Red Hat
            **** provides all features provided by low-level container runtimes, PLUS some high-level ones
            **** As said by Docker : "rkt is CoreOS’s pod-native container engine"
            **** *projet ended / discontinued on 2020/02* and is not maintained anymore.
                ***** for more details on the reasons, see https://github.com/rkt/rkt/issues/4024 +
                The main ones seem to be : 
                ***** the previous development team at CoreOS got dismantled, and post Red Hat acquisition there are no plan to push the development forward
                ***** no more have development plans for rkt (from the new development team)
                ***** a declining engagement from the community

* https://www.ianlewis.org/en/container-runtimes-part-3-high-level-runtimes : *HIGH LEVEL CONTAINER RUNTIMES*

    ** *high-level runtimes* are responsible for *transport and management of container images*, unpacking the image, and *passing off to the low-level runtime* to *run the container*.
    ** Typically, high-level runtimes provide a *daemon* application and an *API* that remote applications can use to logically run containers and monitor them but they sit on top of and *delegate to low-level runtimes* or other high-level runtimes for the actual work. +
    High-level runtimes can also provide *features* that sound low-level, but are *used across individual containers on a machine*. For example, one feature might be the management of network namespaces, and allowing containers to join another container's network namespace.
    ** Exemples of high-level container runtime : 

        *** *Docker*
            **** Originally built as a monolithic daemon, *dockerd*, and the *docker client (Docker CLI)* application. +
            The daemon provided most of the logic of building containers, managing the images, and running containers, along with an API. +
            The command line client could be run to send commands and to get information from the daemon.
            **** It really was *the first* popular runtime to incorporate all of the features needed during the lifecycle of building and running containers, hence its success.
            **** A la base Docker faisait tout, les low et les high level features, mais cela a depuis (v1.11) été scindé en différentes briques, dont containerd et runC. +
            Docker se compose donc maintenant (2021) de docker CLI, dockerd, docker-containerd et docker-runc (les 2 derniers étant simplement des versions packagées de containerd et runc) ainsi que la Docker Engine API
                ***** *dockerd* provides features such as *building images*, and dockerd uses docker-containerd to provide features such as image management and running containers. For instance, Docker's build step is actually just some logic that interprets a Dockerfile, runs the necessary commands in a container using containerd, and *saves the resulting container file system as an image*.

        *** *ContainerD* 
            **** final "d" for daemon, containerd is a daemon
            **** is the high-level runtime that was split off from Docker.
            **** implements downloading images, managing them, and running containers from images. +
            When it needs to *run a container* it unpacks the image into an OCI runtime bundle and *shells out to runc* to run it.
            **** Containerd also provides an API and client application that can be used to interact with it. The *containerd command line client* is *ctr*.
            ****  In contrast with Docker, containerd is *focused solely on running containers*, so it *does NOT provide a mechanism for building containers*.
                ***** Docker was focused on end-user and developer use cases, whereas containerd is focused on operational use cases, such as running containers on servers. Tasks such as building container images are left to other tools.
                ***** traduction simple : containerd can't build images (c'est le travail du daemon dockerd par exemple)
            **** containerd is made *compliant with CRI* through its *CRI plugin* "cri-containerd" (as coming from Docker, it is NOT natively compliant with CRI which comes from Kubernetes)
                ***** see https://github.com/containerd/cri for more details

        *** *rkt*
            **** CAREFUL ! See above, *projet ended in 2020/02* !
            **** rkt is a runtime that has both low-level and high-level features
            **** rkt allows you to *build container images*, *fetch* and *manage container images* in a local repository, and *run them* all from a single command

* https://www.ianlewis.org/en/container-runtimes-part-4-kubernetes-container-run : *KUBERNETES CONTAINER RUNTIMES & CRI*

    ** *Kubernetes* runtimes are *high-level container runtimes* that support the *Container Runtime Interface* (*CRI*) (mandatory to integrate with Kubernetes)

        *** CRI was introduced in Kubernetes 1.5 and acts as a *bridge* between the *kubelet* and the *container runtime*.
            **** *kubelet* : https://kubernetes.io/docs/concepts/overview/components/#kubelet (or https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/) +
            "An *agent* that runs on each node in the cluster. It *makes sure that containers are running in a Pod*. +
            The kubelet takes a set of PodSpecs that are provided through various mechanisms and ensures that the containers described in those PodSpecs are running and healthy. The *kubelet doesn't manage containers which were not created by Kubernetes*"
            **** The kubelet is responsible for managing the container workloads for its node. +
            When it comes to actually run the workload, the kubelet uses CRI to communicate with the container runtime running on that same node. +
            In this way *CRI is simply an abstraction layer* or API that allows you to switch out container runtime implementations instead of having them built into the kubelet.
                ***** *CRI évite donc de coupler kubelet avec le container runtime* (logique, c'est une interface)

    ** The runtime is expected to handle the *management of images* and to *support Kubernetes pods*, as well as *manage the individual containers*. As a consequence, a Kubernetes runtime must be a high-level runtime per our definition in part 3.

    ** *containerd*
        *** implements CRI as a plugin, which is enabled by default
        *** it *supports multiple low-level runtimes* via something called a "runtime handler" starting in version 1.2. The runtime handler is passed via a field in CRI and based on that runtime handler containerd runs an application called a *shim* to start the container. This can be used to run containers using low-level runtimes other than runc, like *gVisor*, *Kata Containers*, or *Nabla Containers*.
            **** *gVisor*, *Kata Containers* et *Nabla Containers* sont souvent comparés car mettant tous en avant une *isolation très forte vis à vis de l'host*
            **** https://alenkacz.medium.com/whats-the-difference-between-runc-containerd-docker-3fc8f79d4d6e : +
            kata containers "is claiming to be all the isolation you love from VMs but that can be easily plugged into all the tooling we have around containers. This means you can spin up these VMs (or kata containers if you wish) through docker or Kubernetes."

    ** *Docker*
        *** Nowadays, Docker itself isn't necessary to support CRI, which is done through the use of containerd

    ** *cri-o*
        *** cri-o is a lightweight *CRI runtime* made as a *Kubernetes specific high-level runtime*.
        *** It supports the management of OCI compatible images and pulls from any OCI compatible image registry.
        *** It *supports runc* and *Clear Containers* as low-level runtimes. +
        It supports other OCI compatible low-level runtimes in theory, but relies on compatibility with the runc OCI command line interface, so in practice it isn't as flexible as containerd's shim API.
        *** *CRI-O* was created to provide a lightweight runtime for Kubernetes which adds an *abstraction layer between the cluster and the runtime that allows for various OCI runtime technologies* (https://developers.redhat.com/blog/2018/11/20/buildah-podman-containers-without-daemons#)

    ** the *CRI Specification*
        *** CRI is a *protocol buffers* and *gRPC* API.
        *** CRI *defines several remote procedure calls* (RPCs) and *message types*. The RPCs are for operations like "pull image" (ImageService.PullImage), "create pod" (RuntimeService.RunPodSandbox), "create container" (RuntimeService.CreateContainer), "start container" (RuntimeService.StartContainer), "stop container" (RuntimeService.StopContainer), etc.
        *** We can interact with a CRI runtime directly using the crictl tool. crictl lets us send gRPC messages to a CRI runtime directly from the command line.

*OCI* : *Image spec* ET *Runtime spec*

    * https://fr.wikipedia.org/wiki/Open_Container_Initiative : L'Open Container Initiative (OCI) est un projet de la Fondation Linux visant à *concevoir des normes ouvertes* pour la virtualisation au niveau du système d'exploitation, surtout les *conteneurs Linux*. Il existe actuellement deux spécifications en cours de développement et en cours d'utilisation: la spécification d'exécution (runtime-spec) et la spécification d'image (image-spec).

    * https://www.docker.com/blog/oci-release-of-v1-0-runtime-and-image-format-specifications/ (TRES BONNE RESSOURCE) : +
    "the *Open Container Project* (OCP) was formed to create a set of container standards and was launched under the auspices of the Linux Foundation in *June 2015 at DockerCon*. It became the Open Container Initiative (*OCI*) as the project evolved that Summer."
        ** cet article du blog de Docker, écrit par Patrick CHANEZON le 19/07/2017, contient également le *détail de toutes les contributions de Docker à l'OCI* jusqu'à cette date.
        ** Voici également l'article du blog de Docker annonçant la création de l'OCP (plus tard renommé OCI) : https://www.docker.com/blog/open-container-project-foundation/
            *** Docker will be donating both our base container format and runtime, runC, to this project, to help form the cornerstone for the new technology.  And, in a particularly exciting recent development, the talented people behind *appc* are now joining us as *co-founders*.
                **** Behing appc (App containers) is the people of rkt, and so CoreOS

    * https://faun.pub/docker-containerd-standalone-runtimes-heres-what-you-should-know-b834ef155426 : +
    "Formed in June 2015, the Open Container Initiative (OCI) aims to establish common standards for software containers in order to avoid a potential fragmentation and divisions inside the container ecosystem."

    * https://opencontainers.org/ : +
    "The Open Container Initiative is an open governance structure for the express purpose of *creating open industry standards around container formats and runtimes*." +
    "Established in *June 2015* by Docker and other leaders in the container industry, the OCI currently contains two specifications: the Runtime Specification (*runtime-spec*) and the Image Specification (*image-spec*). The Runtime Specification outlines how to run a “filesystem bundle” that is unpacked on disk. At a high-level an OCI implementation would download an OCI Image then unpack that image into an OCI Runtime filesystem bundle. At this point the OCI Runtime Bundle would be run by an OCI Runtime."

    * cf "https://www.tutorialworks.com/difference-docker-containerd-runc-crio-oci/" : the Open Container Initiative (OCI) which publishes specifications for images and containers.
        *** cf https://faun.pub/docker-containerd-standalone-runtimes-heres-what-you-should-know-b834ef155426, il est bien question de specifications pour des image-spec et runtime-spec
            **** Dans le schéma de https://www.tutorialworks.com/difference-docker-containerd-runc-crio-oci/, il est expliqué que : +
            "OCI provides specifications for container images and running containers."

    * "https://blog.engineering.publicissapient.fr/2019/12/23/docker-est-mort-vive-docker/" voir en 2:06
    * *runc* est une implémentation de la runtime-spec de l'OCI 
        ** runC a été publié pour la première fois en 2015/07 (https://fr.wikipedia.org/wiki/Open_Container_Initiative)
    * image-spec (OCI image spec) : https://github.com/opencontainers/image-spec
    * runtime-spec (OCI runtime spec) : https://github.com/opencontainers/runtime-spec

* Attention ! Fin 2020 (décembre) *deprecation de docker/docker-shim* (dockershim)
    ** oui, c'est bien confirmé : "the Kubernetes community announced it is deprecating Docker as a container runtime after v1.20". +
    Donc, il s'agit bien de la deprecation de *docker-shim*, ET *NON* de containerd-shim, qui n'a rien à voir sinon le "shim" dans le nom. +
    "Docker-shim was a temporary solution proposed by the Kubernetes community to add support for Docker so that it could serve as its container runtime." +
    Pour plus de détails, voir : 
        *** https://kubesphere.io/blogs/dockershim-out-of-kubernetes/
        *** https://linoxide.com/docker-alternative-container-tools/
        *** https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/ (2020/12/02) : l'annonce officielle sur le blog de Kubernetes
        *** voir également ce site de 2018, https://kubernetes.io/blog/2018/05/24/kubernetes-containerd-integration-goes-ga/, qui a de bons *schémas faisant apparaître dockershim*, ainsi que le CRI-plugin de containerd (le tout en lien avec kubelet)
            **** dockershim is "Docker's CRI implementation"
        *** et pour un schéma montrant bien l'avant et l'après dockershim, voir https://medium.com/nttlabs/docker-20-10-59cc4bd59d37 (2020/12/09)

A VOIR / FACULTATIF : 

* Attention ! 2021/09, changement de licence Docker Desktop, on ne peut plus l'utiliser sur Windows en entreprise.
* Parler de Docker Desktop qui conseille maintenant de passer, avec WSL 2, aux Linux Containers ?

=== Les daemonless et rootless containers et podman

* La 1ere release sur le repo https://github.com/containers/podman/releases date du 2018/04/05

* Pour information, pourquoi podman a pour logo un groupe de phoques ("seal" en anglais) ? Parce que, justement, un groupe de phoques est appelé "a seal POD" en anglais... ;)

* Une présentation de *Podman*, à Devoxx France 2021 (2021/10), par Benjamin Vouillaume : https://www.youtube.com/watch?v=pUFIG2AMDhg
    ** Podman est écrit en Go et supporté massivement par RedHat
    ** Podman utilise *crun*, runtime concurrent de *runc* (également OCI), développé pour Podman
        *** crun semble (beaucoup) plus performant que runc
        *** et la raison d'être, le pourquoi avoir eu besoin de créé *crun* sont les *cgroups v2*
        *** que permettent les cgroups v2 ? 
            **** Faire marcher les containers en *rootless*, c'est à dire *sans que nous soyons root* pour démarrer nos conteneurs +
            C'est un peu la *raison d'être de Podman* : fournir une interface semblable à Docker, tout en étant plus sécure avec le rootless (*on ne démarre pas les containers en root*)
    ** Podman est *daemonless*, contrairement à Docker, qui, à partir de la 1.11, fait :
        *** systemd -> 
        *** commande Docker run qui va démarrer le container -> 
        *** le Docker engine qui tourne pour interpréter cette commande -> 
        *** containerd qui tourne pour interpréter les informations que l'Engine va lui envoyer ->
        *** qui lui-même va appeler runc ->
        *** qui lui même va faire tourner votre application
    ** ALORS que Podman va directement appeler crun, et il n'y a pas de daemon. +
        Donc *pas* de processus qui tourne en arrière plan pour gérer nos containers.
        *** L'intérêt du daemonless est la sécurité. +
        Via de l'Audit Log sur Docker, on se rend compte que tout est en root, tout passe par le daemon (dockerd), donc on ne sait pas qui a fait quoi avec le container
    ** *application container* vs *system container*
        *** *application container* : ceux qu'on utilise le plus fréquemment, on met 1 process dans 1 container (ce que recommande Docker)
        *** *system container* : on va démarrer plein de process dans un container, ce dernier étant au final davantage une "micro-VM" mais containerisée. +
        On peut faire des system container avec Docker, mais il n'a pas réellement été fait pour, alors que c'est supporté par Podman. +
        Dans Podman, il est possible de démarrer directement systemd, le process parent d'une arborescence d'un OS, dans un container.
    ** Podman est très adapté à Kubernetes. +
    Podman sait gérer les pods kubernetes, ce que ne sait pas faire Docker
        *** pods : plusieurs containers isolés mais avec des éléments communs (souvent la partie network)
        *** On va pouvoir jouer un fichier Kubernetes existant directement sur podman pour démarrer vos pods

* https://podman.io/ : What is Podman? Podman is a *daemonless* container engine for developing, managing, and running OCI Containers on your Linux System. Containers can either be run as root or in *rootless* mode.

=== Histoire des containers

En fait, on trouve plus d'infos que je ne le pensais via les recherches Google "evolution of containers" et "history of containers", surtout en passant par la recherche images de Google

    ** https://www.redhat.com/en/blog/history-containers (2015/08) TRES BIEN

        *** *2000* : "jails", an early implementation of container technology, was added to FreeBSD
        *** *2001* : container technology made it to the Linux side of the house +
        "Jacques Gélinas created the VServer project, which according to the 0.0 version’s change log allowed “running several general purpose Linux server on a single box with a high degree of Independence and security.”" +
        The Linux-VServer solution was the first effort on Linux to “separate the user-space environment into distinct units (Virtual Private Servers) in such a way that each VPS looks and feels like a real server to the processes contained within.”
        *** *2006* : Paul Menage (Google) travaille sur les "process containers", plus tard renommé en cgroups (control groups) +
        "Cgroups allow processes to be grouped together, and ensure that each group gets a share of memory, CPU and disk I/O; preventing any one container from monopolizing any of these resources"
        *** *fin 2007* : ajout des 1eres briques de l'implémentation des user namespaces dans le kernel Linux 2.6.23 par Eric Biederman (Red Hat) +
        "Red Hatter Eric W. Biederman’s 2008 user namespaces patches being arguably the most complex and one of the most important namespaces in the context of containers. The implementation of user namespaces allows a process to have it’s own set of users and in particular to *allows a process root privileges inside a container, but not outside*."
        *** *2008* : création du projet Linux Containers (LXC) par des ingénieurs d'IBM. +
        "It layered some userspace tooling on top of cgroups and namespaces"
            **** https://fr.wikipedia.org/wiki/LXC : initial release 2008/08/06
        *** *2014/02/20* : release de la 1ere version 1.0 de LXC
        *** *2014/06/07* : toute première release de *Kubernetes* par Google (1er commit GitHub), qui le présente comme une version open source de Borg (Google’s *internal* container cluster-management system)
            **** Kubernetes en peu de mots : un gestionnaire de cluster de conteneurs open source
            **** pour cette date du 06/06, voir https://techcrunch.com/2018/06/06/four-years-after-release-of-kubernetes-1-0-it-has-come-long-way/
            **** Pour plus de détails sur l'histoire de Kubernetes, voir https://blog.risingstack.com/the-history-of-kubernetes/
        *** *2015* : Docker Inc donne la codebase du projet Docker à l'OCI. +
        "In June 2015, Docker the company, the largest contributor to Docker the project (Red Hat is the second), donated the project’s existing codebase to the Open Container Initiative, a lightweight governance structure under the auspices of the Linux Foundation created to *prevent fragmentation* and promote open standards by “cloud giants” including Red Hat."
            **** ce "prevent fragmentation" est très probablement la principal raison du "split" de Docker opéré par Docker Inc
        *** *2015/07/21* : release de la 1ere version de Kubernetes par Google, et création de la CNCF comme umbrella projet de la Linux Foundation. +
        Google versera / contribuera cette v1.0 de Kubernetes à la CNCF en tant que tout 1er projet et élément fondateur. +
        Pour rappel, la CNCF se définit comme "a Linux Foundation project that was founded in 2015 to help advance container technology and align the tech industry around its evolution" (voir https://en.wikipedia.org/wiki/Cloud_Native_Computing_Foundation et https://fr.wikipedia.org/wiki/Cloud_Native_Computing_Foundation)

    ** https://d2iq.com/blog/brief-history-containers (2018/07)

        *** *1970s* : +
        "The *original idea* of a container has been around since the 1970s, when the concept was first employed on *Unix systems* to *better isolate application code*. While useful in certain application development and deployment scenarios, the *biggest drawback* to containers in those early days was the simple fact that they were *anything but portable*." +
        "Back in the 1970s, *early containers created an isolated environment where services and applications could run without interfering with other processes* – producing something akin to a sandbox to test applications, services, and other processes. The original idea was to isolate the container's workload from production systems in way that *enabled developers to test their applications and processes on production hardware without risking disruption to other services*."

    ** https://blog.aquasec.com/a-brief-history-of-containers-from-1970s-chroot-to-docker-2016 (2020/01) (TRES BIEN)

        *** *1979* : "During the development of Unix version 7 in 1979, the *chroot* system call was introduced, changing the root directory of a process and its children to a new location in the filesystem." +
        "This advance was *the beginning process isolation*: segregating file access for each process. Chroot was added to BSD in 1982."
        *** *2000* : FreeBSD Jails +
        At that time, "a small shared-environment hosting provider came up with FreeBSD jails to achieve *clear-cut separation between its services and those of its customers* for *security* and *ease of administration*. FreeBSD Jails allows administrators to partition a FreeBSD computer system into several independent, smaller systems – called “jails” – with the ability to assign an IP address for each system and configuration."
            **** https://en.wikipedia.org/wiki/FreeBSD_jail : "Jails were first introduced in FreeBSD version 4.0, that was released on *March 14, 2000*"
        *** *2001* : Linux VServer +
        "Like FreeBSD Jails, Linux VServer is a jail mechanism that can partition resources (file systems, network addresses, memory) on a computer system. Introduced in 2001, this operating system virtualization that is implemented by patching the Linux kernel. Experimental patches are still available, but the last stable patch was released in 2006."
        *** *2004* : Solaris Containers +
        "In 2004, the first public beta of Solaris Containers was released that combines system resource controls and boundary separation provided by zones, which were able to leverage features like snapshots and cloning from ZFS."
            **** Cf Wikipedia, les principales caractéristiques du système de fichier ZFS pour Solaris sont, entre autres, sa très haute capacité de stockage, et la gestion de volume.
        *** *2005* : Open VZ (Open Virtuzzo) +
        "This is an operating system-level virtualization technology for Linux which uses a patched Linux kernel for virtualization, isolation, resource management and checkpointing. The code was not released as part of the official Linux kernel."
        *** *2006* : Process Containers (later renamed cgroups / Control Groups) +
        "Process Containers (launched by Google in 2006) was designed for limiting, accounting and isolating resource usage (CPU, memory, disk I/O, network) of a collection of processes. It was renamed “Control Groups (cgroups)” a year later and eventually merged to Linux kernel 2.6.24."
        *** *2008* : LXC +
        "LXC (LinuX Containers) was the first, most complete implementation of Linux container manager. It was implemented in 2008 using cgroups and Linux namespaces, and it works on a single Linux kernel *without requiring any patches*."
        *** *2011* : Warden +
        "CloudFoundry started Warden in 2011, using LXC in the early stages and later replacing it with its own implementation. Warden can isolate environments on any operating system, running as a daemon and providing an API for container management. It developed a client-server model to manage a collection of containers across multiple hosts, and Warden includes a service to manage cgroups, namespaces and the process life cycle."
        *** *2013* : LMCTFY +
        "Let Me Contain That For You (LMCTFY) kicked off in 2013 as an open-source version of Google's container stack (based on Borg internals), providing Linux application containers. Applications can be made “container aware,” creating and managing their own subcontainers. Active deployment in LMCTFY stopped in 2015 after Google started contributing core LMCTFY concepts to libcontainer, which is now part of the Open Container Foundation."
            **** initial release 2013/10/13, et final release (0.4.5) 2014/03/28
        *** *2013* : Docker +
        "When Docker emerged in 2013, containers exploded in popularity. It’s no coincidence the growth of Docker and container use goes hand-in-hand." +
        "Just as Warden did, Docker also used LXC in its initial stages and later replaced that container manager with its own library, libcontainer. But there’s no doubt that Docker separated itself from the pack by offering an entire ecosystem for container management."
        *** *2014/11* : 1ere release de rkt (https://blog.wescale.fr/2017/01/23/introduction-a-rkt/)
        *** *2017* : *Docker's donation of containerd project to the CNCF*
            **** Cette donation a eu le *2017/03/15*, voir l'annonce de Solomon Hykes https://www.docker.com/blog/docker-donates-containerd-to-cncf/ +
            Cet article explique également que containerd a été créé en 2016/12 : +
            "Back in December 2016, Docker spun out its core container runtime functionality into a standalone component, incorporating it into a separate project called containerd, [...]"
        *** 2017/03 : versement / contribution de rkt à la CNCF
        *** 2017/10 : DockerCon 2017, Docker announced they will support the Kubernetes container orchestrator, and Azure and AWS fell in line, with AKS (Azure Kubernetes Service) and Amazon EKS (Amazon Elastic Kubernetes Service)
        *** *2018* : *L'avènement de Kubernetes*, où tous les Cloud providers commencent à proposer leur offre de Kubernetes managé +
        "The massive adoption of Kubernetes pushed cloud vendors such as AWS, Google with GKE (Google Kubernetes Engine), Azure, and Oracle with Container Engine for Kubernetes, to offer managed Kubernetes services. Furthermore, leading software vendors such as VMWare, RedHat, and Rancher started offering Kubernetes-based management platforms."
        
            **** émergences des "*sandbox runtimes*" : *Kata containers*, *gVisor*, *Nabla* : +
            "We also witnessed emerging hybrid technologies that combine *VM-like isolation with container speed*. Open source projects such as Kata containers, gVisor, and Nabla attempt to provide *secured container runtimes* with lightweight virtual machines that perform the same way container do, but provide *stronger workload isolation*." +
            Voir cet article https://www.agaetis.fr/blogpost/les-runtimes-oci qui expliquent bien ce que sont les "*sandbox runtimes*" comme gVisor, Nabla containers et Kata containers : +
            "Les sandbox runtimes, des runtimes qui *isolent un peu plus les conteneurs de la machine hôte* en limitant les interactions entre le kernel et les conteneurs." +
            L'accent est donc mis sur la *SECURITE* : il faut combler les failles de sécurité des containers popularisés par Docker, c'est la raison d'être des sandbox runtimes. +
            "Les sandbox runtimes *limitent les interactions entre le conteneur et le kernel* pour *réduire au maximum la surface d’attaque*, permettant ainsi une plus grande isolation. Dans cette catégorie nous allons voir gVisor,  Nabla containers et Kata containers. Chacun utilisent une méthode différente pour y arriver". +
            Rappelons cette crainte que l'on avait du temps des débuts de Docker en 2013 : +
            "*Concern and hesitation* arose in the IT community regarding the *security of a shared OS kernel*" (https://searchitoperations.techtarget.com/feature/Dive-into-the-decades-long-history-of-container-technology)
                ***** *gVisor* implémente son propre kernel, *Sentry*, et son composant pour les interactions avec le système de fichiers, *Gofer*
                ***** *Nabla containers* utilise la technique de *l’unikernel* qui consiste à packager l’application avec une bibliothèque d’OS qui remplace un OS normal pour aboutir à une image de machine virtuelle minimale et dédiée à l’application.
                ***** *Kata containers* lance les conteneurs dans une *micro-VM dédiée*, optimisée pour démarrer vite et conçue pour cet usage. Un composant sur la machine hôte permet de faire le proxy et d’envoyer les instructions à l’agent Kata via l’hyperviseur. Les micro-VMs sont des VMs avec un minimum de fonctionnalités, seulement le strict nécessaire pour faire fonctionner des conteneurs.
            **** Ces "sandbox runtimes" permettent d’isoler les conteneurs, mais au prix de *performances dégradées*, et parfois plus : 
                ***** *gVisor* n’est pas compatible avec toutes les applications, notamment celles qui nécessitent un accès direct aux système de fichier, et il impactent aussi les performances.
                ***** *Nabla container* induit également une baisse de performance et plus important encore, il n’est pas tout à fait fini et *ne semble plus très maintenu*.
            **** *Kata containers* : lancement de la v1.0 le 2018/05/22 (https://techcrunch.com/2018/05/22/the-kata-containers-project-hits-1-0/)
            **** *gVisor* : release initiale en 2018/05/02 (https://en.wikipedia.org/wiki/GVisor)
                ***** blog de Google annonçant la sortie de gVisor le 2018/05/02 : https://cloud.google.com/blog/products/identity-security/open-sourcing-gvisor-a-sandboxed-container-runtime +
                "To that end, we’d like to introduce gVisor, a new kind of sandbox that helps provide secure isolation for containers, while being more lightweight than a virtual machine (VM). gVisor integrates with Docker and Kubernetes, making it simple and easy to run sandboxed containers in production environments."
                ***** https://www.zdnet.com/article/google-open-sources-gvisor-a-sandboxed-container-runtime/ (2018/05/03) : +
                "With gVisor, Google has introduced a new way to *sandbox containers*. These are containers that provide a *secure isolation boundary* between the host operating system and the application running within the container."
            **** *Nabla containers* : les Nabla containers ont été lancés en 2018/07 https://blog.hansenpartnership.com/a-new-method-of-containment-ibm-nabla-containers/ 
            **** Le choix de ces nouveaux runtimes est expliqué par Justin Cormarck, le CTO de Docker, à la KubeCon 2018 : https://static.sched.com/hosted_files/kccna18/c6/KubeCon_%20How%20to%20Choose%20a%20Kubernetes%20Runtime.pdf / https://www.youtube.com/watch?v=OZJkwvAnLb4 +
            Le choix de ces nouveaux containers runtimes est lié à l'usage de plus en plus massif de Kubernetes, et des containers qu'il fait tourner : de plus en plus de containers qui tournent impliquant une attention plus poussée à leur sécurité

        *** *2019* : les conséquences de l'essor de Kubernetes (le déclin de Docker)
            **** 2019/04 : la CNCF archive le projet rkt, suite à une adoption utilisateur en forte baisse
            **** 2019/11/13 : Docker se scinde en 2 : Mirantis rachète Docker Enterprise, et Docker Inc se recentre autour de Docker Desktop (et Docker Hub) et lève 35 millions auprès de ses précédents investisseurs Benchmark Capital et Insight Partners. +
            Voici l'explication officielle de Docker : +
            "Docker is ushering in a new era with a return to our roots by focusing on advancing developers’ workflows when building, sharing and running modern applications. As part of this refocus, Mirantis announced it has acquired the Docker Enterprise platform business,” Docker said in a statement when asked about this change. “Moving forward, we will expand Docker Desktop and Docker Hub’s roles in the developer workflow for modern apps. Specifically, we are investing in expanding our cloud services to enable developers to quickly discover technologies for use when building applications, to easily share these apps with teammates and the community, and to run apps frictionlessly on any Kubernetes endpoint, whether locally or in the cloud." +
            Pour plus d'explication, voir : 
                ***** https://techcrunch.com/2019/11/13/mirantis-acquires-docker-enterprise/
                ***** https://www.nextinpact.com/lebrief/40573/10329-docker-se-scinde-en-deux--mirantis-rachete-la-branche---entreprise--
        *** *2020/02* : project rkt is ended (https://github.com/rkt/rkt/issues/4024), so same thing for appc

    ** https://searchitoperations.techtarget.com/feature/Dive-into-the-decades-long-history-of-container-technology (2020/04) (TRES BONNES EXPLICATIONS et bon graphique, complet résumant l'histoire des containers avec ses grandes étapes)

        *** *1979* : développement de chroot, dans la version 7 d'Unix +
        "Chroot marked the beginning of container-style process isolation by restricting an application's file access to a specific directory -- the root -- and its children. A key benefit of chroot separation was improved system security, such that an isolated environment could not compromise external systems if an internal vulnerability was exploited."
        *** *2003* : Google introduced Borg, the organization's container cluster management system. +
        "It relied on the *isolation mechanisms that Linux already had in place*. In those early days in the evolution of containers, *security wasn't much of a concern*. Anyone could see what was going on inside the machine, which enabled a system of accounting for who was using the most memory and how to make the system perform better."
        *** *2006* (et pas 2004, erreur du site) : control groups / cgroups +
        "Nevertheless, this kind of container technology could only go so far. This led to the development of process containers, which became control groups (cgroups) as early as 2004. Cgroups noted the relationships between processes and reined in user access to specific activities and memory volumes. *The cgroups concept was absorbed into the Linux kernel in January 2008*, after which the Linux container technology LXC emerged. Namespaces developed shortly thereafter to provide the basis for container network security -- to hide a user's or group's activity from others."
        *** *2013* : l'émergence de Docker +
        Docker floated onto the scene in 2013 with an easy-to-use GUI, and the ability to package, provision and run container technology. Because Docker enabled multiple applications with different OS requirements to run on the same OS kernel in containers, IT admins and organizations saw opportunity for simplification and resource savings. +
        *Unlike VMs*, containers have a significantly smaller resource footprint, are faster to spin up and down, and require less overhead to manage. VMs must also each encapsulate a fully independent OS and other resources, while *containers share the same OS kernel* and use a proxy system to connect to the resources they need, depending upon where those resources are located. +
        *Concern and hesitation* arose in the IT community regarding the *security of a shared OS kernel*. A vulnerable container could result in a vulnerable ecosystem without the right precautions baked into the container technology. Additional complaints early in the modern evolution of containers bemoaned the lack of data persistence, which is important to the vast majority of enterprise applications. Efficient networking also posed problems, as well as the logistics of regulatory compliance and distributed application management.
        *** *2017* : Kubernetes a le vent en poupe
        *** *2017/04* : Microsoft enabled organizations to run Linux containers on Windows Server. This was a major development for Microsoft shops that wanted to containerize applications and stay compatible with their existing systems.
        *** *2020* : Gartner predicts that by 2022, more than 75% of global organizations will be running containerized applications in production, up from less than 30% today. +
        Worldwide container management revenue will grow strongly from a small base of $465.8 million in 2020, to reach $944 million in 2024, according to a new forecast from Gartner, Inc. +
        For more details, see https://www.gartner.com/en/newsroom/press-releases/2020-06-25-gartner-forecasts-strong-revenue-growth-for-global-co 
        *** *2021* : +
        Gartner predicts that by 2022, more than 75% of global organisations will be running containerised applications in production, up from less than 30% today. The analyst’s figures are reflected in the latest Red Hat Enterprise Open Source Report 2021, which shows container adoption is already widespread. Of the 1,250 IT leaders surveyed, just under 50% said they use containers in production to at least some degree. A further 37% use containers for development only, while just 16% are still evaluating or researching container adoption, according to Red Hat. +
        Voir https://www.computerweekly.com/feature/Containers-for-a-post-pandemic-IT-architecture
            **** Red Hat Enterprise Open Source Report 2021 : https://www.redhat.com/rhdc/managed-files/rh-enterprise-open-source-report-f27565-202101-en.pdf

    ** https://oziie.medium.com/something-missed-history-of-container-technology-e978f202464a (2020/03/31) : TRES BONNE RESSOURCE (que de très bonnes explications), et bon graphique résumant l'histoire des containers avec ses grandes étapes, et bonnes explications des techno impliquées

        *** le graphique vient en fait du site www.plesk.com : +
        https://www.plesk.com/blog/business-industry/infographic-brief-history-linux-containerization/

        *** *2013* : Docker +
        "Docker was introduced in 2013 by an San Francisco company that offers PaaS cloud services named dotCloud as an open-source project, and its founder is Solomon Hykes. When it first came out, *it aimed to convert monolitich applications into image and container structure by using LXC* (Linux containers). Later on, it started to develop his own container runtime, *libcontainer*, and after this stage, libcontainer was started to be used."

        *** *2014/12* : rkt +
        Rkt is a secure and lightweight Docker alternative container system developed by CoreOS. It is built on a container standard known as *App Container* or *appc*. For this reason, rkt images can be run on container systems that support the “appc” format. +
        "Unlike Docker, rkt runs containers with un-privileged users (unlike priority… Unlike Docker…). Thus, even if there is a kernel level deficit and the user can get out of the container, this does not affect other containers and users."
            **** rkt venait répondre à certaines des *problèmatiques de sécurité* existant avec Docker : +
            "As it is known, containers are process groups that can be created by granting some rights to users on the system or by processing with root. In addition, the operation of a user in one container is not seen by the other container. Users are safe in this way as long as there is no abuse on the Linux kernel. However, in some systems such as Docker, *malicious users who can get out of the container through an abuse on the kernel can ruin everything*. Such a risk exists despite measures."

        *** *l'avenir* (et la multiplication des runtimes) : *podman* (avec *buildah* et *Skopeo*), et le passage aux *daemonless* runtimes

            **** "*Podman* works with the “runC” we mentioned earlier so it works in accordance with the *daemonless* concept." It corrects some "daemon with" problems : 
                ***** At the point where no news is received from Daemon, there will be no access to the processes.
                ***** All Docker operations are performed by one or more users with the same root privileges. This could create a vulnerability.
            **** Pour une bonne présentation du pourquoi de podman (les problèmes de sécurité de Docker et l'hégémonie de Kubernetes) et une demo de son utilisation, voir https://www.redhat.com/en/blog/say-hello-buildah-podman-and-skopeo (2019/10) +
            "This excites some people who always saw the *monolith daemon that required root access for everything as a problem*. This brings us to the heart of this article – the *daemon-less* and largely *rootless* suite of container management tools."
            **** *Podman ne build pas d'image OCI*, il délègue cela à buildah

            **** *Buildah* : Buildah is a common containerize tool for container systems that comply with the OCI (Open Container Initiative) standards, one of the most important reasons for its development being its power in building container images.
                ***** 1st release v0.11 2018/01/17
                ***** Builday is a tool that facilitates building OCI images
                ***** The build commands in Podman are actually a subset of Buildah commands and they use the same codes.
                ***** Buildah also works as rootless and daemonless.
            **** Voir également cet excellent article sur les daemonless container runtimes Podman et Buildah, ainsi que le lien qui les unit : https://developers.redhat.com/blog/2018/11/20/buildah-podman-containers-without-daemons : +
            "Kubernetes installations can be complex with multiple runtime dependencies and runtime engines. *CRI-O* was created to provide a lightweight runtime for Kubernetes which adds an *abstraction layer between the cluster and the runtime that allows for various OCI runtime technologies*. However you still have the *problem of depending on daemon*(s) in your cluster for builds - I.e. if you are using the cluster for builds you still need a Docker daemon. +
            Enter Buildah. Buildah allows you to have a Kubernetes cluster without any Docker daemon for both runtime and builds. Excellent. But what if things go wrong? What if you want to do troubleshooting or debugging of containers in your cluster? Buildah isn’t really built for that, what you need is a client tool for working with containers and the one that comes to mind is Docker CLI - but then you’re back to using the daemon. +
            This is where Podman steps in. Podman allows you to do all of the Docker commands without the daemon dependency. To see examples of Podman replacing the docker command, see Alessandro Arrichiello's Intro to Podman and Doug Tidwell's Podman—The next generation of Linux container tools. +
            With Podman you can run, build (it calls Buildah under the covers for this), modify and troubleshoot containers in your Kubernetes cluster. With the two projects together, you have a well rounded solution for your OCI container image and container needs."

            **** *Skopeo* : gestion d'image, au sens de téléchargement, push et signature (principalement)

    ** vidéos sympas détaillant les débuts de l'histoire des  containers (jusqu'à Docker), et résumant bien l'usage des namespaces et cgroups : https://www.youtube.com/watch?v=9Egk9Tnc28E&list=PL5JFPVMx5WzXB-NlH13_G8R8dgfz564uo&index=2
        *** les vidéos 2 et 3 de la série présentent (rapidement) l'histoire de la containerisation, et l'écosystème Docker avec l'OCI et CRI (de plus, le speaker explique très rapidement comment installer correctement Docker sur Ubuntu en 2021)

    ** https://faun.pub/the-missing-introduction-to-containerization-de1fbb73efc5 (2019/03): là aussi, une bonne explication de l'histoire des containers
        *** avec une bonne explication de l'*architecture actuelle de Docker* (à partir de la 1.11) : +
--
Prior to version 1.11, Docker engine was used to manage volumes, networks, containers, images, etc.. +
Now, Docker architecture is broken into four components:

    * Docker engine,
    * containerd,
    * containerd-shim
    * and runC.

The binaries are respectively called docker, docker-containerd, docker-containerd-shim, and docker-runc.

Let’s enumerate the step to run a container using the new architecture of docker:

    1. Docker engine creates the container (from an image) and passes it to containerd.
    2. Containerd calls containerd-shim
    3. Containerd-shim uses runC to run the container
    4. Containerd-shim allows the runtime (runC in this case) to exit after it starts the container

Using this new architecture we can run “*daemon-less containers*” and we have two advantages:

    * runC can exit after starting the container and we don’t have to have the whole runtime processes running.
    * containerd-shim keeps the file descriptors like stdin, stdout, and stderr open even when Docker and/or containerd die.
--
        *** Pour un autre *très bon schéma de l'architecture actuelle de Docker* : https://iximiuz.com/en/posts/implementing-container-runtime-shim/ (2021/08/24)
            **** L'article également très bien le fonctionnement du shim containerd-shim

== Plan du talk

Commencez par un petit disclaimer pour le public ? "ce que ce talk n'est pas ?" (à savoir une prez sur comment utiliser Docker, CRI-O ou Podman)

1. un rappel : qu'est-ce qu'un container ?

Julia EVANS en dit la chose suivante (https://jvns.ca/blog/2016/10/10/what-even-is-a-container/) : +
"The word “container” doesn’t mean anything super precise. Basically there are a few *new Linux kernel features* (“namespaces” and “cgroups”) that let you isolate processes from each other. When you use those features, you call it “containers”." +

On trouve également le rappel "tout bête" suivant : *A container is a group of processes*

-> D'où : a container is a "just" a group of processes that are isolated from each other by some means.

[start=2]
2. L'histoire de la containerisation d'hier à aujourd'hui : La frise chronologique

    * cgroups et namespaces : les premisses des containers
    * Puis Docker
        ** prévoir schéma de ce "Docker l'ancien"
    * Puis Docker 1.11 et le "split" avec containerd et runc
        ** nouveau schéma, Docker Engine, server avec containerd et runc
        ** checker où en était Kubernetes à date du split (2016/04). CRI avait-il déjà fait son apparition ?
    * Puis l'arrivée de Kubernetes qui a entraîné l'apparition de CRI
        ** nouveau schéma avec Docker ET Kubernetes, et leurs container runtimes
    * Et maintenant toutes les alternatives possibles à "Docker" (en fait, de nouveaux high et low container runtime)
        ** pour les alternatives, voir https://linoxide.com/docker-alternative-container-tools/
        ** Podman (grâce à Kubernetes) est devenu un incontournable

    On commence par une *frise temporelle complète* du début des containers à nos jours, puis on en propose *une 2nd* avec (opinionated point of view) uniquement les plus grandes étapes que je compte détailler.

    A chaque début de nouvelle section, reprendre où nous en sommes dans la frise temporelle

Les étapes majeures à présenter plus en détails : 

* *Les cgroups et les namespaces*

    ** pourquoi a-t-on fait ça ? Principalement pour des besoins d'isolation
        *** retrouver les 1ers usages

* *L'arrivée de Docker, le début des containers*

    ** et au début, les containers, pour l'immense majorité des devs, c'était Docker et rien que Docker.
        *** Maintenant, il y a Docker la compagnie, et Docker la technologie
    ** Docker la compagnie ? Les images, les containers, la ligne de commande ?
    ** A la base la compagnie Docker a créé un outil simple et ergonomique pour travailler avec les containers, outil appelé "docker" (la CLI docker pour être plus précis)
        *** cette CLI permet très facilement to build images, pull them from registries, create, start and manager containers
    ** et la grosse différence se fait avec le passage à la version 1.11, et l'apparition de containerd et runc

    ** *Docker "à l'ancienne" avant la 1.11 (2016/04)*
        *** https://jvns.ca/blog/2016/10/02/i-just-want-to-run-a-container/

* *Le split de Docker (v1.11.0) : l'apparition de containerd et runc (high level et low level container runtime)*

    On a maintenant le Docker engine
    ** Docker client (CLI, GUI, etc.) 
    ** parle à un Docker Daemon 
    ** qui parle à containerd : un autre daemon qui va aller surveiller vos containers, les redémarrer
        *** containerd supervise les containers (start, stop, pause)
    ** qui parle à runc : une librairie, un espèce de wrapper qui va vous permettre de lancer plus facilement des processus isolés
        *** et c'est runc qui va lancer votre processus de façon isolé via les features de votre kernel (namespaces & co, etc.)
        *** runC can help you avoid being strongly tied to specific technologies, hardware, or cloud service providers.

    ** *containerd* et *runc* ont commencé à apparaître à partir de Docker *1.11.0* (2016/04) ?
        *** à confirmer via https://jvns.ca/blog/2016/10/02/i-just-want-to-run-a-container/ (site de 2016)
            **** OUI, confirmé via https://faun.pub/docker-containerd-standalone-runtimes-heres-what-you-should-know-b834ef155426 : +
            "Docker Engine 1.11 was the first release built on runC (a runtime based on Open Container Intiative technology) and containerd."
        *** Regarder avant tout le blog de Docker : https://www.docker.com/blog/docker-engine-1-11-runc/ (2016/04/13, sortie de Docker 1.11)
            **** "Over the last year (2015), Docker has helped advance the work of the OCI to make it more readily available to more users. It started in *December 2015*, when we *introduced containerd*, a daemon to control runC. This was part of our effort to *break out Docker into small reusable components*."
            **** *2017/03/15* Docker's donation of containerd to the CNCF
        *** voir https://containerd.io/ pour un bon schéma de *containerd*, montrant les low-level runtimes qui gravitent aujourd'hui autour (2021) : https://containerd.io/img/architecture.png

    ** détailler ici les low-level container runtimes, et les high-level container runtimes

* *l'arrivée de Kubernetes, et la démultiplication des runtimes*

    Kubernetes : fait naturellement tourner des containers dans des pods.

    * Donc l'ecosystem des containers est loin de se limiter au seul "Docker", c'est vraiment un *assemblage de diverses technos*, parmi lesquelles on peut citer : 
        ** pour builder des images OCI compliant : Kaniko (Google), buildah (RedHat), Makisu (Uber)
        ** pour lancer des containers depuis des images : CRI-o, rkt, containerd, Kata containers, gVisor, singularity, nabla, podman

    * Parler des confusions possible entre les différents "shim" : le deprecated docker-shim, et containerd-shim

== Frise temporelle complète v1

* *1970s* : Le concept d'isolation émerge du côté des systèmes Unix. +

    "The *original idea* of a container has been around since the 1970s, when the concept was first employed on *Unix systems* to *better isolate application code*. While useful in certain application development and deployment scenarios, the *biggest drawback* to containers in those early days was the simple fact that they were *anything but portable*." +
    "Back in the 1970s, *early containers created an isolated environment where services and applications could run without interfering with other processes* – producing something akin to a sandbox to test applications, services, and other processes. The original idea was to *isolate the container's workload from production systems* in way that *enabled developers to test their applications and processes on production hardware without risking disruption to other services*."

    "During the development of Unix version 7 in 1979, the *chroot* system call was introduced, changing the root directory of a process and its children to a new location in the filesystem." +
        "This advance was *the beginning process isolation*: segregating file access for each process. Chroot was added to BSD in 1982."

    ** Voir également cet excellent article sur les débuts d'Unix (Unics à l'époque, pour "Uniplexed Information and Computing Service") : +
    https://www.spiria.com/fr/blogue/breves-technos/unix-a-50-ans/

* *1979* : GRANDE ETAPE - *chroot* +

    ** "During the development of Unix version 7 in 1979, the *chroot* system call was introduced, changing the root directory of a process and its children to a new location in the filesystem."
    ** "This advance was *the beginning process isolation*: segregating file access for each process. Chroot was added to BSD in 1982."
    ** développement de chroot, dans la version 7 d'Unix
    ** "Chroot marked the beginning of container-style process isolation by restricting an application's file access to a specific directory -- the root -- and its children. A key benefit of chroot separation was improved system security, such that an isolated environment could not compromise external systems if an internal vulnerability was exploited."

        *** system call - http://www.di.uevora.pt/~lmr/syscalls.html : +
        A system call is just what its name implies -- a request for the operating system to do something on behalf of the user's program. The system calls are functions used in the kernel itself.

    ** Pour des exemples de chroot "breakouts", voir https://securityqueens.co.uk/im-in-chroot-jail-get-me-out-of-here/ +
    L'article contient également un bon schéma illustrant le résultat d'un chroot

* *2000/03* : *FreeBSD Jails* +

    ** "jails", an early implementation of container technology, was added to FreeBSD
    ** At that time, "a small shared-environment hosting provider came up with FreeBSD jails to achieve *clear-cut separation between its services and those of its customers* for *security* and *ease of administration*. FreeBSD Jails allows administrators to partition a FreeBSD computer system into several independent, smaller systems – called “jails” – with the ability to assign an IP address for each system and configuration."
    ** https://en.wikipedia.org/wiki/FreeBSD_jail : "Jails were first introduced in FreeBSD version 4.0, that was released on *March 14, 2000*"

    ** Pour un bon schéma illustrant l'usage de Jails, voir https://www.admin-magazine.com/Archive/2013/13/How-to-configure-and-use-jailed-processes-in-FreeBSD/(offset)/6

* *2001* : *Linux VServer* +

    ** Container technology made it to the Linux side of the house +
    "Jacques Gélinas created the VServer project, which according to the 0.0 version’s change log allowed “running several general purpose Linux server on a single box with a high degree of Independence and security.”"
    ** "Like FreeBSD Jails, Linux VServer is a jail mechanism that can partition resources (file systems, network addresses, memory) on a computer system. Introduced in 2001, this operating system virtualization that is implemented by *patching the Linux kernel*. Experimental patches are still available, but the last stable patch was released in 2006."

    ** Linux-VServer is a virtual private server implementation that was created by adding operating system-level virtualization capabilities to the Linux kernel. +
    https://en.wikipedia.org/wiki/Linux-VServer

* *2002/08* : Les 1er *Linux namespaces* (mount namespaces) sont ajoutés au kernel Linux 2.4.19 (2002/08/03)

    ** *namespaces* : allow processes to have their own network / PIDs / users / hostname / mounts / and more !

    ** la vidéo https://www.youtube.com/watch?v=sK5i-N34im8[cgroups, namespaces, and beyond: what are containers made from?] de Jérôme PETAZZONI (Docker) explique en détails les différentes fonctionnalités des *cgroups*, *différents types de namespaces*. +
        ATTENTION ! Elle date de 2015 !
            **** Il est également question des *container runtimes* qui sont basés sur les cgroups et les namespaces. +
            Exemples de container runtimes basés sur des namespaces et des cgroups : 
                ***** *LXC* (Linux Containers) : easy for sysadmins / OPS, hard for devs (requires significant elbow grease)
                ***** *systemd-nspawn*
                ***** *Docker*
                ***** *rkt*
                ***** *runC*
                ***** All those container runtimes use the same kernel features (at that time, 2015 ?)
            **** et maintenant des container runtimes qui ne sont PAS basés sur les namespaces et les cgroups : 
                **** *OpenVZ* : by example Travis CI gives you root in OpenVZ
                **** *Jails* / *Zones*

    ** Bon un bon schéma des différents Linux namespaces, voir https://8gwifi.org/docs/linux-namespace.jsp

    ** *namespaces* are a Linux kernel feature allowing your processes to be separated from the other processes on the computer. +
    You can have PID namespace, networking namespace, mount namespace. +
    Namespaces can be created using the `unshare` program.

    ** *namespaces* limit what you can see : https://youtu.be/sK5i-N34im8?t=1519[Jérôme Petazzoni à la DockerCon 2015]

    ** Pour les *dates* de création des *cgroups* et *namespaces*, voir cet article : https://www.silicon.co.uk/software/open-source/linux-kernel-cgroups-namespaces-containers-186240

    ** *namespaces* were originally developed by *Eric Biederman*, and the final major namespace was merged into *Linux 3.8*. +
        Cf Wikipedia (https://en.wikipedia.org/wiki/Linux_namespaces) : 
        "The Linux Namespaces originated in *2002 in the 2.4.19 kernel* (2002/08/03) with work on the *mount namespace* kind. Additional namespaces were added beginning in 2006[2] and continuing into the future. +
        Adequate containers support functionality was finished in kernel *version 3.8* with the *introduction of User namespaces*."
            **** Et l'info très intéressante est ici : ce sont les user namespaces, introduit avec le kernel 3.8 de Linux qui ont changé la donne, et dont Solomon Hykes dit en 2013 (voir la conf ci-dessous, à 16:19) que, ça y est, "les namespaces marchent maintenant".
            **** https://kernelnewbies.org/Linux_3.8 : "*Linux 3.8* was released on Mon, *18 Feb 2013*."

    ** Description des *mount namespaces* par Jérôme Petazzoni de Docker durant la DockerCon 2015 : https://youtu.be/sK5i-N34im8?t=1666

    ** *Namespaces* let you virtualize system resources, like the file system or networking for each container.
        *** Namespaces are *"what you can see"*

    ** At their core, low-level container runtimes are responsible for setting up these namespaces and cgroups for containers, and then running commands inside those namespaces and cgroups.

    ** fin 2007 : ajout des 1eres briques de l'implémentation des user namespaces dans le kernel Linux 2.6.23 par Eric Biederman (Red Hat) +
        "Red Hatter Eric W. Biederman’s 2008 user namespaces patches being arguably the most complex and one of the most important namespaces in the context of containers. The implementation of user namespaces allows a process to have it’s own set of users and in particular to *allows a process root privileges inside a container, but not outside*."

    ** "Nevertheless, this kind of container technology (speaking of Borg) could only go so far. This led to the development of process containers, which became control groups (cgroups) as early as 2004. Cgroups noted the relationships between processes and reined in user access to specific activities and memory volumes. The cgroups concept was absorbed into the Linux kernel in January 2008, after which the Linux container technology LXC emerged. *Namespaces developed shortly thereafter to provide the basis for container network security* -- to hide a user's or group's activity from others."

    ** vidéos sympas détaillant les débuts de l'histoire des  containers (jusqu'à Docker), et résumant bien l'usage des namespaces et cgroups : https://www.youtube.com/watch?v=9Egk9Tnc28E&list=PL5JFPVMx5WzXB-NlH13_G8R8dgfz564uo&index=2

* *2003* : Google introduced *Borg*, the organization's container cluster management system. +

    ** "It relied on the *isolation mechanisms that Linux already had in place*. In those early days in the evolution of containers, *security wasn't much of a concern*. Anyone could see what was going on inside the machine, which enabled a system of accounting for who was using the most memory and how to make the system perform better."
    ** *Borg* is Google's cluster manager that runs hundreds of thousands of jobs, from many thousands of different applications, across a number of clusters each with up to tens of thousands of machines. +
    See https://research.google/pubs/pub43438/ for more details

* *2004* : *Solaris Containers* +

    "In 2004, the first public beta of Solaris Containers was released that combines system resource controls and boundary separation provided by zones, which were able to leverage features like snapshots and cloning from ZFS."

    ** https://en.wikipedia.org/wiki/Solaris_Containers : +
    Zones act as completely isolated virtual servers within a single operating system instance

    ** ZFS (https://fr.wikipedia.org/wiki/ZFS) : +
    Les caractéristiques de ce système de fichiers sont sa *très haute capacité de stockage*, l'intégration de beaucoup de concepts que l'on trouve sur d'autres systèmes de fichiers, et la *gestion de volume*. Il utilise pour cela des structures de données comme les B-tree "On-Disk", et un adressage des secteurs disque logique au lieu d'un adressage physique. +
    Produit par Sun Microsystems (société rachetée par Oracle en 2009) pour *Solaris 10* et au-delà, il a été conçu par l'équipe de Jeff Bonwick (en). Annoncé pour septembre 2004, il a été intégré à Solaris le 31 octobre 2005 et le 16 novembre 2005 en tant que caractéristique du build 27 d'OpenSolaris. Sun a annoncé que ZFS était intégré dans la mise à jour de Solaris datée de juin 2006, soit un an après l'ouverture de la communauté OpenSolaris.

* *2005* : *Open VZ* +

    "This is an operating system-level virtualization technology for Linux which uses a patched Linux kernel for virtualization, isolation, resource management and checkpointing. The code was not released as part of the official Linux kernel."

    ** voir https://fr.wikipedia.org/wiki/OpenVZ +
    OpenVZ permet à un serveur physique d'exécuter de multiples instances de systèmes d'exploitation isolés, qualifiées de serveurs privés virtuels (VPS) ou environnements virtuels (VE).

    ** pour un bon schéma de l'architecture d'OpenVZ, voir http://www.virtualizationsoftwares.com/openvz-open-virtualization/

* *2006* : début des travaux sur les *Process Containers* chez Google (later renamed *cgroups* / Control Groups)

        ** Début des travaux sur les cgroups par Paul Menage and Rohit Seth chez Google

        ** Paul Menage (Google) travaille sur les "process containers", plus tard renommé en cgroups (control groups) +
        "Cgroups allow processes to be grouped together, and ensure that each group gets a share of memory, CPU and disk I/O; preventing any one container from monopolizing any of these resources"

        ** "Process Containers (launched by Google in 2006) was designed for limiting, accounting and isolating resource usage (CPU, memory, disk I/O, network) of a collection of processes. It was renamed “Control Groups (cgroups)” a year later and eventually merged to Linux kernel 2.6.24."

        ** "Nevertheless, this kind of container technology (speaking of Borg) could only go so far. This led to the development of process containers, which became control groups (cgroups) as early as 2004. Cgroups noted the relationships between processes and reined in user access to specific activities and memory volumes. *The cgroups concept was absorbed into the Linux kernel in January 2008*, after which the Linux container technology LXC emerged. Namespaces developed shortly thereafter to provide the basis for container network security -- to hide a user's or group's activity from others."

        ** Cf wikipedia (https://en.wikipedia.org/wiki/Cgroups), *cgroups* : +
        "cgroups (abbreviated from control groups) is a *Linux kernel feature* that limits, accounts for, and isolates the resource usage (CPU, memory, disk I/O, network, etc.) of a collection of processes."
        ** la vidéo https://www.youtube.com/watch?v=sK5i-N34im8[cgroups, namespaces, and beyond: what are containers made from?] de Jérôme PETAZZONI (Docker) explique en détails les différentes fonctionnalités des *cgroups*, *différents types de namespaces*. +
        ATTENTION ! Elle date de 2015 !
            *** Il est également question des *container runtimes* qui sont basés sur les cgroups et les namespaces. +
            Exemples de container runtimes basés sur des namespaces et des cgroups : 
                **** *LXC* (Linux Containers) : easy for sysadmins / OPS, hard for devs (requires significant elbow grease)
                **** *systemd-nspawn*
                **** *Docker*
                **** *rkt*
                **** *runC*
                **** All those container runtimes use the same kernel features (at that time, 2015 ?)
            *** et maintenant des container runtimes qui ne sont PAS basés sur les namespaces et les cgroups : 
                *** *OpenVZ* : by example Travis CI gives you root in OpenVZ
                *** *Jails* / *Zones*

        ** le travail sur les *cgroups* a commencé en 2006 chez Google sous le nom "process containers", avant d'être renommé en "control groups" pour éviter toute confusion avec le terme "container" dans un contexte Linux Kernel.
            **** cf Wikipedia (https://en.wikipedia.org/wiki/Cgroups) : +
            "A control group (abbreviated as cgroup) is a *collection of processes that are bound by the same criteria* and associated with a set of parameters or limits. These groups can be *hierarchical*, meaning that *each group inherits limits from its parent group*. The kernel provides access to multiple controllers (also called subsystems) through the cgroup interface;[2] for example, the "memory" controller limits memory use, "cpuacct" accounts CPU usage, etc."

        ** Pour les *dates* de création des *cgroups* et *namespaces*, voir cet article : https://www.silicon.co.uk/software/open-source/linux-kernel-cgroups-namespaces-containers-186240

            *** *cgroups* were originally developed by Paul Menage and Rohit Seth of Google, and their first features were merged into *Linux 2.6.24* (*2008/01*) +
            Cf Wikipedia (https://en.wikipedia.org/wiki/Cgroups) : 
            "Engineers at Google (primarily *Paul Menage* and *Rohit Seth*) *started the work on this feature in 2006* under the name "*process containers*".[1] In late 2007, the nomenclature changed to "control groups" to avoid confusion caused by multiple meanings of the term "container" in the Linux kernel context, and the control groups functionality was merged into the Linux kernel mainline in *kernel version 2.6.24*, which was *released in January 2008*."

        ** *cgroups* : limit memory / CPU usage for a group of processes

        ** *cgroups* provide a way to limit the amount of resources, such as CPU and memory, that each container can use.
            *** control groups are "what you can use"
        ** At their core, low-level container runtimes are responsible for setting up these namespaces and cgroups for containers, and then running commands inside those namespaces and cgroups.

        ** vidéos sympas détaillant les débuts de l'histoire des  containers (jusqu'à Docker), et résumant bien l'usage des namespaces et cgroups : https://www.youtube.com/watch?v=9Egk9Tnc28E&list=PL5JFPVMx5WzXB-NlH13_G8R8dgfz564uo&index=2

        ** *cgroups* limit how much you can use : https://youtu.be/sK5i-N34im8?t=1519[Jérôme Petazzoni à la DockerCon 2015]

        ** schéma sur les cgroups : https://www.lightnetics.com/topic/17326/what-are-control-groups-in-linux

* *2007/10* : ajout des 1eres briques de l'implémentation des *user namespaces* dans le kernel Linux 2.6.23 par Eric Biederman (Red Hat) +

    "Red Hatter Eric W. Biederman’s 2008 user namespaces patches being arguably the most complex and one of the most important namespaces in the context of containers. The implementation of user namespaces allows a process to have it’s own set of users and in particular to *allows a process root privileges inside a container, but not outside*."
        ** le kernel Linux 2.6.23 est releasé le 2007/10/10 (https://lwn.net/Articles/253813/)

* *2008/01* : GRANDE ETAPE - ajout de la fonctionnalité des *cgroups* dans le kernel Linux 2.6.24

* *2008/08* : GRANDE ETAPE - création du projet *Linux Containers (LXC)* par des ingénieurs d'IBM. +

    "It layered some userspace tooling on top of cgroups and namespaces"
        ** https://fr.wikipedia.org/wiki/LXC : initial release 2008/08/06
        ** "LXC (LinuX Containers) was the first, most complete implementation of Linux container manager. It was implemented in 2008 using cgroups and Linux namespaces, and it works on a single Linux kernel *without requiring any patches*."

    TO BE COMPLETED

* *2011* : Warden +

    "CloudFoundry started Warden in 2011, using LXC in the early stages and later replacing it with its own implementation. Warden can isolate environments on any operating system, running as a daemon and providing an API for container management. It developed a client-server model to manage a collection of ontainers across multiple hosts, and Warden includes a service to manage cgroups, namespaces and the process life cycle."

* *2013/02* : GRANDE ETAPE - ajout des *user namespaces* au kernel Linux 3.8

    ** A ce moment, on a les cgroups et les namespaces (que Solomon HYKES présente comme si importants), mais il y a également d'autres Linux Kernel features utilisés par les containers que l'on va présenter rapidement.

* *2013/03/20* : ETAPE MAJEURE - *1ere release de Docker*

    ** "When Docker emerged in 2013, containers exploded in popularity. It’s no coincidence the growth of Docker and container use goes hand-in-hand." +
    "Just as Warden did, Docker also used LXC in its initial stages and later replaced that container manager with its own library, libcontainer. But there’s no doubt that Docker separated itself from the pack by offering an entire ecosystem for container management."
    ** Docker floated onto the scene in 2013 with an easy-to-use GUI, and the ability to package, provision and run container technology. Because Docker enabled multiple applications with different OS requirements to run on the same OS kernel in containers, IT admins and organizations saw opportunity for simplification and resource savings. +
    *Unlike VMs*, containers have a significantly smaller resource footprint, are faster to spin up and down, and require less overhead to manage. VMs must also each encapsulate a fully independent OS and other resources, while *containers share the same OS kernel* and use a proxy system to connect to the resources they need, depending upon where those resources are located. +
    *Concern and hesitation* arose in the IT community regarding the *security of a shared OS kernel*. A vulnerable container could result in a vulnerable ecosystem without the right precautions baked into the container technology. Additional complaints early in the modern evolution of containers bemoaned the lack of  data persistence, which is important to the vast majority of enterprise applications. Efficient networking also posed problems, as well as the logistics of regulatory compliance and distributed application management.
    ** "Docker was introduced in 2013 by an San Francisco company that offers PaaS cloud services named dotCloud as an open-source project, and its founder is Solomon Hykes. When it first came out, *it aimed to convert monolitich applications into image and container structure by using LXC* (Linux containers). ater on, it started to develop his own container runtime, *libcontainer*, and after this stage, libcontainer was started to be used."

* *2013/10* : LMCTFY +

    "Let Me Contain That For You (LMCTFY) kicked off in 2013 as an open-source version of Google's container stack (based on Borg internals), providing Linux application containers. Applications can be made “container aware,” creating and managing their own subcontainers. Active deployment in LMCTFY stopped in 015 after Google started contributing core LMCTFY concepts to libcontainer, which is now part of the Open Container Foundation."
    ** initial release 2013/10/13, et final release (0.4.5) 2014/03/28

* *2014/02/20* : release de la 1ere version 1.0 de LXC

* *2014/06/07* : GRANDE ETAPE - toute première release de *Kubernetes* par Google (1er commit GitHub), qui le présente comme une version open source de Borg (Google’s *internal* container cluster-management system)

    ** Kubernetes en peu de mots : un gestionnaire de cluster de conteneurs open source
    ** pour cette date du 06/06, voir https://techcrunch.com/2018/06/06/four-years-after-release-of-kubernetes-1-0-it-has-come-long-way/
        *** Pour plus de détails sur l'histoire de Kubernetes, voir https://blog.risingstack.com/the-history-of-kubernetes/

* *2014/11* : 1ere release de rkt (prononcer "rocket"), le container runtime créé par les équipes de CoreOS.

    ** https://blog.wescale.fr/2017/01/23/introduction-a-rkt/
    ** Rkt is a secure and lightweight Docker alternative container system developed by CoreOS. It is built on a container standard known as *App Container* or *appc*. For this reason, rkt images can be run on container systems that support the “appc” format. +
    "Unlike Docker, rkt runs containers with un-privileged users (unlike priority… Unlike Docker…). Thus, even if there is a kernel level deficit and the user can get out of the container, this does not affect other containers and users."
    ** rkt venait répondre à certaines des *problèmatiques de sécurité* existant avec Docker : +
    "As it is known, containers are process groups that can be created by granting some rights to users on the system or by processing with root. In addition, the operation of a user in one container is not seen by the other container. Users are safe in this way as long as there is no abuse on the Linux kernel. However, in some systems such as Docker, *malicious users who can get out of the container through an abuse on the kernel can ruin everything*. Such a risk exists despite measures."

* *2015/06* : GRANDE ETAPE - *Docker Inc donne la codebase du projet Docker à l'OCI*, projet de la Linux Fondation, *créé pour cette occasion*. +

    "In June 2015, Docker the company, the largest contributor to Docker the project (Red Hat is the second), donated the project’s existing codebase to the *Open Container Initiative*, a lightweight governance structure under the auspices of the Linux Foundation created to *prevent fragmentation* and promote open standards by “cloud giants” including Red Hat."
        ** ce "*prevent fragmentation*" est très probablement la principal raison du "split" de Docker opéré par Docker Inc

* *2015/07* : GRANDE ETAPE - *runc* est publié pour la 1ere fois, et son code a *tout de suite donné par Docker à l'OCI* (runc est l'implémentation de référence de la runtime-spec)

* *2015/07/21* : ETAPE MAJEURE - *release de la 1ere version de Kubernetes* par Google, et *création de la CNCF*, comme umbrella projet de la Linux Foundation, à laquelle Kubernetes sera donné comme élément fondateur. +

    Google versera / contribuera cette v1.0 de Kubernetes à la CNCF en tant que tout 1er projet et élément fondateur. +
    Pour rappel, la CNCF se définit comme "a Linux Foundation project that was founded in 2015 to help advance container technology and align the tech industry around its evolution" (voir https://en.wikipedia.org/wiki/Cloud_Native_Computing_Foundation et https://fr.wikipedia.org/wiki/Cloud_Native_Computing_Foundation)

-> DONC EN 2015/06 et 2015/07, on a la création à la fois de la CNCF et de l'OCI

* *2015/12* : GRANDE ETAPE - Docker introduce *containerd*

* *2016/03/14* : 1ere apparition des cgroups v2 dans le Linux Kernel 4.5

    ** plus tard utilisé par runc et crun
        *** *runc* : runc fully supports cgroup v2 (unified mode) since v1.0.0-rc93, 2021/02/04 (https://github.com/opencontainers/runc)
        *** *crun* : dans la spécification d'origine en 2019/03, et "add support for OCI unified cgroups v2 in v0.15, 2020/09/23" (https://github.com/containers/crun/releases?q=cgroup+v2&expanded=true)

* *2016/04/13* : ETAPE MAJEURE - sortie de la *version 1.11 de Docker* : 1ere release de Docker basée sur containerd et runC (OCI)
    ** cette release marque le passage à l'architecture "moderne" de Docker

-> A partir de 2017, Kubernetes a de plus en plus le vent en poupe

* *2017/03/15* : GRANDE ETAPE - *Docker's donation of containerd project to the CNCF* AND *CoreOS's donation of rkt to the CNCF* 

    ** Cette donation a eu le *2017/03/15*, voir l'annonce de Solomon Hykes https://www.docker.com/blog/docker-donates-containerd-to-cncf/ +
    Cet article explique également que containerd a été créé en 2016/12 : +
    "Back in December 2016, Docker spun out its core container runtime functionality into a standalone component, incorporating it into a separate project called *containerd*, [...]"
    ** Les 2 annonces ont eu lieu le même jour, durant le même meeting du CNCF TOC (Technical Oversight Committee, où Docker et CoreOS étaient déjà représentés) +
    Voir https://www.cncf.io/blog/2017/03/29/rkt-pod-native-container-engine-launches-cncf/ : +
    "On March 15, 2017, at the CNCF TOC meeting, CoreOS and Docker made proposals to add rkt and containerd as new projects for inclusion in the CNCF. During the meeting, we as rkt co-founders, proposed rkt, and Michael Crosby, a containerd project lead and co-founder, proposed containerd."

* *2017/04* : Microsoft enabled organizations to run Linux containers on Windows Server. This was a major development for Microsoft shops that wanted to containerize applications and stay compatible with their existing systems.

* *2017/07/19* : Release of the OCI v1.0 runtime and image specifications

* *2017/10* : DockerCon 2017, Docker announced they will support the Kubernetes container orchestrator, and Azure and AWS fell in line, with AKS (Azure Kubernetes Service) and Amazon EKS (Amazon Elastic Kubernetes Service)

(* 2018 beginnning : CoreOS was acquired by Red Hat at the beginning of 2018)

-> 2018, l'avènement de Kubernetes, tous les plus grands Cloud providers en proposant une offre packagé, ET la démultiplication des container runtimes de types différents (sandbox runtimes, daemonless runtimes)

-> *2018* : ETAPE MAJEURE - *L'avènement de Kubernetes*, où tous les Cloud providers propose leur offre de Kubernetes managé +
"The massive adoption of Kubernetes pushed cloud vendors such as AWS, Google with GKE (Google Kubernetes Engine), Azure, and Oracle with Container Engine for Kubernetes, to offer managed Kubernetes services. Furthermore, leading software vendors such as VMWare, RedHat, and Rancher started offering Kubernetes-based management platforms." +
L'usage croissant de Kubernetes a démultiplié l'usage des containers (1 seul cluster Kubernetes pouvant en faire tourner jusqu'à un maximum de 300 000), rendant la *sécurisation* de ces derniers d'autant plus importante. +
Ce besoin accrue de sécurisation a amené à l'apparition de nouveaux types de runtimes : les sandbox runtimes, ainsi que les daemonless / rootless runtimes 

* émergences des "*sandbox runtimes*" : *Kata containers*, *gVisor*, *Nabla* : +

    "We also witnessed emerging hybrid technologies that combine *VM-like isolation with container speed*. Open source projects such as Kata containers, gVisor, and Nabla attempt to provide *secured container runtimes* with lightweight virtual machines that perform the same way container do, but provide *stronger workload isolation*." +
    Voir cet article https://www.agaetis.fr/blogpost/les-runtimes-oci qui expliquent bien ce que sont les "*sandbox runtimes*" comme gVisor, Nabla containers et Kata containers : +
    "Les sandbox runtimes, des runtimes qui *isolent un peu plus les conteneurs de la machine hôte* en limitant les interactions entre le kernel et les conteneurs." +
    L'accent est donc mis sur la *SECURITE* : il faut combler les failles de sécurité des containers popularisés par Docker, c'est la raison d'être des sandbox runtimes. +
    "Les sandbox runtimes *limitent les interactions entre le conteneur et le kernel* pour *réduire au maximum la surface d’attaque*, permettant ainsi une plus grande isolation. Dans cette catégorie nous allons voir gVisor,  Nabla containers et Kata containers. Chacun utilisent une méthode différente pour y arriver". +
    Rappelons cette crainte que l'on avait du temps des débuts de Docker en 2013 : +
    "*Concern and hesitation* arose in the IT community regarding the *security of a shared OS kernel*" (https://searchitoperations.techtarget.com/feature/Dive-into-the-decades-long-history-of-container-technology)
        *** *gVisor* implémente son propre kernel, *Sentry*, et son composant pour les interactions avec le système de fichiers, *Gofer*
        *** *Nabla containers* utilise la technique de *l’unikernel* qui consiste à packager l’application avec une bibliothèque d’OS qui remplace un OS normal pour aboutir à une image de machine virtuelle minimale et dédiée à l’application.
        *** *Kata containers* lance les conteneurs dans une *micro-VM dédiée*, optimisée pour démarrer vite et conçue pour cet usage. Un composant sur la machine hôte permet de faire le proxy et d’envoyer les instructions à l’agent Kata via l’hyperviseur. Les micro-VMs sont des VMs avec un minimum de fonctionnalités, seulement le strict nécessaire pour faire fonctionner des conteneurs.
    ** Ces "sandbox runtimes" permettent d’isoler les conteneurs, mais au prix de *performances dégradées*, et parfois plus : 
        *** *gVisor* n’est pas compatible avec toutes les applications, notamment celles qui nécessitent un accès direct aux système de fichier, et il impactent aussi les performances.
        *** *Nabla container* induit également une baisse de performance et plus important encore, il n’est pas tout à fait fini et *ne semble plus très maintenu*.

    ** *2018/05/22* : *Kata containers* : lancement de la v1.0 le 2018/05/22 (https://techcrunch.com/2018/05/22/the-kata-containers-project-hits-1-0/)

    ** *2018/05/02* : *gVisor* : release initiale en 2018/05/02 (https://en.wikipedia.org/wiki/GVisor)
        *** blog de Google annonçant la sortie de gVisor le 2018/05/02 : https://cloud.google.com/blog/products/identity-security/open-sourcing-gvisor-a-sandboxed-container-runtime +
        "To that end, we’d like to introduce gVisor, a new kind of sandbox that helps provide secure isolation for containers, while being more lightweight than a virtual machine (VM). gVisor integrates with Docker and Kubernetes, making it simple and easy to run sandboxed containers in production environments."
        *** https://www.zdnet.com/article/google-open-sources-gvisor-a-sandboxed-container-runtime/ (2018/05/03) : +
        "With gVisor, Google has introduced a new way to *sandbox containers*. These are containers that provide a *secure isolation boundary* between the host operating system and the application running within the container."

    ** *2018/07* : *Nabla containers* : les Nabla containers ont été lancés en 2018/07 https://blog.hansenpartnership.com/a-new-method-of-containment-ibm-nabla-containers/ 
    ** Le choix de ces nouveaux runtimes est expliqué par Justin Cormarck, le CTO de Docker, à la KubeCon 2018 : https://static.sched.com/hosted_files/kccna18/c6/KubeCon_%20How%20to%20Choose%20a%20Kubernetes%20Runtime.pdf / https://www.youtube.com/watch?v=OZJkwvAnLb4 +
    Le choix de ces nouveaux containers runtimes est lié à l'usage de plus en plus massif de Kubernetes, et des containers qu'il fait tourner : de plus en plus de containers qui tournent impliquant une attention plus poussée à leur sécurité.

* émergence des *daemonless runtimes* et du *rootless* : *podman* (avec *buildah* et *Skopeo*)

    ** *2018/04* : 1ere release de Podman sur le repo https://github.com/containers/podman/releases +
        "*Podman* works with the “runC” we mentioned earlier so it works in accordance with the *daemonless* concept." It corrects some "daemon with" problems : 
            *** At the point where no news is received from Daemon, there will be no access to the processes.
            *** All Docker operations are performed by one or more users with the same root privileges. This could create a vulnerability.
            *** Pour une bonne présentation du pourquoi de podman (les problèmes de sécurité de Docker et l'hégémonie de Kubernetes) et une demo de son utilisation, voir https://www.redhat.com/en/blog/say-hello-buildah-podman-and-skopeo (2019/10) +
            "This excites some people who always saw the *monolith daemon that required root access for everything as a problem*. This brings us to the heart of this article – the *daemon-less* and largely *rootless* suite of container management tools."
            *** *Podman ne build pas d'image OCI*, il délègue cela à buildah

        *** *Buildah* : Buildah is a common containerize tool for container systems that comply with the OCI (Open Container Initiative) standards, one of the most important reasons for its development being its power in building container images.
            **** 1st release v0.11 2018/01/17
            **** The build commands in Podman are actually a subset of Buildah commands and they use the same codes.
            **** Buildah also works as rootless and daemonless.

        *** Voir également cet excellent article sur les *daemonless container runtimes* Podman et Buildah, ainsi que le lien qui les unit : https://developers.redhat.com/blog/2018/11/20/buildah-podman-containers-without-daemons : +
        "Kubernetes installations can be complex with multiple runtime dependencies and runtime engines. *CRI-O* was created to provide a lightweight runtime for Kubernetes which adds an *abstraction layer between the cluster and the runtime that allows for various OCI runtime technologies*. However you still have the *problem of depending on daemon*(s) in your cluster for builds - I.e. if you are using the cluster for builds you still need a Docker daemon. +
        Enter Buildah. Buildah allows you to have a Kubernetes cluster without any Docker daemon for both runtime and builds. Excellent. But what if things go wrong? What if you want to do troubleshooting or debugging of containers in your cluster? Buildah isn’t really built for that, what you need is a client tool for working with containers and the one that comes to mind is Docker CLI - but then you’re back to using the daemon. +
        This is where Podman steps in. Podman allows you to do all of the Docker commands without the daemon dependency. To see examples of Podman replacing the docker command, see Alessandro Arrichiello's Intro to Podman and Doug Tidwell's Podman—The next generation of Linux container tools. +
        With Podman you can run, build (it calls Buildah under the covers for this), modify and troubleshoot containers in your Kubernetes cluster. With the two projects together, you have a well rounded solution for your OCI container image and container needs."
        
        *** *Skopeo* : gestion d'image, au sens de téléchargement, push et signature (principalement)

-> *2019* : GRANDE ETAPE - les conséquences de l'essor de Kubernetes (le déclin de Docker et d'autres container runtimes)

* *2019/04* : la CNCF archive le projet rkt, suite à une adoption utilisateur en forte baisse

* *2019/11/13* : Docker se scinde en 2 : Mirantis rachète Docker Enterprise, et Docker Inc se recentre autour de Docker Desktop (et Docker Hub) et lève 35 millions auprès de ses précédents investisseurs Benchmark Capital et Insight Partners. +

    Voici l'explication officielle de Docker : +
    "Docker is ushering in a new era with a return to our roots by focusing on advancing developers’ workflows when building, sharing and running modern applications. As part of this refocus, Mirantis announced it has acquired the Docker Enterprise platform business,” Docker said in a statement when asked about this change. “Moving forward, we will expand Docker Desktop and Docker Hub’s roles in the developer workflow for modern apps. Specifically, we are investing in expanding our cloud services to enable developers to quickly discover technologies for use when building applications, to easily share these apps with teammates and the community, and to run apps frictionlessly on any Kubernetes endpoint, whether locally or in the cloud." +
    Pour plus d'explication, voir : 
        ** https://techcrunch.com/2019/11/13/mirantis-acquires-docker-enterprise/
        ** https://www.nextinpact.com/lebrief/40573/10329-docker-se-scinde-en-deux--mirantis-rachete-la-branche---entreprise--

* *2020/02* : project rkt is ended (https://github.com/rkt/rkt/issues/4024), so same thing for appc

* *2020/12* : dockerd qui est déprécié pour Kubernetes 1.20 (2020/12), et remplacé par containerd +

    Kubernetes community announced it is deprecating Docker as a container runtime after v1.20. +
    Pour être plus précis, c'est l'usage du docker daemon (dockerd), au travers de Dockershim, qui est déprécié par Kubernetes. +
    Pour se "connecter à Docker", Kubernetes passera à partir de sa v1.20 par containerd : kubelet appelera directement containerd via son CRI-plugin

* *2020/06* : Gartner predicts that by 2022, more than 75% of global organizations will be running containerized applications in production, up from less than 30% today. +

    Worldwide container management revenue will grow strongly from a small base of $465.8 million in 2020, to reach $944 million in 2024, according to a new forecast from Gartner, Inc. +
    For more details, see https://www.gartner.com/en/newsroom/press-releases/2020-06-25-gartner-forecasts-strong-revenue-growth-for-global-co 

* *2021/01* : Red Hat Enterprise Open Source Report 2021 shows container adoption is already widespread +

    Gartner predicted in 2020 that, by 2022, more than 75% of global organisations will be running containerised applications in production, against 30% in 2020. +
    The analyst’s figures are reflected in the latest Red Hat Enterprise Open Source Report 2021, which shows container adoption is already widespread. +
    Of the 1,250 IT leaders surveyed, just under 50% said they use containers in production to at least some degree. A further 37% use containers for development only, while just 16% are still evaluating or researching container adoption, according to Red Hat. +
    Voir https://www.computerweekly.com/feature/Containers-for-a-post-pandemic-IT-architecture

*Informations globales à donner* : 

    * insister sur les Linux Kernel features, pas que les cgroups et namespaces
        ** montrer un exemple de code expliquant que l'on peut "coder un container" uniquement avec ces features (Gist en GO de *Julien Friedman*)
    * parler des "user namespaces qui marchent vraiment" de la version 3.8 du kernel Linux qui ont, cf Solomon Hykes lui-même, permis la sortie de Docker

    * bien définir ce qu'est un container runtime
        ** et définir ce que sont les low-level et high-level container runtimes
    * schéma de l'architecture de Docker A PARTIR DE LA 1.11
    * schéma des relations entre Docker ET Kubernetes avec les container runtimes qu'ils utilisent
    
    * parler des problèmes de sécurité qui faisaient peur à la sortie de Docker avec les containers : "shared OS kernel"

    * pour parler du daemonless, bien rappeler que tous les noms d'outils se terminant par "d" indiquent qu'il s'agit de daemon
        ** https://en.wikipedia.org/wiki/Daemon_(computing) : a daemon is a computer program that runs as a background process, rather than being under the direct control of an interactive user. +
        For example, syslogd is a daemon that implements system logging facility, and sshd is a daemon that serves incoming SSH connections.

    * redéfinir rapidement ce qu'est un "shim" +
    "In tech terms, a shim is a component in a software system, which acts as a *bridge between different APIs*, or as a compatibility layer. A shim is sometimes added when you want to use a third-party component, but you need a little bit of glue code to make it work."

*Ressources à donner dans le talk* : 

    * la série d'articles de Ian Lewis
    * Le talk durant lequel Solomon Hykes a introduit Docker, le 2013/08/01 : https://www.youtube.com/watch?v=3N3n9FzebAA[Why Docker ?]

* TODO : parler du grand problème de Docker, le "monolith daemon that required root access for everything" +
Ce qui a conduit à l'émergence des daemonless et autre rootless comme Podman et Buildah
* TODO : parler des confusions possibles en dockershim et containerd-shim

* TODO : pour Docker, bien dire que quoi est constitué Docker aujourd'hui, et faire apparaître un schéma des éléments du Docker Engine : +
Docker server (avec dockerd, containerd, containerd-shim, runc), l'API (Docker Engine API) et la CLI (ligne de commande "docker") +
S'inspirer par exemple de https://iximiuz.com/en/posts/implementing-container-runtime-shim/
    ** et en ajouter un de plus pour faire apparaître dockershim et le lien avec Kubernetes avant la v1.20
        *** et en ayant sur le même schéma dockershim et dockerd ce serait fantastique
    ** faire un mix avec le schéma du docker engine disponible ici : https://kubernetes.io/blog/2018/05/24/kubernetes-containerd-integration-goes-ga/ (qui fait apparaître dockerd)
    ** Voici un schéma faisant apparaître tous les éléments de Docker, avec dockershim et dockerd (faire juste attention au schéma indiquant le docker engine, on a l'impression que ce dernier utilise PLUTOT QUE contient dockerd, la CLI et l'API)

=== Outils à tester pour créer un chronologie en ligne 

Voir le https://elearningindustry.com/top-10-free-timeline-creation-tools-for-teachers. +
Les outils suivants ont l'air bien adaptés : 

    * https://timeline.knightlab.com/
    * https://www.timetoast.com/ : je trouve cet outil très bien, à tester

== Frise temporelle réduite

* *1979* : développement de *chroot*, dans la version 7 d'Unix, Le concept d'isolation émerge. +
-> "le point de départ, les prémices de la conteneurisation"
* *2000/03* : *FreeBSD Jails* 
* *2001* : *Linux VServer*
* *2002/08* : Les 1er *Linux namespaces* (mount namespaces) sont ajoutés au kernel Linux 2.4.19 (2002/08/03)
* *2003* : Google introduced *Borg*, the organization's container cluster management system.
* *2004* : *Solaris Containers*
* *2005* : *Open VZ* (Open Virtuzzo)
* *2006* : début des travaux sur les *Process Containers* chez Google (later renamed *cgroups* / Control Groups)
* *2007/10* : ajout des 1eres briques de l'implémentation des *user namespaces* dans le kernel Linux 2.6.23 par Eric Biederman (Red Hat)
* *2008/01* : GRANDE ETAPE - ajout de la fonctionnalité des *cgroups* dans le kernel Linux 2.6.24
* *2008/08* : GRANDE ETAPE - création du projet *Linux Containers (LXC)* par des ingénieurs d'IBM. +
* *2011* : Warden
* *2013/02* : GRANDE ETAPE - ajout des *user namespaces* au kernel Linux 3.8
* *2013/03/20* : ETAPE MAJEURE - *1ere release de Docker* +
-> "Le monde découvre les containers"
* *2013/10* : LMCTFY
* *2014/02/20* : release de la 1ere version 1.0 de LXC
* *2014/06/07* : GRANDE ETAPE - toute première release de *Kubernetes* par Google (1er commit GitHub)
* *2014/11* : 1ere release de rkt (prononcer "rocket"), le container runtime créé par les équipes de CoreOS.
* *2015/06* : GRANDE ETAPE - *Docker Inc donne la codebase du projet Docker à l'OCI*, projet de la Linux Fondation, *créé pour cette occasion*.
* *2015/07* : GRANDE ETAPE - *runc* est publié pour la 1ere fois, et son code a *tout de suite donné par Docker à l'OCI* (runc est l'implémentation de référence de la runtime-spec)
* *2015/07/21* : ETAPE MAJEURE - *release de la 1ere version de Kubernetes* par Google, et *création de la CNCF*, comme umbrella projet de la Linux Foundation, à laquelle Kubernetes sera donné comme élément fondateur.
* *2015/12* : GRANDE ETAPE - Docker introduce *containerd*
* *2016/03/14* : 1ere apparition des cgroups v2 dans le Linux Kernel 4.5
* *2016/04/13* : ETAPE MAJEURE - sortie de la *version 1.11 de Docker* : 1ere release de Docker basée sur containerd et runC (OCI) +
-> cette étape marque l'apparition des low-level et des high-level containers runtimes
* *2017/03/15* : GRANDE ETAPE - *Docker's donation of containerd project to the CNCF*, and *CoreOS's donation of rkt to the CNCF*
* *2017/04* : Microsoft enabled organizations to run Linux containers on Windows Server.
* *2017/07/19* : Release of the OCI v1.0 runtime and image specifications
* *2017/10* : DockerCon 2017, Docker announced they will support the Kubernetes container orchestrator, same thing for Azure and Amazon.
* *2018* : ETAPE MAJEURE - *L'avènement de Kubernetes*, où tous les Cloud providers propose leur offre de Kubernetes managé +
-> A partir de cette date, les containers deviennent présents partout, et dès lors, la *sécurité des containers* devient un sujet primordial.
    ** 2018 marque également l'apparition des "*sandbox runtimes*" : *Kata containers*, *gVisor*, *Nabla*
        *** *2018/05/02* : release initiale de *gVisor* par Google
        *** *2018/05/22* : lancement de la v1.0 de *Kata containers*
        *** *2018/07* : lancemnet des *Nabla containers*
    ** ainsi que celles des *daemonless runtimes* et du *rootless* : *podman* (avec *buildah* et *Skopeo*)
        *** *2018/04* : 1ere release de Podman
* *2019* : GRANDE ETAPE - les conséquences de l'essor de Kubernetes (le déclin de Docker et d'autres container runtimes)
* *2019/04* : la CNCF archive le projet rkt, suite à une adoption utilisateur en forte baisse
* *2019/11/13* : Docker se scinde en 2 : Mirantis rachète Docker Enterprise, et Docker Inc se recentre autour de Docker Desktop (et Docker Hub) 
* *2020/02* : le project rkt est arrêté (https://github.com/rkt/rkt/issues/4024)
* *2020/12* : le daemon docker (dockerd) est déprécié pour Kubernetes 1.20 (2020/12), et est remplacé par containerd
* *2021* : les containers sont maintenant partout, et le constat suivent tiré des études de Gartner : +
"Gartner predicted in 2020 that, by 2022, more than 75% of global organisations will be running containerised applications in production, against 30% in 2020."

~39 dates à présenter dans les 45 min de ce format conférence.

    * si l'on veut garder 5 min pour les questions, cela laisse 40 min pour le talk, soit ~1 min par date
    * 1 min ne sera pas suffisante pour certaines grandes dates (docker, l'apparition des high et low level container runtimes, etc.) +
    Il faudra passer rapidement certaines dates pour se concentrer sur les plus importantes

== Création des slides

1. clone du repo avec récupération du contenu du submodule reveal.js :

    git clone --recursive https://github.com/Ardemius/history-of-containerization.git

2. lancement du container :

    docker run -it -v <path-to-the-cloned-repo>/docs:/documents/ asciidoctor/docker-asciidoctor

    docker run -it -b D:\resources\my-talks-and-trainings\history-of-containerization:/documents/ asciidoctor/docker-asciidoctor

3. génération des slides :

    asciidoctor-revealjs history-of-containerization-slides.adoc -o docs/slides.html


